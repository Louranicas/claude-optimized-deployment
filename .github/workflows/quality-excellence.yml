name: üéØ Quality Excellence Pipeline
# Comprehensive quality assurance workflow for CODE platform
# ULTRA THINK MODE: Development Standards Quality Excellence

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'src/**'
      - 'tests/**'
      - 'pyproject.toml'
      - 'Cargo.toml'
      - 'requirements*.txt'
      - '.github/workflows/**'
  
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'src/**'
      - 'tests/**'
      - 'pyproject.toml'
      - 'Cargo.toml'
      - 'requirements*.txt'
  
  schedule:
    # Daily quality check at 2 AM UTC
    - cron: '0 2 * * *'
  
  workflow_dispatch:
    inputs:
      quality_level:
        description: 'Quality check level'
        required: true
        default: 'standard'
        type: choice
        options:
          - standard
          - comprehensive
          - critical-only
      
      skip_tests:
        description: 'Skip test execution'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.11'
  RUST_VERSION: 'stable'
  NODE_VERSION: '18'
  
  # Quality thresholds
  COVERAGE_THRESHOLD: '85'
  COMPLEXITY_THRESHOLD: '5'
  SECURITY_THRESHOLD: 'medium'
  
  # Tool versions
  BLACK_VERSION: '24.1.0'
  RUFF_VERSION: '0.1.0'
  MYPY_VERSION: '1.8.0'
  BANDIT_VERSION: '1.7.0'

jobs:
  # =============================================================================
  # QUALITY GATE VALIDATION
  # =============================================================================
  
  quality-gates:
    name: üö¶ Quality Gates Validation
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    outputs:
      syntax-check: ${{ steps.syntax.outputs.result }}
      security-critical: ${{ steps.security-critical.outputs.result }}
      quality-level: ${{ steps.determine-level.outputs.level }}
    
    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: üêç Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: üì¶ Install Quality Tools
        run: |
          pip install --upgrade pip
          pip install black==${{ env.BLACK_VERSION }} ruff==${{ env.RUFF_VERSION }}
          pip install bandit==${{ env.BANDIT_VERSION }} safety
      
      - name: üîç Syntax Check (Critical Gate)
        id: syntax
        run: |
          echo "Running syntax validation..."
          
          # Check Python syntax
          python -m py_compile src/**/*.py || SYNTAX_ERRORS=true
          
          # Check for obvious syntax issues
          ruff check src/ --select=E9,F63,F7,F82 --output-format=json > syntax_report.json || true
          
          if [[ "$SYNTAX_ERRORS" == "true" ]] || [[ $(jq length syntax_report.json) -gt 0 ]]; then
            echo "result=fail" >> $GITHUB_OUTPUT
            echo "‚ùå Syntax errors found - blocking pipeline"
            exit 1
          else
            echo "result=pass" >> $GITHUB_OUTPUT
            echo "‚úÖ Syntax validation passed"
          fi
      
      - name: üîí Critical Security Check
        id: security-critical
        run: |
          echo "Running critical security validation..."
          
          # Run Bandit for critical security issues
          bandit -r src/ -f json -o security_critical.json -ll || true
          
          # Check for critical vulnerabilities
          CRITICAL_COUNT=$(jq '.metrics."_totals"."SEVERITY.HIGH" // 0' security_critical.json)
          
          if [[ $CRITICAL_COUNT -gt 0 ]]; then
            echo "result=fail" >> $GITHUB_OUTPUT
            echo "‚ùå Critical security vulnerabilities found: $CRITICAL_COUNT"
            jq -r '.results[] | select(.issue_severity=="HIGH") | "\(.filename):\(.line_number): \(.issue_text)"' security_critical.json
            exit 1
          else
            echo "result=pass" >> $GITHUB_OUTPUT
            echo "‚úÖ No critical security issues found"
          fi
      
      - name: üìä Determine Quality Level
        id: determine-level
        run: |
          if [[ "${{ github.event.inputs.quality_level }}" != "" ]]; then
            echo "level=${{ github.event.inputs.quality_level }}" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" == "schedule" ]]; then
            echo "level=comprehensive" >> $GITHUB_OUTPUT
          elif [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "level=standard" >> $GITHUB_OUTPUT
          else
            echo "level=standard" >> $GITHUB_OUTPUT
          fi

  # =============================================================================
  # CODE QUALITY ANALYSIS
  # =============================================================================
  
  code-quality:
    name: üßÆ Code Quality Analysis
    runs-on: ubuntu-latest
    needs: quality-gates
    if: needs.quality-gates.outputs.syntax-check == 'pass'
    timeout-minutes: 15
    
    strategy:
      matrix:
        analysis:
          - { name: "Style & Formatting", tools: "black,ruff,isort" }
          - { name: "Type Safety", tools: "mypy" }
          - { name: "Complexity Analysis", tools: "radon,complexity" }
          - { name: "Code Quality", tools: "pylint,quality" }
    
    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4
      
      - name: üêç Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: üì¶ Install Dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements-dev.txt
          pip install black==${{ env.BLACK_VERSION }} ruff==${{ env.RUFF_VERSION }}
          pip install mypy==${{ env.MYPY_VERSION }} radon pylint
      
      - name: üîç ${{ matrix.analysis.name }}
        run: |
          mkdir -p quality_reports
          
          case "${{ matrix.analysis.tools }}" in
            *"black"*)
              echo "Running Black formatting check..."
              black --check --diff src/ tests/ > quality_reports/black_report.txt || true
              ;;
            *"ruff"*)
              echo "Running Ruff linting..."
              ruff check src/ tests/ --output-format=json > quality_reports/ruff_report.json || true
              ;;
            *"mypy"*)
              echo "Running MyPy type checking..."
              mypy src/ --json-report quality_reports/mypy_report.json || true
              ;;
            *"radon"*)
              echo "Running complexity analysis..."
              radon cc src/ -s --total-average --json > quality_reports/complexity_report.json || true
              ;;
            *"pylint"*)
              echo "Running Pylint quality analysis..."
              pylint src/ --output-format=json --reports=y > quality_reports/pylint_report.json || true
              ;;
          esac
      
      - name: üìä Upload Quality Reports
        uses: actions/upload-artifact@v3
        with:
          name: quality-reports-${{ matrix.analysis.name }}
          path: quality_reports/
          retention-days: 30

  # =============================================================================
  # SECURITY ANALYSIS
  # =============================================================================
  
  security-analysis:
    name: üîí Security Analysis
    runs-on: ubuntu-latest
    needs: quality-gates
    if: needs.quality-gates.outputs.security-critical == 'pass'
    timeout-minutes: 20
    
    strategy:
      matrix:
        scanner:
          - { name: "Python Security", tool: "bandit", target: "src/" }
          - { name: "Dependency Vulnerabilities", tool: "safety", target: "requirements.txt" }
          - { name: "Secret Detection", tool: "detect-secrets", target: "." }
    
    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: üêç Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: ü¶Ä Setup Rust (if needed)
        if: matrix.scanner.tool == 'cargo-audit'
        uses: actions-rs/toolchain@v1
        with:
          toolchain: ${{ env.RUST_VERSION }}
          override: true
      
      - name: üì¶ Install Security Tools
        run: |
          pip install --upgrade pip
          pip install bandit==${{ env.BANDIT_VERSION }} safety detect-secrets
          
          # Install Rust security tools if needed
          if [[ "${{ matrix.scanner.tool }}" == "cargo-audit" ]]; then
            cargo install cargo-audit
          fi
      
      - name: üîç ${{ matrix.scanner.name }}
        run: |
          mkdir -p security_reports
          
          case "${{ matrix.scanner.tool }}" in
            "bandit")
              echo "Running Bandit security scan..."
              bandit -r ${{ matrix.scanner.target }} \
                -f json -o security_reports/bandit_report.json \
                -ll || true  # Don't fail on findings
              ;;
            "safety")
              echo "Running Safety dependency scan..."
              safety check --json --output security_reports/safety_report.json || true
              ;;
            "detect-secrets")
              echo "Running secret detection..."
              detect-secrets scan --all-files \
                --baseline security_reports/secrets_baseline.json || true
              ;;
            "cargo-audit")
              echo "Running Cargo audit..."
              cargo audit --json --output security_reports/cargo_audit.json || true
              ;;
          esac
      
      - name: üìä Security Report Analysis
        run: |
          echo "Analyzing security scan results..."
          
          # Create summary report
          python3 -c "
import json
import glob
import sys

def analyze_security_reports():
    summary = {
        'total_issues': 0,
        'critical_issues': 0,
        'high_issues': 0,
        'medium_issues': 0,
        'low_issues': 0,
        'reports': {}
    }
    
    for report_file in glob.glob('security_reports/*.json'):
        try:
            with open(report_file) as f:
                data = json.load(f)
            
            if 'bandit' in report_file:
                metrics = data.get('metrics', {}).get('_totals', {})
                summary['critical_issues'] += metrics.get('SEVERITY.HIGH', 0)
                summary['medium_issues'] += metrics.get('SEVERITY.MEDIUM', 0)
                summary['low_issues'] += metrics.get('SEVERITY.LOW', 0)
                summary['reports']['bandit'] = metrics
            
            elif 'safety' in report_file:
                if isinstance(data, list):
                    safety_issues = len(data)
                    summary['high_issues'] += safety_issues
                    summary['reports']['safety'] = {'vulnerabilities': safety_issues}
        
        except Exception as e:
            print(f'Error analyzing {report_file}: {e}')
    
    summary['total_issues'] = (summary['critical_issues'] + 
                              summary['high_issues'] + 
                              summary['medium_issues'] + 
                              summary['low_issues'])
    
    with open('security_reports/security_summary.json', 'w') as f:
        json.dump(summary, f, indent=2)
    
    print(f'Security Analysis Summary:')
    print(f'Total Issues: {summary[\"total_issues\"]}')
    print(f'Critical: {summary[\"critical_issues\"]}')
    print(f'High: {summary[\"high_issues\"]}')
    print(f'Medium: {summary[\"medium_issues\"]}')
    print(f'Low: {summary[\"low_issues\"]}')
    
    # Fail if critical issues found
    if summary['critical_issues'] > 0:
        print('‚ùå Critical security issues found!')
        sys.exit(1)
    else:
        print('‚úÖ No critical security issues')

analyze_security_reports()
"
      
      - name: üìä Upload Security Reports
        uses: actions/upload-artifact@v3
        with:
          name: security-reports-${{ matrix.scanner.name }}
          path: security_reports/
          retention-days: 30

  # =============================================================================
  # TESTING AND COVERAGE
  # =============================================================================
  
  testing:
    name: üß™ Testing & Coverage
    runs-on: ubuntu-latest
    needs: quality-gates
    if: ${{ !inputs.skip_tests }}
    timeout-minutes: 30
    
    strategy:
      matrix:
        test-suite:
          - { name: "Unit Tests", path: "tests/unit/", markers: "unit", coverage: true }
          - { name: "Integration Tests", path: "tests/integration/", markers: "integration", coverage: true }
          - { name: "Security Tests", path: "tests/", markers: "security", coverage: false }
          - { name: "Performance Tests", path: "tests/", markers: "performance", coverage: false }
    
    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4
      
      - name: üêç Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: üì¶ Install Dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt -r requirements-dev.txt
          pip install pytest pytest-cov pytest-benchmark pytest-asyncio
      
      - name: üß™ Run ${{ matrix.test-suite.name }}
        run: |
          mkdir -p test_reports coverage_reports
          
          # Build pytest command
          PYTEST_CMD="python -m pytest ${{ matrix.test-suite.path }} -v"
          
          # Add markers if specified
          if [[ "${{ matrix.test-suite.markers }}" != "" ]]; then
            PYTEST_CMD="$PYTEST_CMD -m ${{ matrix.test-suite.markers }}"
          fi
          
          # Add coverage if enabled
          if [[ "${{ matrix.test-suite.coverage }}" == "true" ]]; then
            PYTEST_CMD="$PYTEST_CMD --cov=src --cov-report=xml:coverage_reports/coverage.xml"
            PYTEST_CMD="$PYTEST_CMD --cov-report=json:coverage_reports/coverage.json"
          fi
          
          # Add performance benchmarking for performance tests
          if [[ "${{ matrix.test-suite.markers }}" == "performance" ]]; then
            PYTEST_CMD="$PYTEST_CMD --benchmark-json=test_reports/benchmark_results.json"
          fi
          
          # Add JUnit XML for all tests
          PYTEST_CMD="$PYTEST_CMD --junitxml=test_reports/junit_results.xml"
          
          echo "Running: $PYTEST_CMD"
          $PYTEST_CMD || TEST_FAILED=true
          
          # Don't fail the workflow for non-critical test failures
          if [[ "$TEST_FAILED" == "true" && "${{ matrix.test-suite.markers }}" != "unit" ]]; then
            echo "‚ö†Ô∏è ${{ matrix.test-suite.name }} had failures (non-blocking)"
          elif [[ "$TEST_FAILED" == "true" ]]; then
            echo "‚ùå ${{ matrix.test-suite.name }} failed (blocking)"
            exit 1
          fi
      
      - name: üìä Coverage Analysis
        if: matrix.test-suite.coverage == true
        run: |
          if [[ -f coverage_reports/coverage.json ]]; then
            COVERAGE=$(python3 -c "
import json
with open('coverage_reports/coverage.json') as f:
    data = json.load(f)
print(f\"{data['totals']['percent_covered']:.1f}\")
")
            echo "Coverage: ${COVERAGE}%"
            
            if (( $(echo "$COVERAGE < ${{ env.COVERAGE_THRESHOLD }}" | bc -l) )); then
              echo "‚ùå Coverage ${COVERAGE}% below threshold ${{ env.COVERAGE_THRESHOLD }}%"
              if [[ "${{ matrix.test-suite.markers }}" == "unit" ]]; then
                exit 1  # Fail for unit tests
              fi
            else
              echo "‚úÖ Coverage ${COVERAGE}% meets threshold"
            fi
          fi
      
      - name: üìä Upload Test Reports
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: test-reports-${{ matrix.test-suite.name }}
          path: |
            test_reports/
            coverage_reports/
          retention-days: 30

  # =============================================================================
  # COMPREHENSIVE QUALITY ANALYSIS
  # =============================================================================
  
  quality-analysis:
    name: üìä Comprehensive Quality Analysis
    runs-on: ubuntu-latest
    needs: [quality-gates, code-quality, security-analysis, testing]
    if: always() && needs.quality-gates.outputs.syntax-check == 'pass'
    timeout-minutes: 15
    
    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4
      
      - name: üêç Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: üì¶ Install Quality Tracker
        run: |
          pip install --upgrade pip
          pip install -r requirements-dev.txt
      
      - name: üì• Download All Reports
        uses: actions/download-artifact@v3
        with:
          path: downloaded_reports/
      
      - name: üîç Run Quality Excellence Tracker
        run: |
          echo "Running comprehensive quality analysis..."
          
          # Consolidate all reports
          mkdir -p quality_reports
          find downloaded_reports/ -name "*.json" -exec cp {} quality_reports/ \;
          
          # Run quality tracker
          python quality_excellence_tracker.py \
            --project-root . \
            --output quality_reports/comprehensive_quality_report.json \
            --dashboard
      
      - name: üìä Generate Quality Dashboard
        run: |
          echo "Generating quality dashboard..."
          
          # Generate comprehensive dashboard
          make -f Makefile.quality metrics-dashboard || true
          
          # Create GitHub summary
          python3 -c "
import json
import sys

try:
    with open('quality_reports/comprehensive_quality_report.json') as f:
        report = json.load(f)
    
    # GitHub Actions summary format
    summary = f'''
# üéØ Quality Excellence Report
    
## Overall Quality Score: {report.get('overall_score', 0):.1f} ({report.get('grade', 'N/A')})
    
### Quality Gates Status
- **Passed**: {report.get('total_gates', 0) - len(report.get('blocking_issues', []))}
- **Failed**: {len(report.get('blocking_issues', []))}
- **Blocking Issues**: {len(report.get('blocking_issues', []))}
    
### Key Metrics
| Category | Status |
|----------|--------|
| Syntax Compliance | {'‚úÖ Pass' if '${{ needs.quality-gates.outputs.syntax-check }}' == 'pass' else '‚ùå Fail'} |
| Security Critical | {'‚úÖ Pass' if '${{ needs.quality-gates.outputs.security-critical }}' == 'pass' else '‚ùå Fail'} |
| Test Coverage | {'‚úÖ Pass' if '${{ env.COVERAGE_THRESHOLD }}' else '‚ö†Ô∏è Check Required'} |
    
### Recommendations
'''
    
    for rec in report.get('recommendations', []):
        summary += f'- {rec}\\n'
    
    # Write to GitHub summary
    with open('quality_summary.md', 'w') as f:
        f.write(summary)
    
    print('Quality analysis complete')
    
    # Set exit code based on blocking issues
    if len(report.get('blocking_issues', [])) > 0:
        print(f'‚ùå {len(report.get(\"blocking_issues\", []))} blocking quality issues found')
        sys.exit(1)
    else:
        print('‚úÖ No blocking quality issues')
        
except Exception as e:
    print(f'Error generating summary: {e}')
    print('‚ö†Ô∏è Quality analysis completed with warnings')
"
      
      - name: üìù Add to Job Summary
        if: always()
        run: |
          if [[ -f quality_summary.md ]]; then
            cat quality_summary.md >> $GITHUB_STEP_SUMMARY
          else
            echo "Quality analysis results not available" >> $GITHUB_STEP_SUMMARY
          fi
      
      - name: üí¨ Comment on PR
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            
            try {
              const summary = fs.readFileSync('quality_summary.md', 'utf8');
              
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: summary
              });
            } catch (error) {
              console.log('Could not create PR comment:', error);
            }
      
      - name: üìä Upload Final Reports
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: comprehensive-quality-report
          path: |
            quality_reports/
            quality_summary.md
          retention-days: 90

  # =============================================================================
  # DEPLOYMENT READINESS CHECK
  # =============================================================================
  
  deployment-readiness:
    name: üöÄ Deployment Readiness
    runs-on: ubuntu-latest
    needs: [quality-analysis]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    timeout-minutes: 10
    
    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4
      
      - name: üìä Download Quality Reports
        uses: actions/download-artifact@v3
        with:
          name: comprehensive-quality-report
          path: quality_reports/
      
      - name: üöÄ Validate Deployment Readiness
        run: |
          echo "Validating deployment readiness..."
          
          # Check if quality report exists
          if [[ ! -f quality_reports/comprehensive_quality_report.json ]]; then
            echo "‚ùå Quality report not found - cannot validate deployment readiness"
            exit 1
          fi
          
          # Parse quality report and check deployment criteria
          python3 -c "
import json
import sys

with open('quality_reports/comprehensive_quality_report.json') as f:
    report = json.load(f)

# Production deployment criteria
PRODUCTION_CRITERIA = {
    'min_overall_score': 80.0,
    'max_blocking_issues': 0,
    'max_critical_security': 0
}

overall_score = report.get('overall_score', 0)
blocking_issues = len(report.get('blocking_issues', []))

print(f'Quality Score: {overall_score}')
print(f'Blocking Issues: {blocking_issues}')

# Check criteria
ready_for_deployment = True
issues = []

if overall_score < PRODUCTION_CRITERIA['min_overall_score']:
    ready_for_deployment = False
    issues.append(f'Quality score {overall_score} below minimum {PRODUCTION_CRITERIA[\"min_overall_score\"]}')

if blocking_issues > PRODUCTION_CRITERIA['max_blocking_issues']:
    ready_for_deployment = False
    issues.append(f'{blocking_issues} blocking issues found (max: {PRODUCTION_CRITERIA[\"max_blocking_issues\"]})')

if ready_for_deployment:
    print('‚úÖ Ready for production deployment')
    
    # Create deployment approval artifact
    with open('deployment_approved.json', 'w') as f:
        json.dump({
            'approved': True,
            'timestamp': '$(date -Iseconds)',
            'quality_score': overall_score,
            'blocking_issues': blocking_issues,
            'criteria_met': True
        }, f, indent=2)
else:
    print('‚ùå Not ready for production deployment')
    for issue in issues:
        print(f'  - {issue}')
    
    # Create deployment rejection artifact
    with open('deployment_rejected.json', 'w') as f:
        json.dump({
            'approved': False,
            'timestamp': '$(date -Iseconds)',
            'quality_score': overall_score,
            'blocking_issues': blocking_issues,
            'issues': issues
        }, f, indent=2)
    
    sys.exit(1)
"
      
      - name: üìä Upload Deployment Status
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: deployment-status
          path: deployment_*.json
          retention-days: 30

# =============================================================================
# WORKFLOW SUMMARY
# =============================================================================

  workflow-summary:
    name: üìã Workflow Summary
    runs-on: ubuntu-latest
    needs: [quality-gates, code-quality, security-analysis, testing, quality-analysis]
    if: always()
    
    steps:
      - name: üìä Workflow Summary
        run: |
          echo "# üéØ Quality Excellence Workflow Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Stage | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Quality Gates | ${{ needs.quality-gates.result == 'success' && '‚úÖ Passed' || '‚ùå Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Code Quality | ${{ needs.code-quality.result == 'success' && '‚úÖ Passed' || needs.code-quality.result == 'skipped' && '‚è≠Ô∏è Skipped' || '‚ùå Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Security Analysis | ${{ needs.security-analysis.result == 'success' && '‚úÖ Passed' || needs.security-analysis.result == 'skipped' && '‚è≠Ô∏è Skipped' || '‚ùå Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Testing | ${{ needs.testing.result == 'success' && '‚úÖ Passed' || needs.testing.result == 'skipped' && '‚è≠Ô∏è Skipped' || '‚ùå Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Quality Analysis | ${{ needs.quality-analysis.result == 'success' && '‚úÖ Passed' || needs.quality-analysis.result == 'skipped' && '‚è≠Ô∏è Skipped' || '‚ùå Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow completed at:** $(date)" >> $GITHUB_STEP_SUMMARY