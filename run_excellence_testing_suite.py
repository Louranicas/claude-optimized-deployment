#!/usr/bin/env python3
"""
Excellence Testing Suite Runner
Meta Tree Mind Map Integration System

This script orchestrates the complete testing framework execution
ensuring highest development standards across 219+ test modules.
"""

import os
import sys
import json
import time
import subprocess
import asyncio
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('testing_excellence.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class ExcellenceTestingSuite:
    """Comprehensive testing suite orchestrator."""
    
    def __init__(self, project_root: str = "/home/louranicas/projects/claude-optimized-deployment"):
        self.project_root = Path(project_root)
        self.results_dir = self.project_root / "test_results"
        self.results_dir.mkdir(exist_ok=True)
        
        # Hardware optimization
        self.cpu_cores = os.cpu_count() or 4
        self.max_workers = min(12, self.cpu_cores)  # Optimize for 16-thread CPU
        
        # Test execution phases
        self.test_phases = {
            "Phase 1": {
                "name": "Environment Setup & Unit Tests",
                "commands": [
                    "make clean",
                    "make install-deps",
                    "make test-unit"
                ],
                "timeout": 300
            },
            "Phase 2": {
                "name": "Integration & FFI Bridge Tests", 
                "commands": [
                    "make test-integration",
                    "make test-ffi"
                ],
                "timeout": 600
            },
            "Phase 3": {
                "name": "Performance & Memory Tests",
                "commands": [
                    "make test-performance",
                    "make test-memory"
                ],
                "timeout": 900
            },
            "Phase 4": {
                "name": "Security & Vulnerability Tests",
                "commands": [
                    "make test-security",
                    "python comprehensive_security_audit.py"
                ],
                "timeout": 1200
            },
            "Phase 5": {
                "name": "E2E & Production Tests",
                "commands": [
                    "python test_full_stack.py",
                    "python test_production_modules_comprehensive.py"
                ],
                "timeout": 1800
            },
            "Phase 6": {
                "name": "Chaos & Reliability Tests",
                "commands": [
                    "make test-stress",
                    "python test_chaos_engineering_comprehensive.py"
                ],
                "timeout": 600
            }
        }
    
    def execute_command(self, command: str, timeout: int = 300) -> Dict[str, Any]:
        """Execute a single command with timeout and capture results."""
        start_time = time.time()
        
        try:
            logger.info(f"Executing: {command}")
            
            result = subprocess.run(
                command,
                shell=True,
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=timeout
            )
            
            duration = time.time() - start_time
            
            return {
                "command": command,
                "return_code": result.returncode,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "duration": duration,
                "success": result.returncode == 0,
                "timestamp": datetime.now().isoformat()
            }
            
        except subprocess.TimeoutExpired:
            duration = time.time() - start_time
            logger.error(f"Command timed out after {timeout}s: {command}")
            
            return {
                "command": command,
                "return_code": -1,
                "stdout": "",
                "stderr": f"Command timed out after {timeout} seconds",
                "duration": duration,
                "success": False,
                "timestamp": datetime.now().isoformat(),
                "timeout": True
            }
            
        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"Command failed with exception: {command} - {e}")
            
            return {
                "command": command,
                "return_code": -1,
                "stdout": "",
                "stderr": str(e),
                "duration": duration,
                "success": False,
                "timestamp": datetime.now().isoformat(),
                "exception": str(e)
            }
    
    def execute_phase(self, phase_name: str, phase_config: Dict[str, Any]) -> Dict[str, Any]:
        """Execute a complete test phase."""
        logger.info(f"üöÄ Starting {phase_name}: {phase_config['name']}")
        
        phase_start = time.time()
        phase_results = []
        
        for command in phase_config["commands"]:
            result = self.execute_command(command, phase_config["timeout"])
            phase_results.append(result)
            
            if not result["success"]:
                logger.warning(f"‚ö†Ô∏è Command failed: {command}")
                # Continue with other commands in phase
        
        phase_duration = time.time() - phase_start
        
        # Calculate phase statistics
        total_commands = len(phase_results)
        successful_commands = sum(1 for r in phase_results if r["success"])
        success_rate = (successful_commands / total_commands * 100) if total_commands > 0 else 0
        
        phase_summary = {
            "phase": phase_name,
            "name": phase_config["name"],
            "start_time": datetime.fromtimestamp(phase_start).isoformat(),
            "duration": phase_duration,
            "total_commands": total_commands,
            "successful_commands": successful_commands,
            "success_rate": success_rate,
            "commands": phase_results,
            "status": "‚úÖ PASSED" if success_rate >= 80 else "‚ö†Ô∏è PARTIAL" if success_rate >= 50 else "‚ùå FAILED"
        }
        
        logger.info(f"üìä {phase_name} completed: {success_rate:.1f}% success rate ({successful_commands}/{total_commands})")
        
        return phase_summary
    
    def execute_parallel_tests(self) -> List[Dict[str, Any]]:
        """Execute specific tests in parallel for optimization."""
        logger.info(f"üîÑ Executing parallel tests with {self.max_workers} workers")
        
        # Define parallel test commands
        parallel_commands = [
            "python test_circle_of_experts_comprehensive.py",
            "python test_mcp_comprehensive.py", 
            "python test_api_integrations.py",
            "python test_rust_integration.py",
            "python test_memory_monitoring_system.py",
            "python test_bash_god_comprehensive.py"
        ]
        
        results = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all commands
            future_to_command = {
                executor.submit(self.execute_command, cmd, 300): cmd 
                for cmd in parallel_commands
            }
            
            # Collect results as they complete
            for future in as_completed(future_to_command):
                command = future_to_command[future]
                try:
                    result = future.result()
                    results.append(result)
                    
                    status = "‚úÖ" if result["success"] else "‚ùå"
                    logger.info(f"{status} Parallel test completed: {command}")
                    
                except Exception as e:
                    logger.error(f"‚ùå Parallel test failed: {command} - {e}")
                    results.append({
                        "command": command,
                        "success": False,
                        "error": str(e),
                        "timestamp": datetime.now().isoformat()
                    })
        
        return results
    
    def generate_coverage_report(self) -> Dict[str, Any]:
        """Generate comprehensive coverage report."""
        logger.info("üìà Generating coverage report...")
        
        coverage_commands = [
            "coverage combine",
            "coverage report --format=json -o coverage/coverage.json",
            "coverage html",
            "coverage xml"
        ]
        
        coverage_results = []
        for cmd in coverage_commands:
            result = self.execute_command(cmd, 120)
            coverage_results.append(result)
        
        # Try to read coverage data
        coverage_data = {}
        coverage_file = self.project_root / "coverage" / "coverage.json"
        if coverage_file.exists():
            try:
                with open(coverage_file) as f:
                    coverage_data = json.load(f)
            except Exception as e:
                logger.warning(f"Could not read coverage data: {e}")
        
        return {
            "commands": coverage_results,
            "coverage_data": coverage_data,
            "coverage_percentage": coverage_data.get("totals", {}).get("percent_covered", 0)
        }
    
    def run_excellence_analysis(self) -> Dict[str, Any]:
        """Run the excellence framework analyzer."""
        logger.info("üîç Running excellence framework analysis...")
        
        result = self.execute_command("python test_framework_analyzer.py", 180)
        
        # Try to read the latest analysis report
        analysis_data = {}
        report_files = list(self.results_dir.glob("testing_excellence_report_*.json"))
        if report_files:
            latest_report = max(report_files, key=lambda x: x.stat().st_mtime)
            try:
                with open(latest_report) as f:
                    analysis_data = json.load(f)
            except Exception as e:
                logger.warning(f"Could not read analysis report: {e}")
        
        return {
            "analysis_command": result,
            "analysis_data": analysis_data
        }
    
    def save_comprehensive_report(self, execution_results: Dict[str, Any]) -> Path:
        """Save comprehensive execution report."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.results_dir / f"excellence_execution_report_{timestamp}.json"
        
        with open(report_file, 'w') as f:
            json.dump(execution_results, f, indent=2, default=str)
        
        return report_file
    
    def print_execution_summary(self, execution_results: Dict[str, Any]):
        """Print formatted execution summary."""
        print("\n" + "="*100)
        print("üöÄ TESTING EXCELLENCE FRAMEWORK EXECUTION SUMMARY")
        print("="*100)
        
        summary = execution_results["summary"]
        print(f"‚è±Ô∏è  Total Execution Time: {summary['total_duration']:.1f} seconds")
        print(f"üìä Overall Success Rate: {summary['overall_success_rate']:.1f}%")
        print(f"üî¢ Total Commands Executed: {summary['total_commands']}")
        print(f"‚úÖ Successful Commands: {summary['successful_commands']}")
        
        print("\nüìã Phase Results:")
        for phase_result in execution_results["phase_results"]:
            print(f"  {phase_result['status']} {phase_result['phase']}: {phase_result['success_rate']:.1f}% "
                  f"({phase_result['successful_commands']}/{phase_result['total_commands']})")
        
        coverage = execution_results.get("coverage_report", {})
        if coverage.get("coverage_percentage"):
            print(f"\nüìà Test Coverage: {coverage['coverage_percentage']:.1f}%")
        
        analysis = execution_results.get("excellence_analysis", {}).get("analysis_data", {})
        if analysis:
            failing_tests = analysis.get("failing_tests", {})
            total_failing = sum(len(issues) for issues in failing_tests.values())
            if total_failing > 0:
                print(f"\nüîß Failing Tests Identified: {total_failing}")
                for category, issues in failing_tests.items():
                    if issues:
                        print(f"  - {category.replace('_', ' ').title()}: {len(issues)} issues")
        
        print("\nüí° Recommendations:")
        if analysis.get("recommendations"):
            for i, rec in enumerate(analysis["recommendations"][:5], 1):
                print(f"  {i}. {rec}")
        else:
            print("  1. Review failed test results")
            print("  2. Increase test coverage")
            print("  3. Fix import dependency issues")
            print("  4. Implement CI/CD pipeline")
        
        print("\n" + "="*100)
    
    async def run_complete_suite(self) -> Dict[str, Any]:
        """Run the complete testing excellence suite."""
        logger.info("üéØ Starting Complete Testing Excellence Suite")
        suite_start = time.time()
        
        execution_results = {
            "execution_id": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "start_time": datetime.now().isoformat(),
            "phase_results": [],
            "parallel_results": [],
            "coverage_report": {},
            "excellence_analysis": {},
            "summary": {}
        }
        
        try:
            # Execute all test phases sequentially
            for phase_name, phase_config in self.test_phases.items():
                phase_result = self.execute_phase(phase_name, phase_config)
                execution_results["phase_results"].append(phase_result)
            
            # Execute parallel tests for optimization
            logger.info("üîÑ Running optimized parallel tests...")
            parallel_results = self.execute_parallel_tests()
            execution_results["parallel_results"] = parallel_results
            
            # Generate coverage report
            coverage_report = self.generate_coverage_report()
            execution_results["coverage_report"] = coverage_report
            
            # Run excellence analysis
            excellence_analysis = self.run_excellence_analysis()
            execution_results["excellence_analysis"] = excellence_analysis
            
        except Exception as e:
            logger.error(f"‚ùå Suite execution failed: {e}")
            execution_results["error"] = str(e)
        
        # Calculate summary statistics
        suite_duration = time.time() - suite_start
        
        total_commands = sum(len(phase["commands"]) for phase in execution_results["phase_results"])
        total_commands += len(execution_results["parallel_results"])
        
        successful_commands = sum(
            phase["successful_commands"] for phase in execution_results["phase_results"]
        )
        successful_commands += sum(
            1 for result in execution_results["parallel_results"] if result.get("success", False)
        )
        
        overall_success_rate = (successful_commands / total_commands * 100) if total_commands > 0 else 0
        
        execution_results["summary"] = {
            "total_duration": suite_duration,
            "total_commands": total_commands,
            "successful_commands": successful_commands,
            "overall_success_rate": overall_success_rate,
            "end_time": datetime.now().isoformat()
        }
        
        # Save comprehensive report
        report_file = self.save_comprehensive_report(execution_results)
        logger.info(f"üìÑ Comprehensive report saved: {report_file}")
        
        # Print summary
        self.print_execution_summary(execution_results)
        
        return execution_results


async def main():
    """Main execution function."""
    print("üéØ Testing Excellence Framework - Meta Tree Mind Map Integration")
    print("Hardware Optimized for 16-thread CPU, 32GB RAM, NVMe SSD")
    print("-" * 80)
    
    suite = ExcellenceTestingSuite()
    
    try:
        # Run complete testing suite
        results = await suite.run_complete_suite()
        
        # Final status
        success_rate = results["summary"]["overall_success_rate"]
        if success_rate >= 90:
            print("üèÜ EXCELLENT: Testing framework performing at excellence level!")
        elif success_rate >= 75:
            print("‚úÖ GOOD: Testing framework performing well with minor issues")
        elif success_rate >= 50:
            print("‚ö†Ô∏è  PARTIAL: Testing framework needs improvement")
        else:
            print("‚ùå CRITICAL: Testing framework requires immediate attention")
        
        return 0 if success_rate >= 75 else 1
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Testing suite interrupted by user")
        return 130
    except Exception as e:
        print(f"\n‚ùå Testing suite failed: {e}")
        logger.exception("Suite execution failed")
        return 1


if __name__ == "__main__":
    # Set environment variables for optimal performance
    os.environ["PYTEST_WORKERS"] = str(min(12, os.cpu_count() or 4))
    os.environ["NODE_OPTIONS"] = "--max-old-space-size=6144 --gc-interval=100 --optimize-for-size"
    
    # Run async main
    exit_code = asyncio.run(main())
    sys.exit(exit_code)