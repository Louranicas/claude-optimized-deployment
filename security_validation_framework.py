#!/usr/bin/env python3
"""
Security Validation Framework
Comprehensive testing and validation system for security controls
"""

import json
import subprocess
import datetime
import os
import re
import asyncio
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from enum import Enum

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ValidationResult(Enum):
    PASS = "PASS"
    FAIL = "FAIL"
    WARNING = "WARNING"
    NOT_TESTED = "NOT_TESTED"

class RiskLevel(Enum):
    CRITICAL = "CRITICAL"
    HIGH = "HIGH"
    MEDIUM = "MEDIUM"
    LOW = "LOW"
    INFO = "INFO"

@dataclass
class SecurityTest:
    name: str
    category: str
    description: str
    command: Optional[str]
    expected_result: str
    risk_level: RiskLevel
    compliance_frameworks: List[str]

@dataclass
class TestResult:
    test_name: str
    result: ValidationResult
    details: str
    evidence: str
    timestamp: datetime.datetime
    risk_level: RiskLevel
    remediation: str

class SecurityValidationFramework:
    """Comprehensive security validation and testing framework"""
    
    def __init__(self, project_root: str = "/home/louranicas/projects/claude-optimized-deployment"):
        self.project_root = Path(project_root)
        self.results_dir = self.project_root / "security_validation_results"
        self.results_dir.mkdir(exist_ok=True)
        
        # Initialize test suites
        self.security_tests: List[SecurityTest] = []
        self.compliance_tests: Dict[str, List[SecurityTest]] = {}
        self.test_results: List[TestResult] = []
        
        # Load test definitions
        self.initialize_security_tests()

    def initialize_security_tests(self):
        """Initialize comprehensive security test suite"""
        
        # OWASP Top 10 Tests
        owasp_tests = [
            SecurityTest(
                name="Access Control Validation",
                category="OWASP A01 - Broken Access Control",
                description="Validate RBAC implementation and permission controls",
                command="python3 -c \"from src.auth.permissions import verify_rbac_controls; verify_rbac_controls()\"",
                expected_result="All access controls properly implemented",
                risk_level=RiskLevel.CRITICAL,
                compliance_frameworks=["OWASP", "NIST", "ISO27001"]
            ),
            SecurityTest(
                name="Cryptographic Implementation Check",
                category="OWASP A02 - Cryptographic Failures",
                description="Verify cryptographic implementations and key management",
                command="python3 -c \"import cryptography; print('Cryptography version:', cryptography.__version__)\"",
                expected_result="Latest cryptography library with no vulnerabilities",
                risk_level=RiskLevel.CRITICAL,
                compliance_frameworks=["OWASP", "NIST", "ISO27001"]
            ),
            SecurityTest(
                name="Injection Prevention Validation",
                category="OWASP A03 - Injection",
                description="Test for SQL injection and command injection vulnerabilities",
                command="grep -r \"shell=True\" src/ || echo 'No shell=True found'",
                expected_result="No unsafe shell executions or SQL injections",
                risk_level=RiskLevel.CRITICAL,
                compliance_frameworks=["OWASP", "NIST"]
            ),
            SecurityTest(
                name="Security Design Review",
                category="OWASP A04 - Insecure Design",
                description="Validate security design patterns and threat modeling",
                command="find . -name '*threat*model*' -o -name '*security*design*'",
                expected_result="Security design documentation and threat models present",
                risk_level=RiskLevel.HIGH,
                compliance_frameworks=["OWASP", "NIST", "ISO27001"]
            ),
            SecurityTest(
                name="Security Configuration Audit",
                category="OWASP A05 - Security Misconfiguration",
                description="Audit security configurations across all components",
                command="find . -name '*.yaml' -o -name '*.json' | grep -i config",
                expected_result="All configurations follow security best practices",
                risk_level=RiskLevel.HIGH,
                compliance_frameworks=["OWASP", "NIST", "ISO27001"]
            ),
            SecurityTest(
                name="Vulnerable Components Scan",
                category="OWASP A06 - Vulnerable and Outdated Components",
                description="Scan for vulnerable dependencies and components",
                command="pip-audit --format=json --output=/tmp/pip_audit.json",
                expected_result="No critical or high vulnerabilities in dependencies",
                risk_level=RiskLevel.CRITICAL,
                compliance_frameworks=["OWASP", "NIST"]
            ),
            SecurityTest(
                name="Authentication Security Check",
                category="OWASP A07 - Identification and Authentication Failures",
                description="Validate authentication mechanisms and session management",
                command="python3 -c \"from src.auth.api import validate_jwt_implementation; validate_jwt_implementation()\"",
                expected_result="Strong authentication with MFA and secure session management",
                risk_level=RiskLevel.HIGH,
                compliance_frameworks=["OWASP", "NIST", "ISO27001"]
            ),
            SecurityTest(
                name="Software Integrity Validation",
                category="OWASP A08 - Software and Data Integrity Failures",
                description="Verify software integrity and supply chain security",
                command="find . -name '*.sig' -o -name 'SBOM*' -o -name '*integrity*'",
                expected_result="Software signatures and SBOM present",
                risk_level=RiskLevel.MEDIUM,
                compliance_frameworks=["OWASP", "NIST"]
            ),
            SecurityTest(
                name="Security Logging Audit",
                category="OWASP A09 - Security Logging and Monitoring Failures",
                description="Validate security logging and monitoring implementation",
                command="grep -r \"logger\\|log\\|audit\" src/ | wc -l",
                expected_result="Comprehensive security logging implemented",
                risk_level=RiskLevel.HIGH,
                compliance_frameworks=["OWASP", "NIST", "ISO27001"]
            ),
            SecurityTest(
                name="SSRF Protection Validation",
                category="OWASP A10 - Server-Side Request Forgery",
                description="Test for SSRF vulnerabilities and protections",
                command="grep -r \"requests\\.get\\|urllib\" src/ | grep -v \"# SSRF protected\"",
                expected_result="All external requests properly validated and protected",
                risk_level=RiskLevel.MEDIUM,
                compliance_frameworks=["OWASP"]
            )
        ]
        
        # NIST Cybersecurity Framework Tests
        nist_tests = [
            SecurityTest(
                name="Asset Inventory Validation",
                category="NIST Identify",
                description="Validate complete asset inventory and classification",
                command="find . -name '*inventory*' -o -name '*asset*'",
                expected_result="Complete asset inventory with security classifications",
                risk_level=RiskLevel.MEDIUM,
                compliance_frameworks=["NIST", "ISO27001"]
            ),
            SecurityTest(
                name="Risk Assessment Process",
                category="NIST Identify",
                description="Validate risk assessment processes and documentation",
                command="find . -name '*risk*' -o -name '*assessment*'",
                expected_result="Documented risk assessment processes",
                risk_level=RiskLevel.MEDIUM,
                compliance_frameworks=["NIST", "ISO27001"]
            ),
            SecurityTest(
                name="Protective Controls Implementation",
                category="NIST Protect",
                description="Validate implementation of protective security controls",
                command="python3 -c \"print('Checking protective controls...')\"",
                expected_result="All NIST protective controls implemented",
                risk_level=RiskLevel.HIGH,
                compliance_frameworks=["NIST"]
            ),
            SecurityTest(
                name="Detection Capabilities Assessment",
                category="NIST Detect",
                description="Assess security detection and monitoring capabilities",
                command="docker ps | grep -E \"siem|monitor|detect\" || echo 'No detection systems running'",
                expected_result="Active security detection and monitoring systems",
                risk_level=RiskLevel.HIGH,
                compliance_frameworks=["NIST"]
            ),
            SecurityTest(
                name="Incident Response Readiness",
                category="NIST Respond",
                description="Validate incident response procedures and readiness",
                command="find . -name '*incident*' -o -name '*response*'",
                expected_result="Documented incident response procedures",
                risk_level=RiskLevel.HIGH,
                compliance_frameworks=["NIST", "ISO27001"]
            ),
            SecurityTest(
                name="Recovery Capabilities Assessment",
                category="NIST Recover",
                description="Assess disaster recovery and business continuity capabilities",
                command="find . -name '*recovery*' -o -name '*backup*' -o -name '*continuity*'",
                expected_result="Documented recovery and continuity procedures",
                risk_level=RiskLevel.MEDIUM,
                compliance_frameworks=["NIST", "ISO27001"]
            )
        ]
        
        # Infrastructure Security Tests
        infrastructure_tests = [
            SecurityTest(
                name="Container Security Baseline",
                category="Infrastructure Security",
                description="Validate container security configurations",
                command="docker images --format \"table {{.Repository}}\\t{{.Tag}}\\t{{.Size}}\"",
                expected_result="All containers follow security best practices",
                risk_level=RiskLevel.HIGH,
                compliance_frameworks=["NIST", "CIS"]
            ),
            SecurityTest(
                name="Network Security Configuration",
                category="Infrastructure Security", 
                description="Validate network security configurations and segmentation",
                command="netstat -tuln | head -20",
                expected_result="Proper network segmentation and minimal exposed services",
                risk_level=RiskLevel.HIGH,
                compliance_frameworks=["NIST", "ISO27001"]
            ),
            SecurityTest(
                name="File System Permissions Audit",
                category="Infrastructure Security",
                description="Audit file system permissions for sensitive files",
                command="find src/ -name '*.py' -exec ls -la {} \\; | head -10",
                expected_result="Proper file permissions with no world-writable files",
                risk_level=RiskLevel.MEDIUM,
                compliance_frameworks=["CIS", "ISO27001"]
            ),
            SecurityTest(
                name="SSL/TLS Configuration Validation",
                category="Infrastructure Security",
                description="Validate SSL/TLS configurations and cipher suites",
                command="openssl version",
                expected_result="Latest SSL/TLS with secure cipher suites",
                risk_level=RiskLevel.HIGH,
                compliance_frameworks=["NIST", "ISO27001"]
            )
        ]
        
        # Application Security Tests
        application_tests = [
            SecurityTest(
                name="Static Code Analysis",
                category="Application Security",
                description="Run static code analysis for security vulnerabilities",
                command="bandit -r src/ -f json -o /tmp/bandit_results.json || echo 'Bandit scan completed'",
                expected_result="No high or critical security issues in code",
                risk_level=RiskLevel.HIGH,
                compliance_frameworks=["OWASP", "NIST"]
            ),
            SecurityTest(
                name="Dependency Security Audit",
                category="Application Security",
                description="Audit dependencies for known vulnerabilities",
                command="safety check --json --output /tmp/safety_results.json || echo 'Safety check completed'",
                expected_result="No vulnerable dependencies",
                risk_level=RiskLevel.CRITICAL,
                compliance_frameworks=["OWASP", "NIST"]
            ),
            SecurityTest(
                name="Secrets Detection Scan",
                category="Application Security",
                description="Scan for hardcoded secrets and credentials",
                command="grep -r -E \"(password|secret|key|token)\\s*=\\s*['\\\"][^'\\\"]+['\\\"]\" src/ | head -5",
                expected_result="No hardcoded secrets in source code",
                risk_level=RiskLevel.CRITICAL,
                compliance_frameworks=["OWASP", "NIST", "ISO27001"]
            ),
            SecurityTest(
                name="API Security Assessment",
                category="Application Security",
                description="Assess API security controls and authentication",
                command="grep -r \"@app\\.route\\|@api\" src/ | head -5",
                expected_result="All APIs properly secured with authentication",
                risk_level=RiskLevel.HIGH,
                compliance_frameworks=["OWASP", "NIST"]
            )
        ]
        
        # Combine all tests
        self.security_tests = owasp_tests + nist_tests + infrastructure_tests + application_tests
        
        # Organize by compliance framework
        self.compliance_tests = {
            "OWASP": [test for test in self.security_tests if "OWASP" in test.compliance_frameworks],
            "NIST": [test for test in self.security_tests if "NIST" in test.compliance_frameworks],
            "ISO27001": [test for test in self.security_tests if "ISO27001" in test.compliance_frameworks],
            "CIS": [test for test in self.security_tests if "CIS" in test.compliance_frameworks]
        }

    async def run_single_test(self, test: SecurityTest) -> TestResult:
        """Run a single security test"""
        logger.info(f"Running test: {test.name}")
        
        result = ValidationResult.NOT_TESTED
        details = ""
        evidence = ""
        remediation = ""
        
        try:
            if test.command:
                # Execute the test command
                process = await asyncio.create_subprocess_shell(
                    test.command,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    cwd=self.project_root
                )
                
                stdout, stderr = await process.communicate()
                
                output = stdout.decode() if stdout else ""
                error = stderr.decode() if stderr else ""
                
                evidence = f"Command: {test.command}\\nOutput: {output}\\nError: {error}"
                
                # Analyze results based on test type
                if "No shell=True found" in output and "Injection" in test.category:
                    result = ValidationResult.PASS
                    details = "No unsafe shell executions detected"
                elif "pip-audit" in test.command and process.returncode == 0:
                    result = ValidationResult.WARNING if "vulnerabilities" in output.lower() else ValidationResult.PASS
                    details = "Dependency audit completed"
                elif "bandit" in test.command:
                    result = ValidationResult.WARNING if process.returncode == 0 else ValidationResult.FAIL
                    details = "Static code analysis completed"
                elif "grep" in test.command and output.strip():
                    if "password\\|secret\\|key" in test.command:
                        result = ValidationResult.FAIL
                        details = f"Potential secrets found: {len(output.splitlines())} matches"
                        remediation = "Remove hardcoded secrets and implement proper secrets management"
                    else:
                        result = ValidationResult.PASS
                        details = f"Search completed: {len(output.splitlines())} matches"
                elif "find" in test.command:
                    file_count = len(output.splitlines()) if output.strip() else 0
                    result = ValidationResult.PASS if file_count > 0 else ValidationResult.WARNING
                    details = f"Found {file_count} relevant files"
                    if file_count == 0:
                        remediation = f"Implement required documentation for {test.category}"
                else:
                    result = ValidationResult.PASS if process.returncode == 0 else ValidationResult.FAIL
                    details = f"Command executed with return code {process.returncode}"
                    
                if result == ValidationResult.FAIL and not remediation:
                    remediation = f"Address issues identified in {test.category}"
                    
            else:
                result = ValidationResult.NOT_TESTED
                details = "Manual test - requires human validation"
                evidence = "No automated command available"
                
        except Exception as e:
            result = ValidationResult.FAIL
            details = f"Test execution failed: {str(e)}"
            evidence = f"Exception: {str(e)}"
            remediation = "Fix test execution environment and retry"
            logger.error(f"Test {test.name} failed with exception: {e}")
        
        return TestResult(
            test_name=test.name,
            result=result,
            details=details,
            evidence=evidence,
            timestamp=datetime.datetime.now(),
            risk_level=test.risk_level,
            remediation=remediation
        )

    async def run_test_suite(self, test_filter: Optional[str] = None) -> List[TestResult]:
        """Run complete security test suite"""
        logger.info("Starting comprehensive security validation")
        
        tests_to_run = self.security_tests
        if test_filter:
            tests_to_run = [test for test in self.security_tests if test_filter.lower() in test.category.lower()]
        
        # Run tests concurrently (but limit concurrency to avoid overwhelming the system)
        semaphore = asyncio.Semaphore(5)  # Max 5 concurrent tests
        
        async def run_with_semaphore(test):
            async with semaphore:
                return await self.run_single_test(test)
        
        self.test_results = await asyncio.gather(*[run_with_semaphore(test) for test in tests_to_run])
        
        logger.info(f"Completed {len(self.test_results)} security tests")
        return self.test_results

    def analyze_results(self) -> Dict[str, Any]:
        """Analyze test results and generate security assessment"""
        if not self.test_results:
            return {"error": "No test results available"}
        
        # Calculate statistics
        total_tests = len(self.test_results)
        passed = len([r for r in self.test_results if r.result == ValidationResult.PASS])
        failed = len([r for r in self.test_results if r.result == ValidationResult.FAIL])
        warnings = len([r for r in self.test_results if r.result == ValidationResult.WARNING])
        not_tested = len([r for r in self.test_results if r.result == ValidationResult.NOT_TESTED])
        
        # Calculate risk summary
        critical_failures = len([r for r in self.test_results if r.result == ValidationResult.FAIL and r.risk_level == RiskLevel.CRITICAL])
        high_failures = len([r for r in self.test_results if r.result == ValidationResult.FAIL and r.risk_level == RiskLevel.HIGH])
        
        # Calculate compliance scores
        compliance_scores = {}
        for framework, tests in self.compliance_tests.items():
            framework_results = [r for r in self.test_results if any(t.name == r.test_name for t in tests)]
            if framework_results:
                framework_passed = len([r for r in framework_results if r.result == ValidationResult.PASS])
                framework_total = len(framework_results)
                compliance_scores[framework] = round((framework_passed / framework_total) * 100, 1)
        
        # Determine overall security level
        if critical_failures > 0:
            security_level = "CRITICAL"
        elif high_failures > 3:
            security_level = "HIGH_RISK"
        elif (passed / total_tests) < 0.7:
            security_level = "MODERATE_RISK"
        elif (passed / total_tests) < 0.9:
            security_level = "LOW_RISK"
        else:
            security_level = "SECURE"
        
        return {
            "timestamp": datetime.datetime.now().isoformat(),
            "summary": {
                "total_tests": total_tests,
                "passed": passed,
                "failed": failed,
                "warnings": warnings,
                "not_tested": not_tested,
                "pass_rate": round((passed / total_tests) * 100, 1),
                "security_level": security_level
            },
            "risk_analysis": {
                "critical_failures": critical_failures,
                "high_failures": high_failures,
                "overall_risk": security_level
            },
            "compliance_scores": compliance_scores,
            "failed_tests": [
                {
                    "name": r.test_name,
                    "risk_level": r.risk_level.value,
                    "details": r.details,
                    "remediation": r.remediation
                }
                for r in self.test_results if r.result == ValidationResult.FAIL
            ],
            "recommendations": []  # Will be populated separately
        }

    def generate_recommendations(self, compliance_scores: Dict[str, float] = None) -> List[Dict[str, Any]]:
        """Generate security recommendations based on test results"""
        recommendations = []
        
        # Critical failures
        critical_failures = [r for r in self.test_results if r.result == ValidationResult.FAIL and r.risk_level == RiskLevel.CRITICAL]
        for failure in critical_failures:
            recommendations.append({
                "priority": "P0_CRITICAL",
                "category": "Critical Security Issue",
                "title": f"Remediate: {failure.test_name}",
                "description": failure.details,
                "action": failure.remediation or "Immediate remediation required",
                "timeline": "24-48 hours"
            })
        
        # High risk failures
        high_failures = [r for r in self.test_results if r.result == ValidationResult.FAIL and r.risk_level == RiskLevel.HIGH]
        for failure in high_failures[:3]:  # Top 3 high risk issues
            recommendations.append({
                "priority": "P1_HIGH",
                "category": "High Risk Security Issue",
                "title": f"Address: {failure.test_name}",
                "description": failure.details,
                "action": failure.remediation or "High priority remediation needed",
                "timeline": "1-2 weeks"
            })
        
        # Compliance improvements
        if compliance_scores:
            for framework, score in compliance_scores.items():
                if score < 80:
                    recommendations.append({
                        "priority": "P2_MEDIUM",
                        "category": "Compliance Improvement",
                        "title": f"Improve {framework} Compliance",
                        "description": f"Current compliance score: {score}%",
                        "action": f"Implement remaining {framework} controls",
                        "timeline": "2-4 weeks"
                    })
        
        return recommendations

    def export_results(self, format: str = "json") -> str:
        """Export test results in specified format"""
        analysis = self.analyze_results()
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if format.lower() == "json":
            filename = f"security_validation_{timestamp}.json"
            filepath = self.results_dir / filename
            
            export_data = {
                "analysis": analysis,
                "detailed_results": [asdict(result) for result in self.test_results]
            }
            
            with open(filepath, 'w') as f:
                json.dump(export_data, f, indent=2, default=str)
                
        elif format.lower() == "md":
            filename = f"security_validation_{timestamp}.md"
            filepath = self.results_dir / filename
            
            report = self.generate_markdown_report(analysis)
            
            with open(filepath, 'w') as f:
                f.write(report)
        
        return str(filepath)

    def generate_markdown_report(self, analysis: Dict[str, Any]) -> str:
        """Generate markdown security validation report"""
        
        report = f"""# Security Validation Report
Generated: {analysis['timestamp']}

## Executive Summary

**Overall Security Level**: {analysis['summary']['security_level']}  
**Test Pass Rate**: {analysis['summary']['pass_rate']}%  
**Critical Failures**: {analysis['risk_analysis']['critical_failures']}  
**High Risk Failures**: {analysis['risk_analysis']['high_failures']}  

## Test Summary

- **Total Tests**: {analysis['summary']['total_tests']}
- **Passed**: {analysis['summary']['passed']} ‚úÖ
- **Failed**: {analysis['summary']['failed']} ‚ùå
- **Warnings**: {analysis['summary']['warnings']} ‚ö†Ô∏è
- **Not Tested**: {analysis['summary']['not_tested']} ‚è∏Ô∏è

## Compliance Scores

"""
        
        for framework, score in analysis['compliance_scores'].items():
            status_emoji = "‚úÖ" if score >= 90 else "‚ö†Ô∏è" if score >= 70 else "‚ùå"
            report += f"- **{framework}**: {score}% {status_emoji}\\n"
        
        report += """
## Failed Tests

"""
        
        for failure in analysis['failed_tests']:
            risk_emoji = "üö®" if failure['risk_level'] == "CRITICAL" else "‚ö†Ô∏è" if failure['risk_level'] == "HIGH" else "‚ö°"
            report += f"""### {failure['name']} {risk_emoji}
- **Risk Level**: {failure['risk_level']}
- **Details**: {failure['details']}
- **Remediation**: {failure['remediation']}

"""
        
        report += """## Recommendations

"""
        
        for rec in analysis['recommendations']:
            priority_emoji = "üö®" if rec['priority'] == "P0_CRITICAL" else "‚ö†Ô∏è" if rec['priority'] == "P1_HIGH" else "‚ö°"
            report += f"""### {rec['title']} {priority_emoji}
- **Priority**: {rec['priority']}
- **Category**: {rec['category']}
- **Description**: {rec['description']}
- **Action**: {rec['action']}
- **Timeline**: {rec['timeline']}

"""
        
        return report

async def main():
    """Main function for running security validation"""
    print("üõ°Ô∏è  Security Validation Framework")
    print("=" * 50)
    
    # Initialize framework
    framework = SecurityValidationFramework()
    
    print(f"üìã Loaded {len(framework.security_tests)} security tests")
    print(f"üéØ Compliance frameworks: {list(framework.compliance_tests.keys())}")
    
    # Run test suite
    print("\\nüîç Running comprehensive security validation...")
    results = await framework.run_test_suite()
    
    # Analyze results
    analysis = framework.analyze_results()
    
    # Generate recommendations separately to avoid recursion
    analysis["recommendations"] = framework.generate_recommendations(analysis["compliance_scores"])
    
    # Display summary
    print(f"\\nüìä VALIDATION SUMMARY")
    print(f"Overall Security Level: {analysis['summary']['security_level']}")
    print(f"Test Pass Rate: {analysis['summary']['pass_rate']}%")
    print(f"Tests: {analysis['summary']['passed']} passed, {analysis['summary']['failed']} failed, {analysis['summary']['warnings']} warnings")
    
    print(f"\\nüö® RISK ANALYSIS")
    print(f"Critical Failures: {analysis['risk_analysis']['critical_failures']}")
    print(f"High Risk Failures: {analysis['risk_analysis']['high_failures']}")
    
    print(f"\\nüìã COMPLIANCE SCORES")
    for framework_name, score in analysis['compliance_scores'].items():
        status = "‚úÖ" if score >= 90 else "‚ö†Ô∏è" if score >= 70 else "‚ùå"
        print(f"{framework_name}: {score}% {status}")
    
    if analysis['failed_tests']:
        print(f"\\n‚ùå FAILED TESTS ({len(analysis['failed_tests'])})")
        for failure in analysis['failed_tests'][:5]:  # Show top 5
            print(f"- {failure['name']} ({failure['risk_level']})")
    
    print(f"\\n‚ö° TOP RECOMMENDATIONS")
    for rec in analysis['recommendations'][:3]:
        print(f"- [{rec['priority']}] {rec['title']}")
    
    # Export results
    json_file = framework.export_results("json")
    md_file = framework.export_results("md")
    
    print(f"\\nüìÑ Results exported:")
    print(f"  JSON: {json_file}")
    print(f"  Markdown: {md_file}")

if __name__ == "__main__":
    asyncio.run(main())