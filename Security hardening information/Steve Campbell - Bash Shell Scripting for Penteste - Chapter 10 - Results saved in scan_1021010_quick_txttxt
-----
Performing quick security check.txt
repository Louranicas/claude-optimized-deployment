# Security Chapter Extract
Book: Steve Campbell - Bash Shell Scripting for Pentesters (2024, Packt Publishing Pvt Ltd) - libgen.li
Chapter: 10 - Results saved in scan_1021010_quick_txt.txt
-----
Performing quick security check on 10.2.10.10 (ports: 80,445) with json output
Security check completed for 10.2.10.10. Results saved in scan_1021010_quick_json.json
-----
Performing thorough security check on 10.2.10.10 (ports: 80,445) with txt output
Security check completed for 10.2.10.10. Results saved in scan_1021010_thorough_txt.txt
-----
Performing thorough security check on 10.2.10.10 (ports: 80,445) with json output
Security check completed for 10.2.10.10. Results saved in scan_1021010_thorough_json.json
-----
Performing quick security check on 10.2.10.10 (ports: 22,3389) with txt output
Security check completed for 10.2.10.10. Results saved in scan_1021010_quick_txt.txt
-----
Performing quick security check on 10.2.10.10 (ports: 22,3389) with json output
Security check completed for 10.2.10.10. Results saved in scan_1021010_quick_json.json
-----
Security Relevance Score: 19
Word Count: 2216
Extracted: 2025-06-13 23:41:12

---

Results saved in scan_1021010_quick_txt.txt
-----
Performing quick security check on 10.2.10.10 (ports: 80,445) with json output
Security check completed for 10.2.10.10. Results saved in scan_1021010_quick_json.json
-----
Performing thorough security check on 10.2.10.10 (ports: 80,445) with txt output
Security check completed for 10.2.10.10. Results saved in scan_1021010_thorough_txt.txt
-----
Performing thorough security check on 10.2.10.10 (ports: 80,445) with json output
Security check completed for 10.2.10.10. Results saved in scan_1021010_thorough_json.json
-----
Performing quick security check on 10.2.10.10 (ports: 22,3389) with txt output
Security check completed for 10.2.10.10. Results saved in scan_1021010_quick_txt.txt
-----
Performing quick security check on 10.2.10.10 (ports: 22,3389) with json output
Security check completed for 10.2.10.10. Results saved in scan_1021010_quick_json.json
-----


     This command structure is particularly useful when you need to process data from multiple sources in combination, allowing for complex parallel
    


      processing tasks.
     




     Now that you’ve been introduced to both
    


      xargs
     


     and parallel, I’ll explain in the next section when to choose
    



     one over
    


      the other.
     






     Comparing xargs and parallel
    



     What are the key differences
    



     between
    


      xargs
     


     and parallel and how do you know when
    



     either one is the right tool for the job?
    

     The following table should help you choose the right tool for
    


      the job:
     















           Aspect
          









           xargs
          









           GNU parallel
          











           Execution
          







         Serial by default.
        

         Can run parallel with the
        


          -P
         


         option, but is
        


          less flexible.
         






         Designed for efficient parallel execution out of
        


          the box.
         










           Complexity
          







         Simpler, lightweight.
        

         Good for
        


          straightforward tasks.
         






         Feature-rich.
        

         Handles complex scenarios, job control, and
        


          load balancing.
         










           Error handling
          







         Basic.
        

         May stop
        


          on errors.
         






         Robust.
        

         Can continue
        


          despite failures.
         










           Availability
          







         Installed by default on most
        


          Unix systems.
         






         Requires
        


          separate installation.
         








     Table 7.1 – A comparison of xargs and parallel features
    



     Having learned how Bash parallel processing works, in the next section, we’ll explore using these concepts in
    


      practical applications.
     






     Achieving parallelism using screen
    



     The
    


      screen
     


     command is a
    



     Linux utility that allows users to
    



     manage multiple terminal sessions within a single window.
    

     It’s particularly useful for running long processes, managing remote sessions, and achieving parallelism in
    


      Bash scripts.
     




     Before proceeding, ensure you have
    


      screen
     


     installed by running the
    


      following command:
     



$ sudo apt update && sudo apt install -y screen


     Here’s how you can use
    


      screen
     


     to run multiple tasks in parallel.
    

     You can find the code in the book’s GitHub
    



     repository as
    


      ch07_screen_1.sh
     


     :
    


perform_task() {
    echo "Starting task $1"
    sleep 5  # Simulating work
    echo "Finished task $1"
}


     The
    


      perform_task
     


     function simply
    



     sleeps for five seconds to simulate
    


      performing work.
     




     The following code creates a new detached
    


      screen
     


     session
    


      named
     




       parallel_tasks
      




      :
     



screen -dmS parallel_tasks


     The
    


      -d
     


     flag starts the session in detached mode, and
    


      m
     


     creates a
    


      new session.
     



for i in {1..5}; do
    screen -S parallel_tasks -X screen -t "Task $i" bash -c "perform_task $i; exec bash"
done


     The preceding
    


      for
     


     loop starts multiple tasks in separate
    


      screen
     


     windows.
    

     This command creates a new window within the
    


      parallel_tasks
     


     session.
    

     The
    


      -X
     


     flag sends a command to the session,
    


      screen
     


     creates a new window,
    


      -t
     


     sets the window title, and
    


      bash -c
     


     executes the specified command in the
    


      new window.
     



screen -S parallel_tasks -X windowlist -b


     The preceding command waits for all windows in the session to close.
    

     It’s useful for synchronizing the completion of
    


      parallel tasks.
     



screen -S parallel_tasks -X quit


     The preceding command terminates the entire screen session once all tasks
    


      are complete.
     




     Now that we have a solid
    



     foundation on the use of
    


      xargs
     


     ,
    


      parallel
     


     , and
    


      screen
     


     , let’s move on to the next section and look at some
    



     practical applications and review best practices for
    


      their use.
     






     Practical applications and best practices
    



     This section will further solidify your understanding of parallel processing in Bash by showing practical applications.
    

     This will be followed by best practices to help you get the most out of learning
    


      these concepts.
     






     Practical applications of Bash parallel processing
    



     In this section, we will use
    



     examples to show the real-world usage of Bash parallel processing
    


      in pentesting.
     




     The first example uses
    



     GNU parallel for
    


      SQL injection
     


     testing, as shown in the
    


      followin
     





      g code:
     



#!/usr/bin/env bash
urls=("http://example.com/login.php" "http://test.com/index.php" "http://site.com/search.php")
echo "${urls[@]}" | parallel -j 3 'sqlmap -u {} --batch --crawl=2'
echo "All SQL injection tests completed."


     The code can be found in the book’s GitHub repository as
    


      ch07_parallel_2.sh
     


     .
    

     Here’s
    


      an explanation:
     






       urls
      


      is an array of URLs
     


       to test
      





       echo "${urls[@]}"
      


      outputs the list
     


       of URLs
      





       parallel -j 3 'sqlmap -u {} --batch --crawl=2'
      


      runs
     


       sqlmap
      


      on each URL with up to three
     


       concurrent jobs
      





     The next example shows
    



     how to do network TCP port scanning in parallel, as
    


      sho
     





      wn here:
     



#!/usr/bin/env bash
ips=$(seq 1 254 | awk '{print "192.168.1." $1}')
echo "$ips" | xargs -n 1 -P 10 -I {} bash -c 'nmap -sP {}'
echo "Network scan completed."


     The code can be found in the book’s GitHub repository as
    


      ch07_xargs_1.sh
     


     .
    

     Here’s
    


      an explanation:
     






       seq 1 254 | awk '{print "192.168.1."
      

       $1}'
      


      generates IP addresses from
     


       192.168.1.1
      



       to
      




        192.168.1.254
       




       .
      





       echo "$ips"
      


      outputs the list
     






       of IPs.
      





       xargs -n 1 -P 10 -I {} bash -c 'nmap -sP {}'
      


      runs
     


       nmap
      


      ’s ping scan (
     


       -sP
      


      ) on each IP, with up to 10 parallel jobs.
     

      The
     


       -n 1
      


      option tells
     


       xargs
      


      to use, at most, one argument per command line.
     

      In this context, it means that
     


       xargs
      


      will run the
     


       nmap
      


      command once for each IP address or hostname it receives
     


       as input.
      





     While the preceding is an example of performing port scanning in parallel,
    


      nmap
     


     already has this capability.
    

     Therefore, let’s explore how to do this in Bash.
    

     You may find yourself in a shell on a system you have exploited and can’t install tools such as
    


      nmap
     


     for one reason or another so you should be prepared to use the system as a pivot into
    


      other networks.
     




     The following Bash script has no external dependencies and scans for live hosts, then port-scans the top 100 TCP ports.
    

     It’s not nearly as fast as it could be if
    


      xargs
     


     or
    


      parallel
     


     were used.
    

     Just keep in mind that, someday, you’ll need something that doesn’t require any external dependencies and you can’t be assured that
    


      xargs
     


     and
    


      parallel
     


     will always be available.
    

     This script should work anywhere with Bash and the
    



       ping
      




      appl
     





      ication:
     



#!/usr/bin/env bash
IP_RANGE="10.2.10.{1..20}"
PORTS=(21 22 23 25 53 80 110 143 443 587 3306 3389 5900 8000 8080 9000 49152 49153 49154 49155 49156 49157 49158 49159 49160 49161 49162 49163 49164 49165 49166 49167 49168 49169 49170 49171 49172 49173 49174 49175 49176 49177 49178 49179 49180 49181 49182 49183 49184 49185 49186 49187 49188 49189 49190 49191 49192 49193 49194 49195 49196 49197 49198 49199 49200 49201 49202 49203 49204 49205 49206 49207 49208 49209 49210 49211 49212 49213 49214 49215 49216 49217 49218 49219 49220 49221 49222 49223 49224 49225 49226 49227 49228 49229 49230 49231)
LIVE_HOSTS=()
for IP in $(eval echo $IP_RANGE); do
    if ping -c 1 -W 1 $IP > /dev/null 2>&1; then
        LIVE_HOSTS+=($IP)
    fi
done
scan_ports() {
    local IP=$1
    for PORT in "${PORTS[@]}"; do
        (echo >/dev/tcp/$IP/$PORT) > /dev/null 2>&1 && echo "$IP:$PORT"
    done
}
# Export the function to use in subshells
export -f scan_ports
# Loop through live hosts and scan ports in parallel
for IP in "${LIVE_HOSTS[@]}"; do
    scan_ports $IP &
done
echo "Waiting for port scans to complete…"
wait


     The code can be found in the book’s GitHub repository as
    


      ch07_no_dependencies_scan.sh
     


     .
    

     Here’s
    


      an explanation:
     






       #!/usr/bin/env bash
      


      : The usual
     


       shebang
      


      that we’ve
     



      covered in earlier chapters.
     

      This basically tells the
     



      shell what program to use to execute the
     


       following code.
      





       IP_RANGE
      


      : Defines the range of IP addresses to scan using brace expansion (
     


       {1..20}
      


      ), which denotes the last octet ranging from 1 to 20 for the base
     


       IP
      




        192.168.1
       




       .
      





       PORTS
      


      : An array holding the
     


       nmap
      


      top 100
     


       TCP ports.
      





       LIVE_HOSTS
      


      : An empty array to store the IP addresses of live hosts that respond
     


       to pings.
      





       for IP in $(eval echo $IP_RANGE)
      


      : Iterates through the expanded list of
     


       IP addresses.
      





       ping -c 1 -W 1 $IP > /dev/null 2>&1
      


      : Sends one ICMP echo request (
     


       -c 1
      


      ) with a 1-second timeout (
     


       -W 1
      


      ) to check whether the host is up.
     

      The output is redirected to
     


       /dev/null
      


      to
     


       suppress it.
      





       LIVE_HOSTS+=($IP)
      


      : Adds the IP address to the
     


       LIVE_HOSTS
      


      array if the host
     


       is up.
      





       scan_ports $IP
      


      : A function that takes an IP address as
     


       an argument.
      





       (echo >/dev/tcp/$IP/$PORT) > /dev/null 2>&1
      


      : Attempts to open a TCP connection to the specified port on the IP address.
     

      If successful, it prints the IP address
     


       and port.
      





       Export the function
      


      : Using
     


       export -f scan_ports
      


      allows the function to be used
     


       in subshells.
      





       for IP in "${LIVE_HOSTS[@]}"
      


      : Iterates through the list of
     


       live hosts.
      





       scan_ports $IP &
      


      : Calls the
     


       scan_ports
      


      function in the background for each IP address, allowing
     


       concurrent execution.
      





       wait
      


      : Waits for all background jobs to
     



      complete before exiting
     


       the script.
      





     The script checks 20 consecutive IP addresses for live hosts and then scans the top 100 TCP ports and completes in 10 seconds on
    


      my system:
     











     Figure 7.2 – A Bash TCP port scanner that should work on any system
    



     Here’s an example of downloading multiple files
    


      in parallel:
     



$ parallel -j 3 wget ::: http://example.com/file1 http://example.com/file2 http://example.com/file3


     Here is
    


      the explanation:
     






       parallel -j 3
      


      : Executes three
     


       parallel jobs
      





       wget :::
      


      : The three URLs following the series of colon characters are
     


       the input
      





     This command downloads
    



     three files concurrently
    


      using
     




       wget
      




      .
     






     Best practices for parallel execution in Bash
    



     This section explores best practices for using
    


      xargs
     


     and
    


      parallel
     


     to execute tasks concurrently, leveraging the full potential of your
    


      system’s resources.
     




     The following are the best
    



     practices for
    


      parallel execution:
     






       Determine the optimal number of jobs
      


      : The ideal number of parallel jobs depends on your system’s CPU and memory capacity.
     

      Start with the number of CPU cores and adjust based on performance.
     

      If you don’t specify a number of jobs, the defaults are one job for
     


       xargs
      


      and one job per CPU core for
     


       GNU parallel.
      





       Monitor resource usage
      


      : Use tools such as
     


       htop
      


      or
     


       vmstat
      


      to monitor CPU and memory usage during parallel execution, ensuring your system remains responsive.
     

      See the man entry for these tools
     


       for examples.
      





       Make a dry run
      


      : You can check what will be run with parallel by including the
     


       --
      




        dry-run
       




       option.
      





       Handle errors gracefully
      


      : Both
     


       xargs
      


      and GNU parallel can capture and log errors.
     

      Use these features to identify and debug issues without halting the
     


       entire process.
      





       Redirect output appropriately
      


      : Redirect the output of each job to separate files or a log system to avoid interleaved and
     


       confusing outputs.
      





       Use meaningful job names
      


      : When using GNU Parallel, you can assign meaningful names to jobs to easily track
     


       their progress.
      





     Parallel execution with
    


      xargs
     


     and GNU parallel can vastly improve the efficiency of Bash scripts, particularly in cybersecurity and pentesting tasks.
    

     By following best practices such as optimizing job numbers, monitoring resources, handling errors, and managing output, you can
    



     harness the full potential of parallel processing to enhance your scripts
    


      and workflows.
     






     Summary
    



     In this chapter, we learned about parallel processing techniques in Bash scripting.
    

     This helped you gain knowledge on the basics of parallel execution using background processes and job control.
    

     We also learned about advanced parallel processing using tools such as
    


      xargs
     


     and GNU parallel and covered managing errors and output in parallel tasks.
    

     The chapter also covered applying parallel processing to
    


      pentesting workflows.
     




     This chapter will help you significantly speed up tasks that involve processing large amounts of data or executing multiple commands simultaneously.
    

     Parallel processing can greatly reduce the time required for network scans, brute-force attacks, or analyzing multiple targets concurrently.
    

     Understanding how to manage parallel tasks helps in creating more efficient and robust scripts for various pentesting scenarios.
    

     The skills learned can be applied to optimize resource usage and improve overall productivity during
    


      security assessments.
     




     By mastering parallel processing in Bash, pentesters can create more powerful and efficient scripts, allowing them to handle complex tasks and large-scale assessments
    


      more effectively.
     




     In the next chapter,
    





     we dive into
    


      Part 2,
     


     where we put all of the Bash goodness that you’ve learned to work for
    


      a pentest.
     















     Part 2: Bash Scripting for Pentesting
    



     In this part, you will apply your foundational Bash scripting knowledge to real-world pentesting scenarios.
    

     Starting with reconnaissance and information gathering, you will learn to automate the discovery of target assets, including DNS enumeration, subdomain mapping, and OSINT collection through Bash scripts.
    

     The section then progresses into web application testing, where you will develop scripts for automating HTTP requests, analyzing responses, and identifying common web vulnerabilities.
    

     Moving deeper into infrastructure testing, you will create scripts for network scanning, service enumeration, and vulnerability assessment automation.
    

     The focus then shifts to post-exploitation techniques, with chapters dedicated to privilege escalation scripting, maintaining persistence, and network pivoting – all orchestrated through Bash.
    

     The section concludes with a comprehensive look at pentest reporting automation, teaching you how to transform raw tool outputs and findings into professional, actionable reports using Bash scripts.
    

     Throughout
    


      Part 2
     


     , each chapter builds upon the previous ones, culminating in a complete toolkit of custom Bash scripts for conducting
    


      thorough pentests.
     




     This part has the
    


      following chapters:
