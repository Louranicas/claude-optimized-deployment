# Security Chapter Extract
Book: Steve Campbell - Bash Shell Scripting for Pentesters (2024, Packt Publishing Pvt Ltd) - libgen.li
Chapter: 1 - It effectively skips the first line (header) of the
Security Relevance Score: 13
Word Count: 3987
Extracted: 2025-06-13 23:41:12

---

It effectively skips the first line (header) of the
     


       CSV file.
      





       {...}
      


      : This block contains the main processing logic for each line that meets the
     



        NR>1
       




       condition.
      





       gsub(/\r/,"")
      


      : This function globally substitutes (
     


       gsub
      


      ) any carriage return characters (
     


       \r
      


      ) with an empty string, effectively removing them from the line.
     

      This helps handle potential Windows-style
     


       line endings.
      





       if($3!="")
      


      : This condition checks whether the third field (severity level) is empty
     


       or not.
      





       count[$3]++
      


      : If the condition is
     


       true
      


      , this increments the count for the severity level found in the third field.
     

      It uses an associative array named
     


       count
      


      with the severity level as
     


       the key.
      





       END {...}
      


      : This block specifies actions to be performed after processing
     


       all lines.
      





       for (severity in count)
      


      : This loop iterates over all unique severity levels stored as keys in the
     



        count
       




       array.
      





       print severity ": " count[severity]
      


      : For each severity level, this prints the severity followed by a colon and space, then the count
     


       of occurrences.
      





       vulnerability_scan.csv
      


      : This is the
     



      input file that the
     



      AWK
     


       command processes.
      





     In summary, this AWK command reads a CSV file, skips the header, removes carriage returns, counts the occurrences of each non-empty severity level, and then prints out a summary of these counts.
    

     It’s designed to handle potential issues such as Windows line endings and empty fields, making it more robust for processing real-world
    


      CSV data.
     




     In another example, we may need to combine multiple files.
    

     Here we have another
    


      fil
     







      e,
     




       asset_info.csv
      




      :
     



IP,Owner,Department
192.168.1.1,John,IT
192.168.1.10,Alice,Marketing
192.168.1.20,Bob,Finance
192.168.1.30,Carol,HR


     We can combine this with our
    


      v
     







      ul
     





      nerability data:
     



$ awk -F',' 'NR==FNR {owner[$1]=$2; dept[$1]=$3; next}{print $11 "," $2 "," $3 "," owner[$1] "," dept[$1]}' asset_info.csv vulnerability_scan.csv


     This is the
    


      resultant output:
     



IP,Vulnerability,Severity,Owner,Department
192.168.1.1,SQL Injection,High,John,IT
192.168.1.1,XSS,Medium,John,IT
192.168.1.10,Outdated SSL,Low,Alice,Marketing
192.168.1.20,Weak Password,High,Bob,Finance
192.168.1.30,Information Disclosure,Medium,Carol,HR


     This script first
    



     reads
    


      asset_info.csv
     


     into memory, then processes
    


      vulnerability_scan.csv
     


     , adding the owner and department information to each
    



     line.
    

     Let’s look at
    


      the explanation:
     






       -F','
      


      : This option sets the field separator to a comma, which is appropriate for
     


       CSV files.
      





       NR==FNR
      


      : This condition is true only when processing the first file (
     


       asset_info.csv
      


      ).
     


       NR
      


      is the current record number across all files, while
     


       FNR
      


      is the record number in the
     


       current file.
      





       {owner[$1]=$2; dept[$1]=$3; next}
      


      : This block executes
     


       for
      




        asset_info.csv
       




       :
      





         owner[$1]=$2
        


        : Creates an associative array,
       


         owner
        


        , where the key is the first field (an asset ID) and the value is the second field (
       


         owner name)
        





         dept[$1]=$3
        


        : Creates an associative array,
       


         dept
        


        , where the key is the first field and the value is the third field (
       


         department name)
        





         next
        


        : Skips to the next record without executing th
       







        e rest of
       


         the script
        







       {print $1 "," $2 "," $3 "," owner[$1] "," dept[$1]}
      


      : This block executes
     


       for
      




        vulnerability_scan.csv
       




       :
      




        Prints the first three fields from the current line
       


         of
        




          vulnerability_scan.csv
         





        Adds the owner and department information by looking up the first field (asset ID) in the
       


         owner
        


        and
       



          dept
         




         arrays
        







       asset_info.csv vulnerability_scan.csv
      


      : These are the input files.
     


       asset_info.csv
      


      is processed first,
     


       then
      




        vulnerability_scan.csv
       




       .
      





     These examples demonstrate how
    


      awk
     


     can be used to process and analyze CSV data from pentesting
    



     activities.
    

     By combining these techniques, you can
    



     create powerful scripts to automate data parsing and report generation for your
    


      pentest findings.
     




     Bash provides several tools that can be used to parse XML data.
    

     We’ll focus on using
    


      xmllint
     


     and
    


      xpath
     


     , which are commonly available on
    


      Linux systems.
     




     First, let’s take a look at the structure of our Nmap XML report.
    

     The Nmap XML file can be found in this chapter’s GitHub repository as
    


      nmap.xml
     


     .
    

     The following is the abbreviated content of this file, showing the
    


      XML nodes:
     



<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE nmaprun>
<nmaprun scanner="nmap" ...>
  <scaninfo .../>
  <verbose .../>
  <debugging .../>
  <host ...>
    <status .../>
    <address .../>
    <hostnames>...</hostnames>
    <ports>
      <port ...>
        <state .../>
        <service .../>
        <script .../>
      </port>
      ...
    </ports>
    ...
  </host>
  ...
</nmaprun>


     Let’s extract all the IP addresses from the scan using
    





     the
    


      following command:
     



$ xmllint --xpath "//host/address[@addrtype='ipv4']/@addr" nmap.xml


     I strongly recommend you have the
    


      nmap.xml
     


     file open in GitHub and compare it to the following explanation as we step
    


      through it.
     




     This command uses
    



     XPath to select all
    


      addr
     


     attributes of
    


      address
     


     elements
    



     that are children of
    


      host
     


     elements and have an
    


      addrtype
     


     of
    


      ipv4
     


     .
    

     With this information in mind, go back and read the XML data again to see this
    


      XML structure.
     




     Here’s
    


      the explanation:
     






       //host
      


      : This selects all host elements anywhere in
     


       the document.
      





       /address
      


      : This selects address elements that are direct children of the
     


       host elements.
      





       [@addrtype='ipv4']
      


      : This is a predicate that filters for address elements where the
     


       addrtype
      


      attribute
     


       equals
      




        ipv4
       




       .
      





       /@addr
      


      : This selects the
     


       addr
      


      attribute of the matching
     


       address elements.
      





     The output can be seen in the
    


      following figure:
     











     Figure 13.1 – The output of the Nmap XML filter
    



     Let’s create a
    



     more complex filter using two criteria from the Nmap XML
    



     data.
    

     We’ll find all hosts that have port
    


      80
     


     open and are running Microsoft IIS.
    

     This combines filtering on port status and
    


      service information.
     




     H
    







     ere’s how we can
    


      do this:
     



$ xmllint --xpath "//host[ports/port[@portid='80' and state/@state='open' and service/@product='Microsoft IIS httpd']]/address[@addrtype='ipv4']/@addr" nmap.xml


     Here, you can see the output of the
    


      preceding command:
     











     Figure 13.2 – The output of the command
    



     As you can see, this is as simple as combining multiple XML queries separated by
    


      and
     


     .
    

     If you want to include ports
    


      80
     


     or
    


      443
     


     , separate them with an
    



       or
      




      keyword.
     




     Next, let’s examine how to parse JSON data.
    

     In these examples, I’m using the
    


      mapcidr
     


     and
    


      httpx
     


     tools from ProjectDiscovery.
    

     My lab network has the network address
    


      10.2.10.0/24
     


     .
    

     I run the following command to fingerprint HTTP/S se
    





     rvers on my
    


      lab network:
     



echo 10.2.10.0/24 | mapcidr -silent | httpx -silent -j > httpx.json


     The
    


      httpx.json
     


     file can be found in this chapter’s directory in the
    


      GitHub repository.
     




     Let’s look at
    


      the explanation:
     






       echo 10.2.10.0/24 |
      


      : This simply sends the
     


       10.2.10.0/24
      


      string into the pipeline (
     


       |
      


      ) and suppresses the program’s
     


       banner (
      




        -silent
       




       ).
      





       mapcidr -silent |
      


      : This takes the input, expands it into individual IP addresses, and passes them into
     


       the pipeline.
      





       httpx -silent -j
      


      : This takes the IP addresses passed in as
     


       stdin
      


      input, fingerprints any webservers listening on default ports, and prints the output in
     


       JSON format.
      





     The following
    



     shows the abbreviated
    



     output of
    


      this command:
     



{"timestamp":"2024-08-24T17:17:41.515583292-04:00","port":"80","url":"http://10.2.10.10","input":"10.2.10.10","title":"IIS Windows Server","scheme":"http","webserver":"Microsoft-IIS/10.0","content_type":"text/html","method":"GET","host":"10.2.10.10","path":"/","time":"91.271935ms","a":["10.2.10.10"],"tech":["IIS:10.0","Microsoft ASP.NET","Windows Server"],"words":27,"lines":32,"status_code":200,"content_length":703,"failed":false,"knowledgebase":{"PageType":"nonerror","pHash":0}}


     The first thing you should do when examining JSON data structures is view the hierarchy by passing the data to
    


      jq .
     


     .
    

     The following example command uses JSON to output all data in the file in a format that’s easier to read to determine how the data
    


      is structured:
     



cat httpx.json | jq .


     The abbreviated output of this command can be seen in the
    


      following figure:
     











     Figure 13.3 – The JSON data structure from httpx
    



     The following script
    



     parses this JSON data and outputs each field, one per
    



     line.
    

     Let’s examine each line of the script to learn how to parse the JSON fields.
    

     This script can be found in this chapter’s GitHub repository
    


      as
     




       ch13_parse_httpx.sh
      




      :
     



#!/usr/bin/env bash
# Function to parse a single JSON object
parse_json() {
    local json="$1"
    # Extract specific fields
    local timestamp=$(echo "$json" | jq -r '.timestamp')
    local url=$(echo "$json" | jq -r '.url')
    local title=$(echo "$json" | jq -r '.title')
    local webserver=$(echo "$json" | jq -r '.webserver')
    local status_code=$(echo "$json" | jq -r '.status_code')
    # Print extracted information
    echo "Timestamp: $timestamp"
    echo "URL: $url"
    echo "Title: $title"
    echo "Web Server: $webserver"
    echo "Status Code: $status_code"
    echo "---"
}
# Read JSON objects line by line
while IFS= read -r line; do
    parse_json "$line"
done


     The
    



     output is
    



     shown in the
    


      following figure:
     











     Figure 13.4 – The output of script ch13_parse_httpx.sh
    



     Now, let’s discuss how to adapt this lesson to any
    


      JSON output:
     






       Identify the structure
      


      : First, examine your JSON output to understand its structure.
     

      Look for the key fields you want
     


       to extract.
      





       Modify the
      



       parse_json
      



       function
      


      : Update the function to extract the fields specific to your JSON structure.
     

      For example, if your JSON has a field called
     


       user_name
      


      , you add
     


       the following:
      


local user_name=$(echo "$json" | jq -r '.user_name')



      Modify the
     


       echo
      


      statements to print the fields you’ve extracted.
     

      If your JSON contains nested objects or arrays, you can use more complex
     


       jq
      


      queries.
     

      Here’s
     


       an example:
      


local first_tech=$(echo "$json" | jq -r '.tech[0]')




     Before examining the preceding code, take a look at
    



       Figure 13
      




      .3
     


     and find the
    


      tech
     


     node.
    

     Using
    


      .tech[0]
     


     , we selected and returned the first result in the array.
    

     If you wanted to return all array results, you would instead use
    


      .tech[]
     


     , which is the
    


      whole array.
     




     The following are
    



     quick tips to help you pa
    





     rse nested JSON data
    




      with
     




       jq
      




      :
     





      For nested objects, use dot
     


       notation:
      




        .parent.child
       




       .
      




      For arrays, use
     


       brackets:
      




        .array[]
       




       .
      




      Combine these for deeply nested
     


       structures:
      




        .parent.array[].child
       




       .
      





     Let’s expand on how to select nested data with examples.
    

     Review the following JSON data before
    


      moving on:
     



{
  "parent": {
    "name": "Family Tree",
    "child": {
      "name": "John",
      "age": 10
    },
    "siblings": [
      {
        "child": {
          "name": "Emma",
          "age": 8
        }
      },
      {
        "child": {
          "name": "Michael",
          "age": 12
        }
      }
    ]
  }
}


     Next, let’s
    



     examine some example
    


      jq
     


     queries for this
    


      nested
     







      structure:
     





      Get the parent name:
     



        jq '.parent.name'
       




       Output:
      


        "
       




         Family Tree"
        






      Get direct child’s name:
     



        jq '.parent.child.name'
       





        Output:
       




         "John"
        






      Get all sibling names (array traversal):
     



        jq '.parent.siblings[].child.name'
       




       Output:
      


        "
       




         Emma" "Michael"
        






      Get all ages from both direct child and siblings:
     


       jq '.
      




        parent.child.age, .parent.siblings[].child.age'
       




       Output:
      


        10
       




         8 12
        







     Armed with the knowledge needed to parse common pentest tool report formats, you’re prepared for the next step.
    

     In the next section, you’ll learn how to store data parsed from pentest tool reports into
    


      SQLite databases.
     






     Storing and managing pentest data with SQLite
    



     SQLite
    



     is a lightweight, serverless database engine that provides an efficient way to store and manage data collected during pentesting.
    

     This section explores how to leverage SQLite in conjunction with Bash scripting to create a system for organizing and querying
    


      pentest findings.
     




     SQLite offers several advantages
    


      for pentesters:
     






       Portability
      


      : SQLite
     



      databases are self-contained files, making them easy to transfer and
     


       back up.
      





       No setup required
      


      : Unlike full-fledged database servers, SQLite doesn’t need installation
     


       or configuration.
      





       Efficient querying
      


      : SQLite supports SQL, allowing for complex data retrieval
     


       and analysis.
      





       Language integration
      


      : Many programming languages, including Bash through command-line tools, can interact with
     


       SQLite databases.
      





     In this section, we’ll cover the
    


      following topics:
     





      How to create SQLite databases using
     


       Bash commands?
      




      How to write scripts that parse tool output and insert data into
     


       SQLite tables?
      




      How to run queries on SQLite databases to generate
     


       report content?
      





     By combining Bash
    



     scripting with SQLite, pentesters can create a flexible and powerful system for managing test data and streamlining the
    


      reporting process.
     




     First, let’s create an
    


      SQLite3
     


     database to store our Nmap scan results.
    

     The following script can be found in
    





     this chapter’s GitH
    





     ub repository
    


      as
     




       ch13_create
      





       _db.sh
      




      :
     



#!/usr/bin/env bash
DB_NAME="pentest_results.db"
sqlite3 $DB_NAME <<EOF
CREATE TABLE IF NOT EXISTS nmap_scans (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    ip_address TEXT,
    hostname TEXT,
    port INTEGER,
    protocol TEXT,
    service TEXT,
    version TEXT,
    scan_date DATETIME DEFAULT CURRENT_TIMESTAMP,
    vulnerability TEXT
);
EOF


     Here’s
    


      the explanation:
     





      First, it
     



      defines the database name
     


       as
      




        pentest_results.db
       




       .
      




      Then, it uses a heredoc (
     


       <<EOF
      


      ) to pass SQL commands
     


       to
      




        SQLite3
       




       .
      




      Next, it creates a table named
     


       nmap_scans
      


      if it doesn’t
     


       already exist.
      




      Lastly, it defines columns for IP address, hostname, port, protocol, service, version, scan date,
     


       and vulnerability.
      





     Next, let’s create a script that takes an Nmap scan as input and inserts the results into our database.
    

     The following script can be found in
    





     this chapter’s GitH
    





     ub repository
    


      as
     




       ch13_nmap_to_db.sh
      




      :
     



#!/usr/bin/env bash
DB_NAME="pentest_results.db"
xmlstarlet sel -t -m "//host" \
    -v "address/@addr" -o "|" \
    -v "hostnames/hostname/@name" -o "|" \
    -m "ports/port" \
        -v "@portid" -o "|" \
        -v "@protocol" -o "|" \
        -v "service/@name" -o "|" \
        -v "service/@version" -n \
    "$1" | while IFS='|' read -r ip hostname port protocol service version; do
    sqlite3 $DB_NAME <<EOF
INSERT INTO nmap_scans (ip_address, hostname, port, protocol, service, version)
VALUES ('$ip', '$hostname', '$port', '$protocol', '$service', '$version');
EOF
done


     Execute the script
    


      as follows:
     



$ ./ch13_nmap_to_db.sh nmap.xml


     When the script
    



     completes, it will print
    


      Data import completed
     


     to
    


      the terminal.
     




     Let’s look at
    


      the explanation:
     





      The
     


       DB_NAME
      


      variable defines the
     


       database name.
      




      It uses
     


       xmlstarlet
      


      to parse the XML Nmap report, extracting
     


       relevant information.
      




      It then formats the extracted data with
     


       |
      


      as
     


       a delimiter.
      




      A
     


       while
      


      loop is used to read the formatted data line
     


       by line.
      




      For each line, it inserts the data into the
     


       nmap_scans
      


      table
     


       using
      




        sqlite3
       




       .
      





     You may have noticed that our database has a field for
    


      vulnerability
     


     , but we didn’t insert any data in this field because we were only populating data from the
    


      Nmap scan.
     




     To update an
    



     existing record in the
    


      nmap_scans
     


     table to add a vulnerability where it was previously NULL, you can use the SQL UPDATE statement.
    

     Here’s how you can do this using Bash and the Sqlite3
    


      command-line tool:
     



$ sqlite3 pentest_results.db "UPDATE nmap_scans SET vulnerability = 'VULNERABILITY_DESCRIPTION' WHERE ip_address = 'IP_ADDRESS' AND port = PORT_NUMBER AND vulnerability IS NULL;"


     Replace the placeholders with your
    


      actual data:
     






       VULNERABILITY_DESCRIPTION
      


      : The description of the vulnerability you want
     


       to add.
      





       IP_ADDRESS
      


      : The IP address of the
     


       target system.
      





       PORT_NUMBER
      


      : The port number where the vulnerability
     


       was found.
      





     For example, if you want to update the record for the
    


      10.2.10.10
     


     IP on port
    


      80
     


     to add an
    


      SQL Injection vulnerability
     


     de
    







     scription, you will use
    


      the following:
     



$ sqlite3 pentest_results.db "UPDATE nmap_scans SET vulnerability = 'SQL Injection vulnerability' WHERE ip_address = 10.2.10.10 AND port = 80 AND vulnerability IS NULL;"


     This command will update the vulnerability field for the record that matches the specified IP address and port, but only if the vulnerability field is currently
    


      NULL
     


     .
    

     This ensures you don’t overwrite any existing
    


      vulnerability descriptions.
     




     If you want to update the vulnerability regardless of whether it’s
    


      NULL
     


     or not, you can remove th
    







     e
    


      AND vulnerability IS
     




       NULL
      




      condition:
     



$ sqlite3 pentest_results.db "UPDATE nmap_scans SET vulnerability = 'SQL Injection vulnerability' WHERE ip_address = 10.2.10.10' AND port = 80;"


     Now that we
    



     have data in our database, let’s create a script to query and display the results.
    

     The following script can be found
    





     in this chapter’s GitHub repository
    


      as
     




       ch13_read_db.sh
      




      :
     



#!/usr/bin/env bash
DB_NAME="pentest_results.db"
# Function to truncate strings to a specified length
truncate() {
    local str="$1"
    local max_length="$2"
    if [ ${#str} -gt $max_length ]; then
        echo "${str:0:$max_length-3}..."
    else
        printf "%-${max_length}s" "$str"
    fi
}
# Print header
printf "%-15s | %-15s | %-5s | %-8s | %-15s | %-20s | %s\n" \
    "IP Address" "Hostname" "Port" "Protocol" "Service" "Version" "Vulnerability"
printf "%s\n" "$(printf '=%.0s' {1..109})"
# Query and format the results
sqlite3 -separator "|" "$DB_NAME" "SELECT ip_address, hostname, port, protocol, service, version, vulnerability FROM nmap_scans ORDER BY ip_address, port;" |
while IFS='|' read -r ip hostname port protocol service version vulnerability; do
    ip=$(truncate "$ip" 15)
    hostname=$(truncate "$hostname" 15)
    port=$(truncate "$port" 5)
    protocol=$(truncate "$protocol" 8)
    service=$(truncate "$service" 15)
    version=$(truncate "$version" 20)
    vulnerability=$(truncate "$vulnerability" 20)
    printf "%-15s | %-15s | %-5s | %-8s | %-15s | %-20s | %s\n" \
        "$ip" "$hostname" "$port" "$protocol" "$service" "$version" "$vulnerability"
done
echo "Query completed."


     The following
    



     figure shows the output from
    


      this script:
     











     Figure 13.5 – The database contents
    



     Here’s an explanation of
    


      the code:
     






       DB_NAME="pentest_results.db"
      


      sets a variable with the name of the
     


       database file.
      




      The
     


       truncate()
      


      function is defined.
     

      It takes two arguments: a string and a maximum length.
     

      It checks whether the string is longer than the maximum length.
     

      If it is, it cuts the string short and adds
     


       ...
      


      at the end.
     

      If not, it pads the string with spaces to reach the maximum length.
     

      This function helps format the output to fit in
     


       fixed-width columns.
      




      The script then prints a header row.
     


       printf
      


      is used to format the output.
     


       %-15s
      


      means left-align this string and pad it to 15 characters.
     

      The
     


       |
      


      characters are used to visually separate columns.
     

      A line of equal signs is printed to separate the header from the data.
     


       printf '=%.0s' {1..109}
      


      prints 109
     


       equal signs.
      




      The script then queries the database.
     


       sqlite3
      


      is the command to interact with the SQLite database.
     


       -separator "|"
      


      tells SQLite to use pipe characters (
     


       |)
      


      to separate columns in its output.
     

      The SQL query selects all columns from the
     


       nmap_scans
      


      table, ordered by IP address
     


       and port.
      




      The output
     



      of the query is piped into a while loop.
     


       IFS='|
      


      ’ sets the
     


       Internal Field Separator
      


      to
     


       |
      


      , which tells the script how to split the
     



      input into fields.
     


       read -r
      


      reads a line of input and splits it into variables.
     

      Inside the loop, each field (
     


       ip
      


      ,
     


       hostname
      


      , etc.) is processed by the
     


       truncate
      


      function.
     

      This ensures each field fits within its designated column width.
     

      The formatted data is then printed using
     


       printf
      


      .
     

      This creates neatly aligned columns in
     


       the output.
      




      After the loop ends,
     


       Query completed.
      


      is printed to indicate the script has
     


       finished running.
      





     This script takes data from an SQLite database and presents it in a neatly formatted table, making it easy to read and analyze the results of
    


      network scans.
     




     By using these scriptdatabase andtomate the process of running Nmap scans, storing the results in a SQLite3 database, and querying the data for analysis.
    

     This approach allows for efficient data management and retrieval during
    


      pentesting activities.
     




     In the next section, you’ll learn how to extract data from the database and integrate it with
    


      reporting tools.
     






     Integrating Bash with reporting tools
    



     Writing a
    



     pentest report is both the most important as
    



     well as the least liked part of any pentest.
    

     Customers or system owners never get to see the work you do.
    

     Their opinion of how well you performed a pentest depends on the quality of the report.
    

     Pentesters usually dislike reporting because it’s not nearly as fun as
    



       popping shells
      




      .
     




     Automating data normalization and report generation can significantly improve report quality while reducing time spent on reporting.
    

     This section provides Bash tools and techniques to streamline your reporting process.
    

     While not creating a comprehensive pentest report, it offers adaptable examples you can tailor to your standards
    


      and workflow.
     




     This section will cover the basics of LaTeX, explain how to interact with the SQLite3 database using Bash, and demonstrate how to generate a
    


      PDF report.
     




     LaTeX
    



     is a high-quality typesetting system designed for the production of technical and scientific documentation.
    

     It is widely used in academia and professional settings for creating complex documents with consistent formatting, mathematical equations,
    


      and cross-references.
     




     For pentesters, LaTeX offers
    




      several advantages:
     





      Consistent formatting across
     


       large documents
      




      Easy integration of code snippets and
     


       command outputs
      




      Ability to generate professional-looking
     


       reports programmatically
      




      Support for complex tables
     


       and figures
      





     Let’s start by creating a Bash script to query our
    


      SQLite3
     


     database and format the results for use in our LaTeX document.
    

     The following script can be
    





     found in this chapter’s GitHub repository
    


      as
     




       ch13_generate_report.sh
      




      :
     



#!/usr/bin/env bash
DB_NAME="pentest_results.db"
# Function to query the database and format results
query_db() {
    sqlite3 -header -csv $DB_NAME "$1"
}
# Get all unique IP addresses
ip_addresses=$(query_db "SELECT DISTINCT ip_address FROM nmap_scans;")
# Create LaTeX content
create_latex_content() {
    echo "\\documentclass{article}"
    echo "\\usepackage[margin=1in]{geometry}"
    echo "\\usepackage{longtable}"
    echo "\\usepackage{pdflscape}"
    echo "\\begin{document}"
    echo "\\title{Penetration Test Report}"
    echo "\\author{Your Name}"
    echo "\\maketitle"
    echo "\\section{Scan Results}"
    IFS=$'\n'
    for ip in $ip_addresses; do
        echo "\\subsection{IP Address: $ip}"
        echo "\\begin{landscape}"
        echo "\\begin{longtable}{|p{2cm}|p{2cm}|p{1.5cm}|p{1.5cm}|p{3cm}|p{3cm}|p{4cm}|}"
        echo "\\hline"
        echo "Hostname & IP & Port & Protocol & Service & Version & Vulnerability \\\\ \\hline"
        echo "\\endfirsthead"
        echo "\\hline"
        echo "Hostname & IP & Port & Protocol & Service & Version & Vulnerability \\\\ \\hline"
        echo "\\endhead"
        query_db "SELECT hostname, ip_address, port, protocol, service, version, vulnerability
                  FROM nmap_scans
                  WHERE ip_address='$ip';" | sed 's/,/ \& /g; s/$/\\\\ \\hline/'
        echo "\\end{longtable}"
        echo "\\end{landscape}"
    done
    echo "\\end{document}"
}
# Generate LaTeX file
create_latex_content > pentest_report.tex
# Compile LaTeX to PDF
pdflatex pentest_report.tex


     Let’s
    



     look at
    


      the
     







      explanation:
     






       query_db()
      


      : This creates a function to query
     


       the database.
      





       sqlite3 -header -csv $DB_NAME "$1"
      


      : This function executes SQLite queries.
     

      It uses the
     


       -header
      


      option to include column names and
     


       -csv
      


      to output in
     


       CSV format.
      





       escape_latex()
      


      : This function escapes special LaTeX characters to prevent
     


       compilation errors.
      





       ip_addresses=$(query_db "SELECT DISTINCT ip_address FROM nmap_scans WHERE vulnerability IS NOT NULL AND vulnerability != '';" | tail -n +2)
      


      : This query fetches all unique IP addresses with vulnerabilities, skipping the
     


       header row.
      





       create_latex_content()
      


      : This function generates the LaTeX
     


       document structure.
      





       for ip in $ip_addresses; do
      


      : This loop processes each IP address, creating a subsection
     


       for each.
      





       query_db "SELECT hostname,…
      


      : This nested loop processes each vulnerability for a given IP address, formatting it for the
     


       LaTeX table.
      




      These commands generate the LaTeX file and compile it into
     


       a PDF:
      





         create_latex_content >
        




          pentest_report.tex
         






         pdflatex -
        




          interaction=nonstopmode pentest_report.tex
         








     To generate
    



     your
    



     pente
    





     st report, simply run the
    


      Bash script:
     



$ chmod +x ch13_generate_report.sh
./generate_report.sh


     This will create a file named
    


      pentest_report.pdf
     


     in your
    


      current directory.
     




     The following figure shows our very simple
    


      pentest report:
     











     Figure 13.6 – Our simple pentest report PDF
    



     You can further
    



     customize your report by adding more sections, such as
    



     an executive summary or recommendations, including graphics or charts to visualize data, using LaTeX packages for syntax highlighting of
    


      code snippets.
     




     For example, to add an executive summary, you could modify the
    



       create_latex_content
      




      function:
     



create_latex_content() {
    # ... (previous content)
    echo "\\section{Executive Summary}"
    echo "This penetration test was conducted to assess the security posture of the target network.
    The scan revealed multiple vulnerabilities across various systems, including outdated software versions and misconfigured services. Detailed findings are presented in the following sections."
    # ... (rest of the content)
}


     This section explored methods for using Bash scripts to streamline the creation of professional pentesting reports.
    

     It covered techniques for interfacing Bash with document preparation systems such as LaTeX to generate polished PDF reports.
    

     Adapt the provided methodology to your own standards to streamline your
    


      reporting process.
     






     Summary
    



     This chapter focused on using Bash to streamline the reporting phase of pentesting.
    

     It covered techniques for automating data collection, organizing findings, and generating comprehensive reports.
    

     We explored how to extract relevant information from tool outputs, parse and clean data, and store it efficiently using SQLite databases.
    

     We also addressed integrating Bash scripts with reporting tools such as LaTeX to create professional PDF reports.
    

     By leveraging Bash for these tasks, pentesters can significantly reduce the time and effort required for report generation while ensuring accuracy and consistency in
    


      their findings.
     




     The next chapter will examine methods for creating Bash scripts that can evade detection by endpoint security during
    


      pentesting engagements.
     















     Part 3: Advanced Applications of Bash Scripting for Pentesting
    



     In this part, you will explore cutting-edge applications of Bash scripting in modern pentesting scenarios.
    

     This final section begins with sophisticated evasion and obfuscation techniques, teaching you how to craft Bash scripts to bypass antivirus and endpoint detection systems while maintaining operational effectiveness.
    

     The section then ventures into the intersection of
    


      artificial intelligence
     


     (
    


      AI
     


     ) and pentesting, showing how to leverage Bash scripts to interact with AI models for enhanced vulnerability detection and automated decision-making during security assessments.
    

     Finally, you will learn to integrate your Bash scripting expertise into DevSecOps workflows, including automating security tests in CI/CD pipelines and creating custom, security-focused Kali Linux builds.
    

     This advanced section challenges you to push the boundaries of traditional Bash scripting, preparing you for emerging trends in the cybersecurity landscape while maintaining a focus on practical, real-world applications that can be immediately implemented in professional
    


      pentesting engagements.
     




     This part has the
    


      following chapters:
