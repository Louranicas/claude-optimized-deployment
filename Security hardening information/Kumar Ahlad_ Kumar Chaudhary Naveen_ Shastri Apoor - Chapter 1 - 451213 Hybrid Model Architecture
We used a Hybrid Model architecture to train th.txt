# Security Chapter Extract
Book: Kumar, Ahlad_ Kumar Chaudhary, Naveen_ Shastri, Apoorva S._ Sing - Digital Defence_ Harnessing the Power of Artificial Intelligence for Cybersecurity and Digital Forensics (2025, CRC Press) - libgen.li
Chapter: 1 - 4.5.1.2.1.3 Hybrid Model Architecture
We used a Hybrid Model architecture to train the balanced data. This proposed architecture is more complex when compared to the traditional ML and DL algorithms used earlier. This architecture comprises of hidden layers that are used to capture complex representations or patterns in a dataset making it more robust. The Hybrid Model architecture consisted of 1D CNN and an LSTM layer. 1D CNN was for feature extraction and the LSTM layer was for sequential data processing. The 1D CNN architecture was for the selection of features that can be best used to optimize the performance of the mode during training. It helps in identifying simple patterns within your data which will then be used to form more complex patterns within higher layers (LSTM layer). The structure of the Hybrid Model architecture is shown in Figure 4.8.
Security Relevance Score: 3
Word Count: 973
Extracted: 2025-06-13 23:40:52

---

4.5.1.2.1.3 Hybrid Model Architecture
We used a Hybrid Model architecture to train the balanced data. This proposed architecture is more complex when compared to the traditional ML and DL algorithms used earlier. This architecture comprises of hidden layers that are used to capture complex representations or patterns in a dataset making it more robust. The Hybrid Model architecture consisted of 1D CNN and an LSTM layer. 1D CNN was for feature extraction and the LSTM layer was for sequential data processing. The 1D CNN architecture was for the selection of features that can be best used to optimize the performance of the mode during training. It helps in identifying simple patterns within your data which will then be used to form more complex patterns within higher layers (LSTM layer). The structure of the Hybrid Model architecture is shown in Figure 4.8.



Long Description for Figure 4.8
The input data is processed through a 1D convolutional layer with 64 filters, a kernel size of 3, and ReLU activation. This is followed by a max-pooling layer with a pool size of 2, which reduces the spatial dimensions of the feature maps. The output is then flattened into a 1D vector. The flattened data is passed to an LSTM (Long Short-Term Memory) layer with 50 units, which captures sequential dependencies in the data. The output of the LSTM layer is concatenated with the output of a dense layer containing 64 units and ReLU activation. This combined representation is fed into a final dense output layer with two units and a softmax activation function, which produces the classification probabilities. The architecture is annotated with key parameters such as the number of filters, kernel size, pool size, and activation functions for clarity.

FIGURE 4.8 Structure of the Hybrid Model architecture utilized.

The model starts with a 1D CNN branch, which is effective for capturing spatial patterns in sequential data. Within the CNN layer, a Conv1D layer was employed to extract features using 64 filters of size 3, followed by a ReLU activation function to introduce non-linearity. Subsequently, a MaxPooling1D layer was applied to down-sample the features, reducing the spatial dimensions while preserving important information. The output from the CNN layers was then flattened into a 1D feature vector using a Flatten layer, preparing it for combination with the LSTM branch. In parallel, the model included an LSTM branch, which specializes in capturing long-term dependencies in sequential data. Within the LSTM layer, an LSTM layer with 50 units and ReLU activation was implemented to learn temporal patterns in the sequential data. The outputs from both the CNN and LSTM branches were concatenated together, merging the spatial and temporal features learned by each branch. The concatenated features were then passed through a Dense layer with 64 units and ReLU activation, allowing for further abstraction and representation of the combined features. Finally, the output from the Dense layer was fed into a Dense output layer with a single neuron and sigmoid activation, which outputs the final prediction as a probability for binary classification.
The model was trained with a batch size of 32 and was compiled using the Adam optimizer with a learning rate of 0.001 and binary cross-entropy as the loss function. During training, early stopping was implemented with a patience of five epochs and restoring the best weights. The early stopping was to monitor the performance of the model during training.


4.5.1.2.1.4 1D Convolutional Neural Network
Just like the LSTM, which is mainly utilized for sequential training of a model, the 1D CNN model has been proven to be very effective for sequential training of a model on time-series datasets. The input shape for this model is (n, l), where n is the number of samples and l is the length of each sample (length of time steps in the time-series data). The 1D CNN as shown in Figure 4.9 utilizes the same architecture as the native CNN network (three-dimensional convolutional neural network). The only difference is the input shape for the CNN model architecture. The reason for the proposed model, the 1D CNN model, is because of its ability to extract pertinent features from the temporal patterns, thus providing an effective method for analyzing time-series data in IDS frameworks [24]. It is ideally suited for identifying irregularities and intrusions in network and system logs due to its capacity to capture local dependencies and hierarchical representations.



Long Description for Figure 4.9
First, a 1D convolutional layer with 32 filters and ReLU activation extracts features. This is followed by a max-pooling layer with a pool size of 2, reducing spatial dimensions. The second convolutional layer with 64 filters and ReLU activation extracts higher-level features, followed by another max-pooling layer. The output is then flattened into a 1D vector. A dense layer with 128 units and ReLU activation learns complex patterns, while a dropout layer with a rate of 0.5 helps prevent overfitting. The final output layer, with two units and softmax activation, produces classification probabilities. Key parameters like filter count and kernel size are labeled.

FIGURE 4.9 Structure of the ID-CNN Model architecture utilized.

For the 1D CNN model, the input data was reshaped to incorporate a channel dimension, facilitating compatibility with the CNN layers. The input data was reshaped to incorporate a channel dimension (n, l, 1). Here, n represents the number of samples, l denotes the length of each sample (time steps in time-series data), and one signifies the channel dimension required by the Conv1D layers. The model architecture includes two Conv1D layers optimized for feature extraction from the sequential input: the first Conv1D layer utilized 32 filters with a kernel size of 3, followed by a ReLU activation function. The second Conv1D layer utilized 64 filters with a kernel size of 3, also activated by ReLU. Max-pooling layers followed each Conv1D layer, employing a pool size of
