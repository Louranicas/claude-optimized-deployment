# Security Chapter Extract
Book: Kumar, Ahlad_ Kumar Chaudhary, Naveen_ Shastri, Apoorva S._ Sing - Digital Defence_ Harnessing the Power of Artificial Intelligence for Cybersecurity and Digital Forensics (2025, CRC Press) - libgen.li
Chapter: 4 - Hybrid Model(1D CNN & LSTM)
313,058
Security Relevance Score: 3
Word Count: 179
Extracted: 2025-06-13 23:40:52

---

Hybrid Model(1D CNN & LSTM)
313,058






4.5.1.2.1.2 Stacked Long Short-Term Memory
A Stacked LSTM model architecture comprises of multiple hidden LSTM layers where the output of the first layer is passed to the next layer until the last layer. The choice of this algorithm/model architecture for training is that it is robust in generalization and sequential time-series computation. Given our primary goal of detecting intrusions in a network, which requires analyzing sequences of data to determine network security, the sequential nature of LSTMs makes it an ideal choice. As an extension of the RNN model, it has been proven effective in sequential training tasks, mitigating the vanishing gradient problem of the RNN model. Due to the long sequential computations, the RNN model has difficulty learning from longer dependencies, thus, it reaches a point where the model finds it extremely difficult to learn further which reduces the efficiency of the trained agent.
LSTM models address this by introducing a cell structure with three gates: the forget gate, the input gate, and the output gate which is portrayed in Figure 4.
