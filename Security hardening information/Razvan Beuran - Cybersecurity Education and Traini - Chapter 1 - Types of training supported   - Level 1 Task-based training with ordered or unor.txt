# Security Chapter Extract
Book: Razvan Beuran - Cybersecurity Education and Training (2025, Springer Nature Singapore) - libgen.li
Chapter: 1 - Types of training supported:   - Level 1: Task-based training with ordered or unordered questions.   - Level 1: Milestone-based training with automatic milestone checks.   - Level 2: Both task-based and milestone-based modes.2. Structuring of tasks/milestones:   - Level 1: No relationships between tasks; all available at once.   - Level 1: Defined relationships; tasks are prerequisites for others.   - Level 2: Both modes supported.3. Types of questions:   - Level 1: Only short-answer questions.   - Level 1: Only multiple-choice questions.   - Level 2: Both short-answer and multiple-choice questions."Fig. 13.1Example of training definition capability assessment criteria
Sandbox Definition Criteria There are 13 criteria in the sandbox definition class, which are also organized logically into two groups, similar to the case of the training definition class:Functional capabilities: Criteria concerning the functionality provided via sandbox definitions, which includes sandbox customization, supported types of hosts and operating systems, security features, etc. (SD-1 through SD-7).Usability capabilities: Criteria related to the usability of the sandbox definitions, including the creation of definitions, their representation and validation, the availability of documentation, etc. (SD-8 through SD-13).
We note that for criterion SD-7, which refers to network topology capabilities, figures are used in the explanation for the sake of clarity. The figures depict the topologies mentioned in the text, such as bus, star, tree, and hybrid topologies. This ensures that there is no misunderstanding when evaluating this particular capability.Discussion The total number of criteria in the two classes that make up the training content representation category is 25. This relatively large number of criteria emphasizes the importance of training content representation for a training platform, an aspect that we believe is often overlooked by training platform developers.
Security Relevance Score: 6
Word Count: 1771
Extracted: 2025-06-13 23:41:15

---

Types of training supported:   - Level 1: Task-based training with ordered or unordered questions.   - Level 1: Milestone-based training with automatic milestone checks.   - Level 2: Both task-based and milestone-based modes.2. Structuring of tasks/milestones:   - Level 1: No relationships between tasks; all available at once.   - Level 1: Defined relationships; tasks are prerequisites for others.   - Level 2: Both modes supported.3. Types of questions:   - Level 1: Only short-answer questions.   - Level 1: Only multiple-choice questions.   - Level 2: Both short-answer and multiple-choice questions."Fig. 13.1Example of training definition capability assessment criteria
Sandbox Definition Criteria There are 13 criteria in the sandbox definition class, which are also organized logically into two groups, similar to the case of the training definition class:Functional capabilities: Criteria concerning the functionality provided via sandbox definitions, which includes sandbox customization, supported types of hosts and operating systems, security features, etc. (SD-1 through SD-7).Usability capabilities: Criteria related to the usability of the sandbox definitions, including the creation of definitions, their representation and validation, the availability of documentation, etc. (SD-8 through SD-13).
We note that for criterion SD-7, which refers to network topology capabilities, figures are used in the explanation for the sake of clarity. The figures depict the topologies mentioned in the text, such as bus, star, tree, and hybrid topologies. This ensures that there is no misunderstanding when evaluating this particular capability.Discussion The total number of criteria in the two classes that make up the training content representation category is 25. This relatively large number of criteria emphasizes the importance of training content representation for a training platform, an aspect that we believe is often overlooked by training platform developers.
13.2.2.2 Network Environment ManagementThe network environment management (EM) category of assessment criteria includes those characteristics and features of cybersecurity training platforms that are used in relation with the network environment used for training. This includes creating the network environment, orchestrating the actions that are needed to modify it during the training, and destroying the environment when the training ends.Environment Management Criteria The total number of criteria in this category is 13, and they are organized into three groups, based on their scope:Functional capabilities: Criteria regarding network environment management functionality, including environment creation and control, support for executing action and network attacks, etc. (EM-1 through EM-5).Performance capabilities: Criteria that refer to network environment management performance, namely the reliability and efficiency of environment creation, and resource consumption (EM-6 through EM-8).Usability capabilities: Criteria related to network environment management usability, such as the type of user interface, the degree of automation, and monitoring features (EM-9 through EM-13).
Note that for criteria EM-6 through EM-8, which refer to performance assessment, the following information is also provided along with the text-based explanation to ensure that the assessment can be conducted objectively: 1.The reference network topology that is to be used when conducting the capability assessment tests. 2.Equations that describe mathematically the conditions that must be met to reach a certain capability level. Discussion We consider that the criteria related to the performance capabilities of a training platform are an important contribution of this assessment category, as they provide objective numerical metrics for evaluating the performance characteristics of the platform. Nevertheless, since the training platform must be deployed in order to be able to conduct those measurements, these criteria are more difficult to assess compared to all the other criteria that can be evaluated only based on platform specifications and/or source code.
13.2.2.3 Training Activity FacilitationThe training activity facilitation (AF) category of assessment criteria refers to those characteristics and features of a cybersecurity training platform that make it easier to use. The capabilities in this category include aspects such as: how tasks are assigned to participants via the training platform, how instructors can follow trainee progress during the training activity, how to record participant results, and so on.Activity Facilitation Criteria This category includes a total number of 20 criteria, which are organized into three groups, as described below:Pretraining setup capabilities: Criteria in relation with pretraining setup activities, including automation of the deployment, import of the training content, visibility and access time control for training sessions, etc. (AF-1 through AF-6).Training execution capabilities: Criteria regarding various training execution characteristics, for instance, the use of an LMS or intelligent tutoring system (ITS), educational features such as scaffolding or cheating prevention and detection (AF-7 through AF-18).Post-training assessment capabilities: Criteria related to post-training assessment activities, namely the export of training data, and result analysis across different training sessions (AF-19 and AF-20).Discussion The training activity facilitation category includes a large number of criteria compared to the other classes we discussed so far. This is because there is a wide variety of features that a platform can provide in order to facilitate training activities from many perspectives.We note, however, that the criteria in this category are not essential for a cybersecurity training platform to function. Therefore, we suggest considering them initially simply as optional capabilities that would make the platform easier to use. Nevertheless, some activity facilitation features can become a requirement, for instance, if organizers decide they should be present as a differentiating factor of their training platform. In that case, those specific features and the corresponding capability levels should be examined in a rigorous manner.

13.2.3 Assessment ProcedureNow that we have introduced the capability assessment methodology, we will also provide some practical advice on how to actually assess the capabilities of a given cybersecurity training platform.13.2.3.1 Preparation StepsBefore proceeding with the assessment, it is necessary to conduct several preparation steps, as it will be explained next.Acquire Documentation First of all, it is important to retrieve the available documentation about the cybersecurity training platform that is to be assessed. For some platforms, this could mean collecting information from the related websites, but obtaining a detailed user guide, if available, is best.Deploy Platform The information available in documentation and user guides may be somewhat vague, making it difficult to correctly assess the actual capabilities of a training platform. Deploying the platform on the organization servers is the best way of trying its features in view of a detailed assessment. Moreover, it also allows those who deploy the platform to determine how good the documentation is.We remark that, due to the overhead and practical challenges of training platform deployment, trying to conduct the assessment exclusively based on documentation is possible, especially if more than a few platforms are to be evaluated. However, for assessing the criteria related to platform performance, which require measurements, deployment cannot be avoided.Acquire Source Code Although the documentation and platform deployment should provide, in general, enough information for the general assessment of a training platform, for an even more accurate appraisal of some of the capability assessment criteria it may be necessary to get hold of the actual source code of the platform. If one has enough technical expertise, this additional source of information makes it possible to determine precisely the exact capability level for any platform feature.Decide Assessment Profile Before starting the assessment, evaluators should also decide which of the assessment criteria are most relevant for a given purpose. Even though all the criteria could be assessed each time, some of them may not be of interest for a given target.To facilitate this task, the concept of assessment profile was introduced in [3], with several examples being provided, such as Simple Training Activity or Unsupervised Training Activity. Those profiles state the criteria that must necessarily be evaluated when the corresponding types of activities are being planned. Defining a custom profile that meets the specific requirements of an organization is also possible, thus further reducing the assessment load.
13.2.3.2 Assessment Process SuggestionsOnce the preparations for assessment are completed, the capability assessment tool should be used to conduct the actual assessment. In this context, we provide below several suggestions on how to proceed with this task so as to minimize the effort needed to conduct the assessment.1.As the initial step of the assessment, one should go through all the relevant assessment criteria, possibly by using an assessment profile, and try to evaluate the corresponding capability level based on the available documentation, as this is the approach that requires the least amount of work. 2.For those criteria that could not be assessed based on the available documentation, one can try to contact the developers of the platform. However, for open-source platforms, obtaining detailed answers may be difficult due to the volunteer nature of the developers. 3.If there are still any assessment criteria for which the capability level is undecided, one should deploy the platform, and evaluate those criteria directly by using it. This alternative has the advantage that it will also provide a direct experience with that platform, at the cost of having to deploy it. 4.For performance capability assessments, the platform must necessarily be deployed; if deployment was not required at the previous step, then it must be done now. Since these criteria can also be evaluated at the end, delaying the deployment is acceptable. Moreover, if a precise assessment is not absolutely necessary, obtaining estimated hardware requirements and performance metrics from platform developers is also a possibility. 
Next we will illustrate how the capability assessment can be conducted in practice, and how its results can be interpreted, through a case study on the cybersecurity training platform we discussed in Chap. 12, CyTrONE.


13.3 CyTrONE Capability AssessmentIn this section, we will present in detail the capability assessment that we conducted for the integrated cybersecurity training framework CyTrONE [2, 5] by using the methodology discussed so far.As developers of CyTrONE, the assessment was based mainly on our firsthand knowledge of the framework. In some cases, however, we referred to the module user guides and source code to determine what features are available to end users, as some features of CyTrONE are still under implementation, and not fully ready for use or documented. In addition, we have performed the experiments required to access the performance-related capabilities of CyTrONE.In what follows, we will examine the results of the capability assessment by looking at each of the three categories of assessment criteria that were defined in the methodology, namely training content representation, network environment management, and training activity facilitation.13.3.1 Training Content RepresentationFor the training content representation category, the two aspects to assess are the capabilities related to training definition and sandbox definition. The results of the assessment are plotted in the form of radar charts in Figs. 13.2 and 13.3, respectively. Since the maximum values that can be assigned for each assessment criterion are not the same, the maximum capability levels are also plotted with a lighter color as reference.Radar chart comparing two datasets, "Maximum" and "CyTrONE," across twelve dimensions labeled TD-1 to TD-
