# Security Chapter Extract
Book: Shilpa Mahajan_ Mehak Khurana_ Vania Vieira Estrela - Applying Artificial Intelligence in Cybersecurity Analytics and Cyber Threat Detection (2024, WILEY) - libgen.li
Chapter: 2 - “HC activities in multimedia” (HCM) can be deemed as follows [33, 38]: (i) media production, (ii) annotation, (iii) organization, (iv) archival, (v) retrieval, (vi) sharing, analysis, and (vii) communication, which can be further clustered into three major groups: production, analysis, and interaction.
Multimedia production entails anthropological creative media tasks [39] for illustration, photographing, generating audio, remixing, etc. All media production aspects implicated must directly encompass humans in HCM. Multimedia creation has two main dimensions:
Security Relevance Score: 26
Word Count: 5095
Extracted: 2025-06-13 23:40:31

---

“HC activities in multimedia” (HCM) can be deemed as follows [33, 38]: (i) media production, (ii) annotation, (iii) organization, (iv) archival, (v) retrieval, (vi) sharing, analysis, and (vii) communication, which can be further clustered into three major groups: production, analysis, and interaction.
Multimedia production entails anthropological creative media tasks [39] for illustration, photographing, generating audio, remixing, etc. All media production aspects implicated must directly encompass humans in HCM. Multimedia creation has two main dimensions:

The first entails cultural and social dynamics. HCM production should ponder cultural differences and be planned consistent with the target culture of a given deployment.
The second is to mull over human abilities. HCM production participants should be able to finalize their activities throughout production.

Multimedia analysis is an HCM activity type that automatically scrutinizes general human deeds and social behavior. There is a wide‐ranging area of potentially relevant usages, from simplifying and enhancing human communications to refining data access and recovery in business, entertainment, and individual domains.
Multimedia interaction is the dialog activity portion of HCM, whose behavior comprehension is crucial. For this reason, professionals can employ technologies to assist such communication so that humans can connect with computers organically. Cultural differences and social environment are essential facets for creating natural contact owing to the probable various cultural origins. Some varieties encompass (a) face‐to‐face communications wherever the interaction is physically located and real time, (b) live‐computer‐mediated communications with physically remote interaction but remains real time, and (c) non‐real‐time computer‐mediated communications, for instance, instant SMS, email, etc.


Figure 12.2 Intelligent cyber‐physical system involving sub‐systems that rely on AI.


The “HC Design Process” (HCDP) first involves how the user learns about the product’s target audience and understand clients’ needs. Empathizing will lead to research and asking the target audience precise questions about their growth goals. Competitor analysis may be used to uncover extra product market design opportunities during this research stage. After gathering user and product demand data, the designer will utilize sketches and wireframes to brainstorm design alternatives. A user interface’s information architecture, space allocation, and content functionality are outlined in wireframing. Consequently, a wireframe usually lacks colors and visuals and focuses on interface functionality [40]. HCDP has two final phases:

The designer will usually transform paper sketches or low‐fidelity wireframes into high‐fidelity models upon enriched wireframing or sketching. Prototyping lets designers’ probe, try their ideas further, and take notice of the overall design concept. High‐fidelity implies the prototype is “clickable” or interactive and simulates an authentic application.
The designer can test usability after creating a high‐fidelity prototype. This test involves collecting participants’ experiences for the sake of benchmarking. These partakers must represent the product’s target audience and take them through the prototype’s possible contention points as if using it. Usability testing aims to identify any design issues needing improvement and analyze how real users interact with the resultant product. It is vital to follow up on the users’ comportment and choices besides asking them about their thoughts while operating the prototype to run a hands‐on usability test.

HCM can be deemed as follows: (i) media production, (ii) annotation, (iii) organization, (iv) archival, (v) retrieval, (vi) sharing, analysis, and (vii) communication, which can be further bundled into three key groups: production, analysis, and interaction.
Multimedia production entails anthropological creative media tasks for illustration, photographing, generating audio, remixing, etc. All media production aspects implicated must directly encompass humans in HCM. Multimedia creation has two main dimensions:

The first entails cultural and social dynamics. HCM production should ponder cultural differences and be planned consistent with the target culture of a given deployment.
The second is to mull over human abilities. HCM production participants should be able to finalize their activities throughout production.

Multimedia analysis is an HCM activity type that automatically scrutinizes general human deeds and social behavior. There is a wide‐ranging area of potentially relevant usages, from simplifying and augmenting anthropological communications to refining data access and recovery in business, entertainment, and individual domains.
Multimedia interaction is the dialog activity portion of HCM. For this reason, professionals can connect with computers organically. Cultural differences and social environments are essential facets for creating natural contact. Some communication varieties encompass (a) face‐to‐face wherever the interaction happens indoors, physically, and in real time, (b) live‐computer mediated with remote interaction that remains real time, and (c) non‐real‐time, for instance, SMS, and email.
The HCDP helps the operator to learn about the product’s target audience and understand needs to direct research and inquire the audience about their goals. Competitor analysis may uncover extra product market design opportunities during this research stage. After gathering user and product demand data, the designer will utilize sketches and wireframes to brainstorm alternatives. A user interface’s information structural design, space apportionment, and content functionality are outlined in wireframing. Consequently, a wireframe usually lacks colors and visuals, focusing on interface functionality. HCDP has two final phases:

The designer usually transforms low‐fidelity wireframes or sketches into high‐fidelity models. Prototyping lets designers probe, further their ideas, and take notice of the whole project concept. High‐fidelity implies the prototype is “clickable” or interactive, simulating an authentic application.
The designer can test usability after crafting a high‐fidelity prototype. This test involves collecting participants’ experiences for the sake of benchmarking. These partakers must represent the product’s target audience and examine the prototype’s possible contention points as if using it. Usability testing aims to identify any design issues needing improvement and to analyze the way real users interact with the resultant product. It is vital to follow up on the users’ comportment and choices besides asking them about their thoughts while operating the prototype during a hands‐on usability test.



12.3 Improving Cybersecurity Through Deep Learning (DL) Models: AI‐HCC Systems
This section adds another feature to DL/ML methodologies that expedites gaining network knowledge from unsupervised facts and clarifies complex problems. AI and CS can be extensively used to protect companies from phishing, spear‐phishing, drive‐by attacks, password attacks, denial of service, etc.

12.3.1 Inserting DL in Cloud, Fog, and Edge Computing
First, with three key paradigms (CC, FC, and EC), it is imperative to distinguish the target user from other stakeholders [41]. In Figure 12.2, the most important participants are part of a nursing home. Users interact with a product or service (e.g., outpatients, common sense smartphone handlers, students, etc.). In contrast, stakeholders care about something or participate in a company’s action/service delivery. Design must take into account user needs and stakeholder aspirations. Some stakeholders are end users, but not all. The data management initiative affects executives, managers, sponsors, clients, and regulators. Since users are the people using the software or service and stakeholders are business members or someone helping the user in loco (e.g., caretaker, maintenance person, etc.) responsible for planning and preparing features, (i) stakeholders’ focus is on the business, and the features are how they create value, and (ii) the user’s “quality of experience” (QoE) and associated “quality of service” (QoS) are gateways to the product’s features. Overlooking or overemphasizing insights from the stakeholder’s perspective can harm the user and the product. Remote resources should preserve strong defenses and records of activities via a blockchain setting that ensures fair play from all sides and viewpoints of a transaction to minimize stakeholder disasters.
Due to a lack of anthropocentric approaches, HCC designs must prevent faults and disasters in CPS elements, including HW and software (SW).
IoT devices connect EC resources to users or applications outside the data center, near the activity it supports. Data center physical, access, and network security are lost due to deployment changes, ill‐design, or human error. Edge apps are a massive step toward M2M without human control, which is risky for most businesses. “Edge security” (ES) threats are serious. Understanding these issues and their solutions for seamless business operations. EC security enhances data center security and compliance. This requires protecting edge device access physically and through a user interface as well as data center technologies but is suitable for outside deployment. ES can protect consumers and sensitive data at a company’s “edge” by protecting data that lives or travels through devices outside centralized data centers. Data leaks are one of the most prominent ES dangers because hackers can easily access data stored locally on devices rather than centrally. Hackers can access sensitive data on individual or networked devices.
CC provides Internet‐based servers, storage, networks, software, and analytics. Data leakage from poor cloud security across cloud networks causes IP theft, contract breaches, and virus assaults. Hackers can control how organizations provide clients. CC cybersecurity (C3) cannot prevent all threats since customers do not control CC. However, a good C3 approach significantly decreases risks. CC is often safer than on‐premise computing despite these hazards. Virtualization is crucial to cloud deployment. Multi‐tenant ecosystems’ customers may not share data [42–49]. Cloud storage is kept, managed, backed up remotely, and accessible to clients via a network. Virtualization relies on the hypervisor, which runs several VMs on a single HW host. Hypervisors manage many operating systems on a shared physical system. NIST again divides into four implementation strategies depending on consumer cloud suitability and intent. Organizations can use public or private cloud services [42–49] and selecting services is problematic for stakeholders (including users) and business decision‐makers, leading to user‐centered evaluation. Due to the abundance of cloud service providers offering similar cloud services, choosing the best one is tough. Many articles have been proposed in recent years. To detect and prevent unwanted transfer or deletion of valuable data, “data loss prevention” (DLP) software is essential. Cloud infrastructure developers must offer safe APIs for clients, but stakeholders should not worry. Illogical CS exhausts people operating throughout cyberspace and makes them prone to blunders and incorrect decisions. Employee education and conscientization, safe data backup strategies, regulated data access, encryption, strong password protection, repeated tests, and extensive cloud governance policies are needed to prevent cloud security concerns in enterprises. It’s crucial to balance automated and human decision‐making. Again, combining HCC and blockchain can provide good cybersecurity and privacy without keeping data.
FC receives encrypted data, making data retrieval difficult. Unlike CC, owners must develop a safe index for data search when uploading data to fog nodes. Data will be searched using different keywords after fog node processing. FC makes time‐sensitive data storage and analysis easier locally. FC minimizes cloud data volume and distance, lowering security, and privacy risks. FC can be vulnerable to IP address spoofing and MitM attacks. FC uses edge and cloud resources, requiring HW. FC inherits EC, CC vulnerabilities, and distributed infrastructure. New security and privacy issues increase the need to secure communication channels, ensure data integrity, prevent illegal access, and address crucial privacy concerns. Some main DL cybersecurity applications follow [42–49].
Trace of Intrusion Detection: “Artificial NNs” (ANNs), “convolutional neural networks” (CNNs), and “recurrent neural networks” (RNNs) are DL variants that can deliver more competent ID/IP schemes by scrutinizing the Internet traffic with superior accuracy, lessening the false alerts’ number, and aiding security teams in differentiating bad from suitable network activities. Some example solutions comprise “user entity and behavior analytics” (UEBA), “web application firewall” (WAF), and “next‐generation firewall” (NGFW).
Malware Detection: Traditional malware solutions, e.g., common firewalls, detect malware via a signature‐based detection system. If a company keeps a database of notorious threats, it updates the stored data frequently to integrate the latest threats. While this practice is effective against vulnerabilities, it struggles to cope with more innovative threats. DNNs can distinguish more unconventional threats and are not contingent on recalling known signatures and standard attack patterns. As an alternative, they “understand” the system and can identify suspicious doings that might signpost the existence of corrupt actors or malware.
Spam and Social Engineering (SE) Recognition: “Natural language processing” (NLP) can aid one in quickly detecting and dealing with spam and other SE forms. NLP learns everyday communication and language pattern conditions and uses statistical models to spot and block spam, employing TensorFlow to augment email spam detection capabilities.
Network Traffic Analysis (NTA): DL and combinations of ANNs and ML or metaheuristics have shown promising outcomes in investigating HTTPS network traffic to hunt for malicious activities, which is advantageous in dealing with many weaknesses similar to SQL injections and DoS outbreaks.
Behavior Analytics (BA): Analyzing and tracking front‐end client activities and comportments is a crucial DL‐established security practice in any organization. These tasks are much more defying than recognizing customary malicious doings against networks because they bypass security protection mechanisms and habitually do not raise flags and alarms. User BA (UBA) and User/Entity BA (UEBA) are great tools against such occurrences. After an educative period, it can grasp standard employee behavioral forms and identify suspicious activities, e.g., accessing the system at uncommon hours, possibly indicating an insider attack, and raising alerts.
Monitoring Emails: Watching employees’ official email accounts is vital to prevent cyber outbreaks. To exemplify, phishing attacks are commonly instigated through emails to personnel and questioning them for sensitive data. Cybersecurity SW with DNN can evade these kinds of vulnerabilities. NLP can also scan emails for any distrustful behavior.
Analyzing Mobile Endpoints: DL is already reaching mainstream on mobile equipment and driving voice‐based experiences through mobile assistants. So, DNN can identify and analyze threats against portable endpoints when the enterprise has to inhibit the growing number of mobile devices’ malware.
Enhancing Human Analysis: DL in CS can help humans detect malicious outbreaks, endpoint protection, analyze the network, and perform vulnerability evaluations. Through this, humans can decide on things better by discerning ways and means to solve problems.
Task Automation: The main DL benefit is automating repetitive tasks that enable staff to emphasize more critical work. There exist a few CS tasks that can be automated with ML. Organizations can undertake tasks faster and better by incorporating DL into their functions.
WebShell: A piece of code that can be malevolently loaded into a website to offer access to modify the server’s Webroot, allowing attackers to access the database. DL can help perceive normal shopping behavior. The AI model can be trained to discern normal and malign behavior.
Network Risk Scoring (NRS): DL can analyze previous cyberattack datasets and regulate what network’s areas were impaired by a particular attack, thus helping prevent the attack concerning a given network area.


12.3.2 DL and HCI
HCI mainly scrutinizes the information exchange between humans and computers, encompassing cognitive psychology, multimedia, ergonomics, “virtual reality” (VR), and “augmented reality” (AR) [50]. The HCI exchange relies on interactive human‐handled devices (e.g., keyboard, mouse, joysticks, wearables, and position trackers) and computer‐human cooperative devices (viz printers, plotters, monitors, and helmet‐mounted monitors) [51–53]. The HCI progression process involves voice interaction, image recognition, AR, VR, and somatosensory interfaces [54–56]. Voice has maximum input effectiveness and the most relaxed interaction, where the products’ adoption scenarios can quickly broaden options. Image recognition can help automation of driving and security for traffic situation identification and human features recognition. AR and VR aid in immersion for interaction, visualization, and movement [57, 58]. People’s body movements can ease interacting with nearby devices or real/remote environments through motion sensing without any complex controller.
DL has proven relatively hopeful in language processing, speech/image recognition, and information retrieval [59–61] (Figure 12.3). Other tactics embrace context‐aware systems, behavioral information synthesis from modeling user investigations, embedded dialog agents, or natural speech treatment, all utilizing DL to support human exchanges with smart designs. DL adoptions hinge on building models mimicking the human neural connections, which process and extract meaning from sound, images, and writings. Data features are labeled hierarchically through several transformation phases, leading to data interpretations. This enables ML and deep NNs designs to improve decision‐making by imitating the human brain and neurons’ interconnections [62–65]. DL adoption in HCI can expand speech and image recognition accuracy while enhancing interaction realism. Language understanding explores the language caveats of HCIs. Contrasting to speech recognition, which transforms speech into text or matching commands, language comprehension comprises creating machines that grasp human language. Sensors can gradually ameliorate HCI because of the environmental digitization tendency brought by the IoT. Media content, real/virtual environments, objects, and individuals are all experiencing a digitalized process. Interface design is essential, and how to craft and deploy a natural HCI will turn into a critical proposition. Intelligent devices that comprehend scenes in the environment will become more realistic, stress‐free, and humanized HCI, which happens once the user has a suitable help guide and, thus, the user does not need too much knowledge about memory function or instantaneous operation understanding.


Figure 12.3 An HCC stage with DL.


Incorporating audio, imagery information, touch screens, and video is vital to HCI, making its design, data output, and user interaction more flexible. Media has transformed people’s intercommunication. HCI requirements related to self‐service machines, transportation information displays, and shopping mall displays require similar HW solutions. A text mining scheme named two‐level conceptual link analysis surpasses the traditional HCI, where the keyboard is obligatory, causing certain adoption limitations. Embedded hardware is paramount for HCI scenes’ rendering and aid in many other possibilities. Intelligent HCI combined with DL works intensively in gesture, speech, emotion, and NLP recognition. Various recognition approaches are projected and verified through testing to attain high recognition accuracy. Consequently, applying DL in HCI design can widen the application expectations.



12.4 Case Studies
A rational, shielded two‐party protocol model with a hybrid architecture provides new adversary verification/validity [65–67]. This design has also proven safe in the occurrence of new adversaries under the ideal/real paradigm [68] (Figures 12.2 and 12.3).

12.4.1 HCI Use Cases
HCI refers to the information exchange between people and HW with a computer informing folks through any output, like display devices, people entering relevant data through input devices. The concrete virtual realization of a multimodal simulation interface shows the real environment and coexisting agents. The most noticeable content is represented by behavior and dialog. VoxWorld is a simulation platform for making HCIs [69, 70] that follows a multimodal dialog structure that converses through language, gesticulation, facial expressions, actions, and gaze locating in a task‐oriented interactive setting. The 3D image acquisition cost is falling with continuous sensor developments. Gesture recognition under depth and red–green–blue (RGB) imageries gradually led to pattern recognition developments.
Nonetheless, most deep gesture image processing methodologies are reasonably simple, ignoring the relationship and impact between both modes and failing to fully use different modes’ interrelated factors. Depth image information [71] assists in solving the above problems, assuming the independent and associated multimodal data features and constructing an adaptive weight procedure to fuse different features. Simulation outcomes excelled the customary DL gesture image processing schemes, and the recognition rate was more elevated, with superior recognition accuracy that surpasses other advanced methods. Testes gauge the method’s viability and robustness and often point out that multimodal image acquisition through DL can augment gesture recognition accuracy in HCI systems.
The same application consequences are also mirrored in the context‐aware framework. Data‐driven tools for continuous human motion scrutiny and human–machine cooperation necessitate prediction in future intelligent businesses to ameliorate robots’ planning and control besides ending shared tasks [72]. Numerical examples can verify the engine’s feasibility, and fallouts should meet the prerequisites. Likewise, a context‐aware citation recommender can model an end‐to‐end memory network [73] utilizing “bidirectional long short‐term memory” (Bi‐LSTM) to assimilate papers and citation contexts. Still, tryouts on different datasets confirmed the model’s superior performance.
Moreover, context‐aware intelligent HCI systems, as client modeling research advises, DL also widely applies to user modeling grounded on past interaction matrices and recommender systems under equivalent function learning arrangements. Existing DL‐founded recommendation schemes usually employ the user’s interaction history to accomplish static user preference modeling. A time‐aware DL framework modeling dynamic user predilections through an attention mechanism and estimate matching scores constructed on DL [74]. It considerably and consistently beats current time‐aware and DL‐reliant recommendation methodologies.
The literature has abundant multidisciplinary content since HCI embraces a wide range, with restricted studies exhibiting the vast picture. Such analyses afford a superior understanding, revealing current issues, obstacles, and potential exploration gaps. HCI research trends revealed topics beyond glimpses, bearing in mind their development stage, number of applications, and acceleration to offer a panoramic outlook presenting trends augmenting and deteriorating over time [75]. The HCI investigations’ shift from machine‐oriented structures to human‐oriented systems signposts its future path toward up‐context, intuiting adaptive systems. Combining emotion analysis with humane knowledge absorption research helped construct a forward‐thinking, simple, safe, and effective HCI for emotion inspection [76], relating facial expressions and audio signals to discover macro expressions and produce an emotion index that explains users’ mental health. Collected users’ records are observed to analyze the person’s mental health and arrange for counseling solutions for a worthy treatment effect in humans. AI, HCI, and intelligent robot collaboration (cobots) technologies are crucial and thought‐provoking content. Regarding SW and HW, the previous techniques investigate and try to craft a natural HCI atmosphere, providing collaborations between HCI and robots [77] through present reading, technologies for writing, listening, speaking, and catering to other senses can be improved to solve some of the noteworthy HCI challenges. Hence, DL performance in other HCI intelligent systems can be better.


12.4.2 Cybersecurity and HCI Use Cases
Adversarial learning (AL) and ML can help address network security concerns at the frontier [78, 79]. The adversarial issue may arise when learning algorithms do not effectively exploit the input feature set, allowing invaders to center on a narrow feature collection to deceive the model. Two crucial classifiers can fix this. A “random forest” (RF) model called “weighted RF” (WRF) can support input feature recognition evenly. Selecting a clustering subset of trees throughout runtime augments performance. NNs can rely on extra soft restrictions that relate the objective function with weight variances to base classification decisions on better‐distributed feature groupings. These methods have amended the learned model’s robustness compared to baseline systems.
A hybrid “convolution neural network” (CNN) model has emerged in Ref. [80], in which a dilated‐based CNN furthers the recognition accuracy. A numerical NN speeds up the identification process. In dilated‐based DL designs, the convolution and pooling layers have been substituted by dilated convolution, which shrinks computation costs. Weight parameters are quantized by the quantitative NN‐built scheme to an integer power of two, transforming multiplications into shift operations, thus significantly dropping the time.
Employing the IoT in CPSs, like autonomous driving, big data analysis is required with high precision and negligible latency. DL supports robust analytic skills at the cloud and edge layers with low latency for effectual big data exploration. However, existing research failed to address particular obstacles, viz. security, centralized control, adversarial incidents, and privacy. The work in Ref. [81] proposes DeepBlockIoTNet, a secure DL aimed at an IoT network with blockchain. The DL occurs among edge nodes in a decentralized, safe manner at the edge layer. The blockchain DL module eradicates centralized authority control while strengthening security. The experimental evaluation supported higher accuracy.
Rapid information handling and Internet technology growth has led to “electronic health records” (EHR). The research in Ref. [82] has innovated EHR cybersecurity prevention regarding feature selection and classification through DL methodologies. At this time, input EHR data are processed to eliminate null values and noise. This treated data is selected according to their features, exploiting a kernel‐based, gradient‐boosting NN with classification via a stochastic CNN. A cryptographic cloud‐established CPS blockchain model has enhanced the network’s data security.
As the Internet matures, so do security weaknesses. There are numerous ways to secure a cyber‐environment, and the best choice must always be elected. AI has given technology a new perspective by making life easier for ordinary users with its unique ideas. In this AI‐type of architecture, the computer works with hidden layers to emulate the human mind and produce output. DNA‐centered security hinging on DL that behaves like cryptography and secures cyber data transit is being studied [83]. Recent studies confirm that this combination ameliorates data security. The DNA via DL has strengthened security systems by inhibiting significant cyberattacks. Suppose one focuses on the health sector [84, 85], which handles patient health data records. According to Ref. [83], integrating DNA sequence and DL methods improves data confidentiality, integrity, authorization, and authentication for genuine users. Medical practitioners need this. DNA security mechanisms improve the health privacy of information through DL techniques.
Due to wireless mediums’ shortcomings, ad hoc networks are vulnerable to several threats and attacks [86, 87]. Due to this, intrusion detection, security, privacy, and validation in ad‐hoc networks are currently of great interest. This research identifies wireless ad‐hoc network assaults and offers solutions. The work in Refs. [88, 89] covers black holes, wormholes, selective forwarding, Sybil, and denial‐of‐service attacks. This research presents a trust‐based safe routing strategy for mobile ad‐hoc networks to reduce black hole node interference. When black hole nodes are in the routing path, network performance suffers. Thus, a routing technique is introduced to minimize black hole node‐related packet loss. This routing system has been experimentally tested to determine the best secure path for packet delivery between sources and destinations. A wireless network is segmented and routed poorly when wormholes invade. One may locate wormholes by employing ordinal multi‐dimensional scaling and round‐trip duration in wireless ad hoc nets with sparse or dense topologies. The approach described can find wormholes with short routes and long path links. This stratagem is experimentally investigated to ensure that this ad hoc network has no hidden wormholes. Three methods to defend wireless ad‐hoc networks from selective forwarding attacks are devised. The first solution exploits a reward‐punishment mechanism to stimulate three nodes to forward messages in busy ad‐hoc networks [90–93]. A novel adversarial model (with three node kinds and their behaviors) employs the incentive‐based technique to prevent nodes from acting separately, warranting packet‐forwarding collaboration. The second authenticates intermediary nodes in resource‐constrained ad‐hoc networks to safely transport packets using non‐cooperative game theory. This model leverages game theory. This game finds a desired equilibrium that makes multihop communication physically viable, which is discovered. The third procedure accomplishes binary searches and control packets. It can catch malicious nodes in multihop, hierarchical ad‐hoc networks. The cluster head can accurately identify the malign node by analyzing packet sequences dropped from a source node. A lightweight symmetric encryption via binary playfair protects data transmission. Experiments suggested that the encryption approach is energy‐, time‐, and memory‐efficient. This lightweight encryption method reduces Sybil attacks in clustered wireless ad‐hoc networks.



12.5 Discussion
HCC‐AI systems can identify shadow data, monitor data access irregularities, and inform security personnel about potentially harmful conduct by data users, reducing time in finding and fixing issues. Cybersecurity for CPSs relying on DL can make (i) existing things more usable for people, (ii) something esthetically pleasing, (iii) a product out of an abstract algorithm or idea, and (iv) businesses grow by leveraging technology into a product‐market fit; this diversity is why people will say their field is different. Hence, these structures allow the convivence of these entities:

Human: Someone needs to understand people;
Computers: They intercede between humans and CPSs, albeit they also need to talk to other machines;
Interaction: All CPS parts need to work together well. This part comprises networking, sensors, actuators, and controllers.

DL algorithms can detect information‐sensitive patterns and monitor access and transmission to avoid unauthorized data leaking. These models can appraise network data flow, reveal shortcomings, and establish security policies to safeguard sensitive data.
Security issues are more likely when large amounts of data are transferred through networks. FC reduces the amount of data being transferred back and forth to the cloud, reducing latency due to local computation while minimizing security risks.

12.5.1 HCC‐AI Advantages
HCI is the larger field of understanding how humans and computers interact. HCC only comes up when HCI developers are discovered to be making bad designs that work, allowing companies to make technological products accessible to individuals with disabilities. It helps “user experience” (UX) designers and others understand each user’s needs relating to technology. It shows that not all users interact with technology in the same way.
HCC is a reaction to the early emphasis in HCI on figuring out how to make it possible for humans to adapt to computers. HCC suggested that understanding what humans want and need is most important, putting the focus of adaptation on the shoulders of the computer rather than the human. Human error can be reduced with the same HCI technology, and all these losses can be avoided.
AI‐HCC design lets one better understand oneself’s and people’s needs, motivations, and concerns, but it also makes for a more efficient, more flexible design process.
The reliability and scalability of an HCC‐AI take people’s abilities as human thinkers and allow these ideas to scale to serve much larger data needs. AI aims to help humans, but without human input and understanding, it can only help so much.


12.5.2 HCC‐AI Caveats
HCC‐AI systems work faster and necessitate fewer hands. This advanced technology has benefits, albeit it does create some dangers. The most upsetting risks are the misapplication of technology and a negative effect on human experiences [94–99]. HCC‐AI can impact several realms: human‐technology synergy, human‐environment collaborations, cause loss of jobs, ethics, moral values, privacy, security, criminality, well‐being, health, happiness, general accessibility, universal access, healthy learning without emotional disruption, creativity, social organization, and democracy.



12.6 Conclusion
Human‐centered design relies on fostering empathy through being alert to and aware to all implicated human stakeholders and is attentive on identifying resolutions through an open, non‐judgmental approach. It is rooted in a conviction that a new participant’s mindset will impel one to better, more inventive solutions.
Cloud computing has transformed how devices connect over the Internet, resulting in the IoT, a multitude of linked gadgets that can perceive and answer back to human needs, and a vast data volume. The FC and EC layers pose several HCC problems, too.
In DL, a multi‐layer model employs sequential layers. One layer’s output turns out to be the subsequent layer’s input. Unsupervised learning learns valuable features and advanced structures from low‐level qualities. Supervised learning optimizes network parameters with improved learning using labeled data. DL development relies on high‐performance computers to train wide‐ranging neural networks whose input can benefit from swiftly applying an enormous amount of neural network‐labeled data to alleviate training drawbacks. DL models can unravel the most complex problems. HCC in fog and MENs is a major issue. Thus, DL‐grounded development solutions are expected to aid fog, and MEN’s HCC, sharing means to mitigate the already growing number of cybersecurity glitches besides their aftermaths in a planet that is implementing AI indiscriminately, without considering HCC issues.


References

1 Ho, J. and Wang, C. (2021). Human‐centered AI using ethical causality and learning representation for multi‐agent deep reinforcement learning. In: Proc. 2nd Int’l Conference on Human‐Machine Systems (ICHMS), 1–
