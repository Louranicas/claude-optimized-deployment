# Distributed Rate Limiting System

## Overview

This distributed rate limiting system provides comprehensive protection against abuse and ensures fair resource allocation across the Claude-Optimized Deployment Engine. The system supports multiple algorithms, scopes, and high-throughput scenarios while maintaining sub-millisecond response times.

## Features

### üèóÔ∏è Architecture
- **Distributed**: Redis-backed for multi-instance deployments
- **High Performance**: Sub-50ms average latency even under load
- **Highly Available**: Supports Redis Sentinel for failover
- **Scalable**: Handles 1000+ requests per second per instance

### üßÆ Rate Limiting Algorithms

#### 1. Token Bucket
- **Use Case**: Smooth traffic with burst capability
- **Benefits**: Allows controlled bursts while maintaining average rate
- **Best For**: API endpoints that need to handle traffic spikes

```python
config = RateLimitConfig(
    requests=100,      # Base rate: 100 requests
    window=60,         # Per 60 seconds
    algorithm=RateLimitAlgorithm.TOKEN_BUCKET,
    burst=150          # Allow bursts up to 150 requests
)
```

#### 2. Sliding Window
- **Use Case**: Precise rate limiting over time
- **Benefits**: Smooth rate limiting without reset spikes
- **Best For**: User-facing APIs requiring consistent experience

```python
config = RateLimitConfig(
    requests=1000,     # 1000 requests
    window=3600,       # Per hour (3600 seconds)
    algorithm=RateLimitAlgorithm.SLIDING_WINDOW
)
```

#### 3. Fixed Window
- **Use Case**: Simple, resource-efficient rate limiting
- **Benefits**: Low memory usage, simple reset behavior
- **Best For**: Background services, bulk operations

```python
config = RateLimitConfig(
    requests=500,      # 500 requests
    window=300,        # Per 5 minutes
    algorithm=RateLimitAlgorithm.FIXED_WINDOW
)
```

### üéØ Rate Limiting Scopes

#### Per-IP Rate Limiting
Protects against individual IP abuse:
```python
scope=RateLimitScope.PER_IP
```

#### Per-User Rate Limiting
Enforces user-specific quotas:
```python
scope=RateLimitScope.PER_USER
```

#### Global Rate Limiting
Protects infrastructure resources:
```python
scope=RateLimitScope.GLOBAL
```

#### Per-Endpoint Rate Limiting
Endpoint-specific protection:
```python
scope=RateLimitScope.PER_ENDPOINT
```

## Quick Start

### 1. Basic Setup

```python
from src.core.rate_limiter import DistributedRateLimiter, RateLimitConfig, RateLimitAlgorithm, RateLimitScope

# Initialize rate limiter
rate_limiter = DistributedRateLimiter("redis://localhost:6379/0")
await rate_limiter.initialize()

# Configure endpoint
config = RateLimitConfig(
    requests=100,
    window=60,
    algorithm=RateLimitAlgorithm.SLIDING_WINDOW,
    scope=RateLimitScope.PER_IP
)
rate_limiter.configure_endpoint("/api/users", [config])

# Check rate limit
results = await rate_limiter.check_rate_limit(
    endpoint="/api/users",
    ip_address="192.168.1.100",
    user_id="user123"
)

if all(r.allowed for r in results):
    # Process request
    print("Request allowed")
else:
    # Handle rate limit
    print("Rate limit exceeded")
```

### 2. FastAPI Integration

```python
from fastapi import FastAPI, Depends
from src.core.rate_limit_init import initialize_rate_limiting_for_app
from src.core.rate_limit_middleware import rate_limit_dependency

app = FastAPI()

# Initialize rate limiting
await initialize_rate_limiting_for_app(
    app=app,
    environment="production",
    enable_monitoring=True
)

# Use dependency for specific endpoints
@app.post("/api/auth/login")
async def login(
    credentials: LoginRequest,
    _: None = Depends(rate_limit_dependency())
):
    # Endpoint automatically protected by rate limiting
    return {"token": "..."}
```

### 3. Custom Configuration

```python
from src.core.rate_limit_config import RateLimitingConfig, EndpointRateLimitConfig

# Create custom configuration
config = RateLimitingConfig()

# Add authentication endpoints with strict limits
auth_config = EndpointRateLimitConfig(
    endpoint_pattern="POST:/auth/*",
    description="Authentication endpoints"
)
auth_config.add_config(
    requests=5, window=300,  # 5 requests per 5 minutes
    algorithm=RateLimitAlgorithm.FIXED_WINDOW,
    scope=RateLimitScope.PER_IP
)

config.add_custom_endpoint(auth_config)
```

## Configuration Examples

### Production API Configuration

```python
# Multi-layer protection for production API
PRODUCTION_CONFIG = {
    # Authentication - Very strict
    "POST:/auth/login": [
        RateLimitConfig(
            requests=5, window=300,
            algorithm=RateLimitAlgorithm.FIXED_WINDOW,
            scope=RateLimitScope.PER_IP
        ),
        RateLimitConfig(
            requests=10, window=3600,
            algorithm=RateLimitAlgorithm.SLIDING_WINDOW,
            scope=RateLimitScope.PER_IP
        )
    ],
    
    # API endpoints - Balanced
    "*/api/*": [
        RateLimitConfig(
            requests=1000, window=3600,
            algorithm=RateLimitAlgorithm.SLIDING_WINDOW,
            scope=RateLimitScope.PER_USER
        ),
        RateLimitConfig(
            requests=100, window=60, burst=150,
            algorithm=RateLimitAlgorithm.TOKEN_BUCKET,
            scope=RateLimitScope.PER_IP
        ),
        RateLimitConfig(
            requests=10000, window=60,
            algorithm=RateLimitAlgorithm.SLIDING_WINDOW,
            scope=RateLimitScope.GLOBAL
        )
    ],
    
    # File uploads - Resource protection
    "POST:/upload/*": [
        RateLimitConfig(
            requests=10, window=300,
            algorithm=RateLimitAlgorithm.FIXED_WINDOW,
            scope=RateLimitScope.PER_IP
        ),
        RateLimitConfig(
            requests=50, window=3600,
            algorithm=RateLimitAlgorithm.SLIDING_WINDOW,
            scope=RateLimitScope.PER_USER
        )
    ]
}
```

### User Tier Configurations

```python
from src.core.rate_limit_config import UserTierConfig

# Free tier users
free_limits = UserTierConfig.free_tier()
# - 100 requests per hour
# - 10 requests per minute

# Premium tier users  
premium_limits = UserTierConfig.premium_tier()
# - 1000 requests per hour
# - 50 requests per minute with burst to 100

# Enterprise tier users
enterprise_limits = UserTierConfig.enterprise_tier()
# - 10,000 requests per hour
# - 200 requests per minute with burst to 500
```

## Monitoring and Metrics

### Real-Time Monitoring

```python
from src.core.rate_limit_monitoring import RateLimitMonitor

# Initialize monitor
monitor = RateLimitMonitor(redis_client)
await monitor.start()

# Get real-time statistics
stats = await monitor.get_real_time_stats()
print(f"Requests per minute: {stats['requests_per_minute']}")
print(f"Denial rate: {stats['denial_rate']:.1%}")

# Get metrics summary
summary = await monitor.get_metrics_summary()
print(f"Total requests: {summary.total_requests}")
print(f"Top denied endpoints: {summary.top_denied_endpoints}")
```

### API Endpoints for Monitoring

The system provides REST API endpoints for monitoring:

- `GET /rate-limits/metrics` - Overall metrics
- `GET /rate-limits/metrics/endpoint/{endpoint}` - Endpoint-specific metrics
- `GET /rate-limits/metrics/real-time` - Real-time statistics
- `GET /rate-limits/config` - Current configuration
- `POST /rate-limits/reset` - Reset rate limits (admin only)

### Rate Limit Headers

HTTP responses include informative headers:

```http
X-RateLimit-Limit: 100
X-RateLimit-Remaining: 95
X-RateLimit-Reset: 1640995200
X-RateLimit-Scope: per_ip
X-RateLimit-Algorithm: sliding_window
Retry-After: 60
```

## High-Throughput Scenarios

### Performance Characteristics

The rate limiting system is optimized for high-throughput scenarios:

- **Throughput**: 1000+ requests/second per instance
- **Latency**: <50ms average response time
- **Memory**: Efficient Redis usage with configurable TTL
- **Scalability**: Horizontal scaling with Redis clustering

### Load Testing Results

```bash
# Example load test results
Total requests: 10,000
Duration: 45.2s
Throughput: 221.2 req/s
Average latency: 23.5ms
99th percentile: 87.3ms
Error rate: 0.0%
```

### Optimization Tips

1. **Choose the Right Algorithm**:
   - Token Bucket: For bursty traffic
   - Sliding Window: For smooth limiting
   - Fixed Window: For resource efficiency

2. **Configure Appropriate TTLs**:
   ```python
   config = RateLimitConfig(
       requests=1000,
       window=3600,
       redis_ttl=7200  # Keep data for 2 hours
   )
   ```

3. **Use Multi-Layer Protection**:
   ```python
   # Layer 1: Per-IP (prevents abuse)
   # Layer 2: Per-User (quota enforcement)  
   # Layer 3: Global (infrastructure protection)
   ```

4. **Monitor Performance**:
   ```python
   # Regular health checks
   health = await rate_limiter.get_metrics()
   if health["redis_info"]["connected"]:
       print("Rate limiting healthy")
   ```

## Deployment

### Docker Compose

```yaml
version: '3.8'
services:
  redis:
    image: redis:7-alpine
    command: redis-server /etc/redis/redis.conf
    volumes:
      - ./config/redis.conf:/etc/redis/redis.conf
      
  app:
    build: .
    environment:
      - REDIS_URL=redis://redis:6379/0
      - ENVIRONMENT=production
    depends_on:
      - redis
```

### Environment Variables

```bash
# Redis configuration
REDIS_URL=redis://localhost:6379/0
REDIS_POOL_SIZE=20

# Rate limiting configuration
ENVIRONMENT=production
RATE_LIMITING_ENABLED=true

# Monitoring
ENABLE_RATE_LIMIT_MONITORING=true
METRICS_RETENTION_DAYS=7
```

### Production Checklist

- [ ] Redis configured with appropriate memory limits
- [ ] Redis Sentinel configured for high availability
- [ ] Rate limiting rules configured per environment
- [ ] Monitoring and alerting set up
- [ ] Load testing completed
- [ ] Error handling tested
- [ ] Failover scenarios tested

## Testing

### Unit Tests

```bash
# Run rate limiting tests
pytest tests/test_rate_limiting.py -v

# Run with coverage
pytest tests/test_rate_limiting.py --cov=src.core.rate_limiter
```

### Integration Tests

```bash
# Test with real Redis
pytest tests/test_rate_limiting.py::TestHighThroughputScenarios -v

# Load testing
python scripts/rate_limiting_demo.py
```

### Manual Testing

```bash
# Start the demo
python scripts/rate_limiting_demo.py

# Test specific endpoint
curl -X POST http://localhost:8000/api/test \
  -H "Content-Type: application/json" \
  -d '{"test": "data"}'
```

## Troubleshooting

### Common Issues

1. **Redis Connection Errors**:
   ```python
   # Check Redis connectivity
   redis_client = aioredis.from_url("redis://localhost:6379")
   await redis_client.ping()
   ```

2. **High Latency**:
   - Check Redis memory usage
   - Monitor network connectivity
   - Review rate limiting configuration

3. **Rate Limits Not Working**:
   ```python
   # Verify configuration
   metrics = await rate_limiter.get_metrics()
   print(metrics["endpoint_configs"])
   ```

4. **Memory Usage**:
   - Configure appropriate TTLs
   - Use efficient algorithms
   - Monitor Redis memory

### Debug Mode

```python
import logging
logging.getLogger("src.core.rate_limiter").setLevel(logging.DEBUG)
```

### Health Checks

```bash
# Application health
curl http://localhost:8000/health

# Rate limiting health
curl http://localhost:8000/rate-limits/health
```

## Security Considerations

### Rate Limiting Security

1. **IP Address Validation**:
   - Validate X-Forwarded-For headers
   - Handle proxy scenarios correctly
   - Prevent header spoofing

2. **User Authentication**:
   - Verify user identity before applying user-scoped limits
   - Handle anonymous users appropriately

3. **Redis Security**:
   - Use authentication in production
   - Disable dangerous commands
   - Configure network security

### Best Practices

1. **Fail Open**: Allow requests when Redis is unavailable
2. **Gradual Limits**: Implement warning thresholds
3. **Bypass Mechanisms**: Admin overrides for emergencies
4. **Audit Logging**: Log rate limiting decisions

## Contributing

### Development Setup

```bash
# Install dependencies
pip install -r requirements-dev.txt

# Start Redis
docker run -d -p 6379:6379 redis:7-alpine

# Run tests
pytest tests/test_rate_limiting.py

# Run demo
python scripts/rate_limiting_demo.py
```

### Adding New Algorithms

1. Extend `BaseRateLimiter`
2. Implement `check_rate_limit` method
3. Add to `DistributedRateLimiter._get_limiter`
4. Add comprehensive tests

### Code Style

```bash
# Format code
black src/core/rate_*.py

# Check types
mypy src/core/rate_*.py

# Lint
flake8 src/core/rate_*.py
```

## API Reference

### Core Classes

#### `DistributedRateLimiter`
Main rate limiting class with Redis backend.

#### `RateLimitConfig`
Configuration for rate limiting rules.

#### `RateLimitMiddleware`
FastAPI middleware for automatic rate limiting.

#### `RateLimitMonitor`
Monitoring and metrics collection.

### Configuration Enums

#### `RateLimitAlgorithm`
- `TOKEN_BUCKET`
- `SLIDING_WINDOW`
- `FIXED_WINDOW`

#### `RateLimitScope`
- `PER_IP`
- `PER_USER`
- `GLOBAL`
- `PER_ENDPOINT`

For detailed API documentation, see the inline docstrings in the source code.

## Support

For issues, questions, or contributions:

1. Check the troubleshooting guide above
2. Review existing GitHub issues
3. Create a new issue with:
   - Environment details
   - Configuration used
   - Error messages/logs
   - Steps to reproduce

---

**Claude-Optimized Deployment Engine Rate Limiting System v1.0.0**