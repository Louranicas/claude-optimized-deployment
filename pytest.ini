[tool:pytest]
# Comprehensive pytest configuration for Claude Optimized Deployment
# Testing infrastructure with high-volume test support and clear reporting

# Test discovery
testpaths = tests
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*

# Test execution with comprehensive reporting and parallel execution
addopts = 
    -v
    --strict-markers
    --strict-config
    --tb=short
    --cov=src
    --cov-report=html:coverage/html
    --cov-report=xml:coverage/coverage.xml
    --cov-report=term-missing:skip-covered
    --cov-report=json:coverage/coverage.json
    --cov-fail-under=80
    --cov-branch
    --junitxml=test-results/pytest-results.xml
    --html=test-results/pytest-report.html
    --self-contained-html
    --maxfail=10
    --durations=20
    --durations-min=1.0
    --cache-show
    --disable-warnings
    --hypothesis-show-statistics
    --hypothesis-profile=dev
    -n auto
    --dist=loadscope

# Minimum version
minversion = 7.0

# Markers for comprehensive test categorization
markers =
    unit: Unit tests for individual components
    integration: Integration tests across components
    e2e: End-to-end tests with real systems
    performance: Performance and load tests
    security: Security and vulnerability tests
    mutation: Mutation testing for code quality
    property: Property-based testing with Hypothesis
    slow: Tests that take longer than 10 seconds
    fast: Tests that complete quickly (< 1 second)
    network: Tests that require network access
    docker: Tests that require Docker
    kubernetes: Tests that require Kubernetes
    aws: Tests that require AWS credentials
    azure: Tests that require Azure credentials
    external: Tests that require external services
    mcp_server: Tests for MCP server implementations
    mcp_client: Tests for MCP client implementations
    smoke: Smoke tests for basic functionality
    regression: Regression tests for performance/security
    memory: Memory usage and leak detection tests
    stress: High-load stress tests
    chaos: Chaos engineering tests
    api: API endpoint tests
    auth: Authentication and authorization tests
    database: Database integration tests
    cache: Caching layer tests
    monitoring: Monitoring and metrics tests
    deployment: Deployment and infrastructure tests
    experimental: Experimental or unstable tests

# Test execution timeouts
timeout = 300
timeout_method = thread

# Warnings configuration
filterwarnings =
    error
    ignore::UserWarning
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::ImportWarning
    ignore::RuntimeWarning:asyncio
    ignore::pytest.PytestUnraisableExceptionWarning

# Required pytest plugins
required_plugins =
    pytest-cov
    pytest-html
    pytest-xdist
    pytest-timeout
    pytest-mock
    pytest-asyncio
    pytest-benchmark
    pytest-randomly
    pytest-repeat
    pytest-memray
    hypothesis

# Asyncio configuration
asyncio_mode = auto
asyncio_default_fixture_loop_scope = function

# Test collection configuration
collect_ignore = 
    setup.py
    build/
    dist/
    *.egg-info/
    node_modules/
    target/
    .venv/
    venv/
    examples/
    benchmarks/deployment_only

# Log configuration for test debugging
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S
log_file = test-results/pytest.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(filename)s:%(lineno)d: %(funcName)s(): %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Test discovery patterns
norecursedirs = .git .tox dist build *.egg node_modules target

# Console output configuration
console_output_style = progress
disable_warnings = true

# Performance test configuration
benchmark_only = false
benchmark_sort = mean
benchmark_compare_fail = mean:5%
benchmark_disable = false
benchmark_warmup = true
benchmark_warmup_iterations = 3

# Coverage configuration with comprehensive thresholds
[coverage:run]
source = src
parallel = true
branch = true
concurrency = multiprocessing,thread
omit = 
    */tests/*
    */test_*
    */__pycache__/*
    */migrations/*
    */venv/*
    */env/*
    */node_modules/*
    */target/*
    setup.py
    conftest.py
    */conftest.py
    */_version.py
    */version.py
    */versions.py

[coverage:report]
# Comprehensive coverage reporting with strict thresholds
precision = 2
show_missing = true
skip_covered = false
skip_empty = true
sort = -cover
fail_under = 80
exclude_lines =
    pragma: no cover
    pragma: nocover
    def __repr__
    def __str__
    if self.debug:
    if settings.DEBUG
    raise AssertionError
    raise NotImplementedError
    if 0:
    if False:
    if __name__ == .__main__.:
    class .*\(Protocol\):
    class .*\(TypedDict\):
    @(abc\.)?abstractmethod
    @overload
    @typing.overload
    \.\.\.$
    pass$
    except ImportError:
    except ModuleNotFoundError:
    # TYPE_CHECKING
    if TYPE_CHECKING:

[coverage:html]
directory = coverage/html
title = Claude Optimized Deployment Coverage Report
show_contexts = true
skip_covered = false
skip_empty = true

[coverage:xml]
output = coverage/coverage.xml

[coverage:json]
output = coverage/coverage.json
pretty_print = true
show_contexts = true

# Coverage thresholds per module
[coverage:paths]
source = 
    src/
    */site-packages/

# Module-specific coverage requirements
[coverage:report:modules]
# Core modules require higher coverage
src/core/* = 90
src/auth/* = 85
src/security/* = 90
src/circle_of_experts/* = 85
src/mcp/* = 80
src/monitoring/* = 75
src/database/* = 80