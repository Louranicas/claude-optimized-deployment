# Logstash configuration for Claude-Optimized Deployment Engine
# Processes and enriches logs before sending to Elasticsearch

input {
  # Receive logs from Filebeat
  beats {
    port => 5044
    ssl => true
    ssl_certificate => "/etc/logstash/certs/logstash.crt"
    ssl_key => "/etc/logstash/certs/logstash.key"
  }
  
  # Direct syslog input for infrastructure logs
  syslog {
    port => 5514
    type => "infrastructure"
  }
  
  # HTTP input for direct log shipping
  http {
    port => 8080
    codec => json
    type => "direct"
  }
}

filter {
  # Parse JSON logs
  if [type] == "filebeat" {
    json {
      source => "message"
      target => "parsed"
    }
    
    # Move parsed fields to root
    mutate {
      rename => {
        "[parsed][timestamp]" => "timestamp"
        "[parsed][level]" => "level"
        "[parsed][logger]" => "logger"
        "[parsed][message]" => "message"
        "[parsed][correlation_id]" => "correlation_id"
        "[parsed][data]" => "data"
        "[parsed][performance]" => "performance"
        "[parsed][security_audit]" => "security_audit"
      }
    }
  }
  
  # Parse timestamp
  date {
    match => ["timestamp", "ISO8601"]
    target => "@timestamp"
  }
  
  # Add processing timestamp
  ruby {
    code => "event.set('processed_at', Time.now.utc.iso8601(3))"
  }
  
  # Enrich MCP operations
  if [logger] =~ /^mcp\./ {
    mutate {
      add_field => { "log_category" => "mcp_operations" }
    }
    
    # Calculate operation success rate
    if [performance][duration_ms] {
      ruby {
        code => "
          duration = event.get('[performance][duration_ms]')
          if duration < 100
            event.set('[performance][speed_category]', 'fast')
          elsif duration < 1000
            event.set('[performance][speed_category]', 'normal')
          else
            event.set('[performance][speed_category]', 'slow')
          end
        "
      }
    }
  }
  
  # Enrich AI operations
  if [logger] =~ /^ai\./ {
    mutate {
      add_field => { "log_category" => "ai_operations" }
    }
    
    # Calculate token usage
    if [data][prompt_tokens] and [data][response_tokens] {
      ruby {
        code => "
          prompt = event.get('[data][prompt_tokens]').to_i
          response = event.get('[data][response_tokens]').to_i
          event.set('[data][total_tokens]', prompt + response)
        "
      }
    }
  }
  
  # Enrich security audit logs
  if [security_audit] {
    mutate {
      add_field => { 
        "log_category" => "security"
        "compliance_required" => "true"
      }
    }
    
    # Add GeoIP for access logs
    if [security_audit][ip_address] {
      geoip {
        source => "[security_audit][ip_address]"
        target => "[security_audit][geo]"
      }
    }
  }
  
  # Error classification
  if [level] in ["ERROR", "CRITICAL"] {
    mutate {
      add_field => { "alert_required" => "true" }
    }
    
    # Extract error type
    if [exception][type] {
      mutate {
        add_field => { "error_type" => "%{[exception][type]}" }
      }
    }
  }
  
  # Performance metrics aggregation
  if [performance] {
    # Add percentile buckets for duration
    if [performance][duration_ms] {
      ruby {
        code => "
          duration = event.get('[performance][duration_ms]').to_f
          buckets = [50, 100, 200, 500, 1000, 2000, 5000, 10000]
          bucket = buckets.find { |b| duration <= b } || 10000
          event.set('[performance][duration_bucket]', bucket)
        "
      }
    }
  }
  
  # Add environment metadata
  mutate {
    add_field => {
      "cluster" => "${CLUSTER_NAME:default}"
      "region" => "${AWS_REGION:us-east-1}"
      "deployment" => "${DEPLOYMENT_ID:unknown}"
    }
  }
  
  # Remove unnecessary fields
  mutate {
    remove_field => ["host", "agent", "ecs", "parsed", "@version"]
  }
}

output {
  # Send to Elasticsearch
  elasticsearch {
    hosts => ["${ELASTICSEARCH_HOSTS:localhost:9200}"]
    index => "code-logs-%{[environment]}-%{+YYYY.MM.dd}"
    template_name => "code-logs"
    template => "/etc/logstash/templates/code-logs.json"
    template_overwrite => true
    
    # Document ID for deduplication
    document_id => "%{[correlation_id]}-%{[timestamp]}"
    
    # Authentication
    user => "${ELASTICSEARCH_USER:elastic}"
    password => "${ELASTICSEARCH_PASSWORD:}"
    ssl => true
    ssl_certificate_verification => true
  }
  
  # Send errors to dedicated index
  if [level] in ["ERROR", "CRITICAL"] {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOSTS:localhost:9200}"]
      index => "code-errors-%{[environment]}-%{+YYYY.MM.dd}"
      user => "${ELASTICSEARCH_USER:elastic}"
      password => "${ELASTICSEARCH_PASSWORD:}"
      ssl => true
    }
  }
  
  # Send security audit logs to compliance index
  if [log_category] == "security" {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOSTS:localhost:9200}"]
      index => "code-security-audit-%{+YYYY.MM}"
      user => "${ELASTICSEARCH_USER:elastic}"
      password => "${ELASTICSEARCH_PASSWORD:}"
      ssl => true
    }
  }
  
  # Send performance metrics to metrics index
  if [performance] {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOSTS:localhost:9200}"]
      index => "code-metrics-%{[environment]}-%{+YYYY.MM.dd}"
      user => "${ELASTICSEARCH_USER:elastic}"
      password => "${ELASTICSEARCH_PASSWORD:}"
      ssl => true
    }
  }
  
  # Real-time alerts via webhook
  if [alert_required] == "true" {
    http {
      url => "${ALERT_WEBHOOK_URL:http://localhost:8081/alerts}"
      http_method => "post"
      format => "json"
      mapping => {
        "level" => "%{[level]}"
        "message" => "%{[message]}"
        "logger" => "%{[logger]}"
        "correlation_id" => "%{[correlation_id]}"
        "timestamp" => "%{[timestamp]}"
        "environment" => "%{[environment]}"
        "error_type" => "%{[error_type]}"
      }
    }
  }
  
  # Debug output (disable in production)
  if "${DEBUG_LOGGING:false}" == "true" {
    stdout { 
      codec => rubydebug 
    }
  }
}