"""
Automated SLA validation test suite.

Provides:
- Comprehensive SLA testing framework
- Real-time validation tests
- Performance regression detection
- Automated compliance checks
"""

import asyncio
import pytest
import json
from typing import Dict, List, Optional, Any, Callable
from datetime import datetime, timedelta
from dataclasses import dataclass
import statistics
import logging

from .sla import SLATracker, SLAObjective, SLAType, get_sla_tracker
from .sla_alerting import SLAAlertManager, get_sla_alert_manager
from .sla_history import SLAHistoryTracker, get_sla_history_tracker
from .sla_dashboard import SLADashboardAPI, get_sla_dashboard_api
from .prometheus_client import get_prometheus_client

__all__ = [
    "SLATestResult",
    "SLATestSuite",
    "SLAValidator",
    "run_sla_validation"
]

logger = logging.getLogger(__name__)


@dataclass
class SLATestResult:
    """Result of an SLA test."""
    test_name: str
    objective_name: str
    passed: bool
    message: str
    actual_value: Optional[float] = None
    expected_value: Optional[float] = None
    metadata: Dict[str, Any] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return {
            "test_name": self.test_name,
            "objective_name": self.objective_name,
            "passed": self.passed,
            "message": self.message,
            "actual_value": self.actual_value,
            "expected_value": self.expected_value,
            "metadata": self.metadata or {}
        }


class SLAValidator:
    """Validates SLA implementations and calculations."""
    
    def __init__(self):
        self.sla_tracker = get_sla_tracker()
        self.alert_manager = get_sla_alert_manager()
        self.history_tracker = get_sla_history_tracker()
        self.dashboard_api = get_sla_dashboard_api()
        self.prometheus_client = get_prometheus_client()
    
    async def validate_sla_calculation(self, objective_name: str) -> SLATestResult:\n        \"\"\"Validate that SLA calculation matches expected logic.\"\"\"\n        if objective_name not in self.sla_tracker.objectives:\n            return SLATestResult(\n                test_name=\"sla_calculation_validation\",\n                objective_name=objective_name,\n                passed=False,\n                message=f\"Objective {objective_name} not found\"\n            )\n        \n        objective = self.sla_tracker.objectives[objective_name]\n        \n        try:\n            # Get current report\n            report = await self.sla_tracker.check_objective(objective)\n            \n            # Validate basic constraints\n            if not (0 <= report.compliance_percent <= 100):\n                return SLATestResult(\n                    test_name=\"sla_calculation_validation\",\n                    objective_name=objective_name,\n                    passed=False,\n                    message=f\"Compliance percent out of range: {report.compliance_percent}\",\n                    actual_value=report.compliance_percent\n                )\n            \n            if not (0 <= report.error_budget_remaining <= 100):\n                return SLATestResult(\n                    test_name=\"sla_calculation_validation\",\n                    objective_name=objective_name,\n                    passed=False,\n                    message=f\"Error budget out of range: {report.error_budget_remaining}\",\n                    actual_value=report.error_budget_remaining\n                )\n            \n            # Validate error budget calculation\n            expected_budget = self.sla_tracker.calculate_error_budget(\n                objective, report.compliance_percent\n            )\n            \n            if abs(report.error_budget_remaining - expected_budget) > 0.01:\n                return SLATestResult(\n                    test_name=\"sla_calculation_validation\",\n                    objective_name=objective_name,\n                    passed=False,\n                    message=\"Error budget calculation mismatch\",\n                    actual_value=report.error_budget_remaining,\n                    expected_value=expected_budget\n                )\n            \n            return SLATestResult(\n                test_name=\"sla_calculation_validation\",\n                objective_name=objective_name,\n                passed=True,\n                message=\"SLA calculation validation passed\",\n                actual_value=report.compliance_percent\n            )\n            \n        except Exception as e:\n            return SLATestResult(\n                test_name=\"sla_calculation_validation\",\n                objective_name=objective_name,\n                passed=False,\n                message=f\"Validation failed with error: {e}\"\n            )\n    \n    async def validate_prometheus_connectivity(self) -> SLATestResult:\n        \"\"\"Validate Prometheus connectivity and data availability.\"\"\"\n        try:\n            # Test basic connectivity\n            metrics = await self.prometheus_client.query(\"up\")\n            \n            if not metrics:\n                return SLATestResult(\n                    test_name=\"prometheus_connectivity\",\n                    objective_name=\"system\",\n                    passed=False,\n                    message=\"No metrics returned from Prometheus\"\n                )\n            \n            # Check for required metrics\n            required_metrics = [\n                \"http_requests_total\",\n                \"http_request_duration_seconds\",\n                \"up\"\n            ]\n            \n            missing_metrics = []\n            for metric in required_metrics:\n                try:\n                    result = await self.prometheus_client.query(metric)\n                    if not result:\n                        missing_metrics.append(metric)\n                except Exception:\n                    missing_metrics.append(metric)\n            \n            if missing_metrics:\n                return SLATestResult(\n                    test_name=\"prometheus_connectivity\",\n                    objective_name=\"system\",\n                    passed=False,\n                    message=f\"Missing required metrics: {missing_metrics}\",\n                    metadata={\"missing_metrics\": missing_metrics}\n                )\n            \n            return SLATestResult(\n                test_name=\"prometheus_connectivity\",\n                objective_name=\"system\",\n                passed=True,\n                message=\"Prometheus connectivity validated\",\n                actual_value=len(metrics)\n            )\n            \n        except Exception as e:\n            return SLATestResult(\n                test_name=\"prometheus_connectivity\",\n                objective_name=\"system\",\n                passed=False,\n                message=f\"Prometheus connectivity failed: {e}\"\n            )\n    \n    async def validate_alerting_thresholds(self, objective_name: str) -> SLATestResult:\n        \"\"\"Validate alerting threshold configuration.\"\"\"\n        if objective_name not in self.alert_manager.alerting_rules:\n            return SLATestResult(\n                test_name=\"alerting_thresholds\",\n                objective_name=objective_name,\n                passed=False,\n                message=f\"No alerting rule found for {objective_name}\"\n            )\n        \n        rule = self.alert_manager.alerting_rules[objective_name]\n        objective = self.sla_tracker.objectives[objective_name]\n        \n        # Validate threshold ordering\n        thresholds = [\n            rule.critical_threshold,\n            rule.high_threshold,\n            rule.medium_threshold\n        ]\n        \n        if not all(thresholds[i] <= thresholds[i+1] for i in range(len(thresholds)-1)):\n            return SLATestResult(\n                test_name=\"alerting_thresholds\",\n                objective_name=objective_name,\n                passed=False,\n                message=\"Alert thresholds not properly ordered\",\n                metadata={\"thresholds\": thresholds}\n            )\n        \n        # Validate thresholds are below SLA target\n        if rule.medium_threshold >= objective.target:\n            return SLATestResult(\n                test_name=\"alerting_thresholds\",\n                objective_name=objective_name,\n                passed=False,\n                message=\"Medium threshold should be below SLA target\",\n                actual_value=rule.medium_threshold,\n                expected_value=objective.target\n            )\n        \n        return SLATestResult(\n            test_name=\"alerting_thresholds\",\n            objective_name=objective_name,\n            passed=True,\n            message=\"Alerting thresholds validated\"\n        )\n    \n    async def validate_historical_data_consistency(self, objective_name: str) -> SLATestResult:\n        \"\"\"Validate consistency of historical SLA data.\"\"\"\n        try:\n            # Get recent history\n            history = await self.history_tracker.get_history(\n                objective_name,\n                start_time=datetime.now() - timedelta(hours=24),\n                limit=100\n            )\n            \n            if len(history) < 5:\n                return SLATestResult(\n                    test_name=\"historical_data_consistency\",\n                    objective_name=objective_name,\n                    passed=False,\n                    message=f\"Insufficient historical data: {len(history)} points\"\n                )\n            \n            # Check for data anomalies\n            compliance_values = [point.compliance_percent for point in history]\n            \n            # Check for impossible values\n            invalid_values = [v for v in compliance_values if not (0 <= v <= 100)]\n            if invalid_values:\n                return SLATestResult(\n                    test_name=\"historical_data_consistency\",\n                    objective_name=objective_name,\n                    passed=False,\n                    message=f\"Invalid compliance values found: {invalid_values}\"\n                )\n            \n            # Check for excessive volatility (may indicate calculation issues)\n            if len(compliance_values) > 1:\n                std_dev = statistics.stdev(compliance_values)\n                if std_dev > 20:  # More than 20% standard deviation seems suspicious\n                    return SLATestResult(\n                        test_name=\"historical_data_consistency\",\n                        objective_name=objective_name,\n                        passed=False,\n                        message=f\"Excessive volatility detected: {std_dev:.2f}% std dev\",\n                        actual_value=std_dev\n                    )\n            \n            # Check timestamp ordering\n            timestamps = [point.timestamp for point in history]\n            if timestamps != sorted(timestamps):\n                return SLATestResult(\n                    test_name=\"historical_data_consistency\",\n                    objective_name=objective_name,\n                    passed=False,\n                    message=\"Historical data timestamps not properly ordered\"\n                )\n            \n            return SLATestResult(\n                test_name=\"historical_data_consistency\",\n                objective_name=objective_name,\n                passed=True,\n                message=\"Historical data consistency validated\",\n                actual_value=len(history)\n            )\n            \n        except Exception as e:\n            return SLATestResult(\n                test_name=\"historical_data_consistency\",\n                objective_name=objective_name,\n                passed=False,\n                message=f\"Historical data validation failed: {e}\"\n            )\n    \n    async def validate_dashboard_data_integrity(self) -> SLATestResult:\n        \"\"\"Validate dashboard data integrity and consistency.\"\"\"\n        try:\n            dashboard_data = await self.dashboard_api.get_dashboard_data()\n            \n            # Validate overall health score\n            if not (0 <= dashboard_data.overall_score <= 100):\n                return SLATestResult(\n                    test_name=\"dashboard_data_integrity\",\n                    objective_name=\"dashboard\",\n                    passed=False,\n                    message=f\"Invalid overall score: {dashboard_data.overall_score}\"\n                )\n            \n            # Validate objectives data structure\n            if not dashboard_data.objectives:\n                return SLATestResult(\n                    test_name=\"dashboard_data_integrity\",\n                    objective_name=\"dashboard\",\n                    passed=False,\n                    message=\"No objectives data in dashboard\"\n                )\n            \n            # Check each objective has required fields\n            required_fields = [\n                \"compliance_percent\", \"current_value\", \"target\",\n                \"error_budget_remaining\", \"is_compliant\"\n            ]\n            \n            for obj_name, obj_data in dashboard_data.objectives.items():\n                missing_fields = [field for field in required_fields if field not in obj_data]\n                if missing_fields:\n                    return SLATestResult(\n                        test_name=\"dashboard_data_integrity\",\n                        objective_name=\"dashboard\",\n                        passed=False,\n                        message=f\"Missing fields in {obj_name}: {missing_fields}\"\n                    )\n            \n            return SLATestResult(\n                test_name=\"dashboard_data_integrity\",\n                objective_name=\"dashboard\",\n                passed=True,\n                message=\"Dashboard data integrity validated\"\n            )\n            \n        except Exception as e:\n            return SLATestResult(\n                test_name=\"dashboard_data_integrity\",\n                objective_name=\"dashboard\",\n                passed=False,\n                message=f\"Dashboard validation failed: {e}\"\n            )\n    \n    async def validate_error_budget_burn_rate(self, objective_name: str) -> SLATestResult:\n        \"\"\"Validate error budget burn rate calculations.\"\"\"\n        try:\n            burn_rate = await self.sla_tracker.get_error_budget_burn_rate(objective_name)\n            \n            # Burn rate should be positive\n            if burn_rate < 0:\n                return SLATestResult(\n                    test_name=\"error_budget_burn_rate\",\n                    objective_name=objective_name,\n                    passed=False,\n                    message=f\"Invalid burn rate: {burn_rate}\",\n                    actual_value=burn_rate\n                )\n            \n            # Extremely high burn rates may indicate calculation issues\n            if burn_rate > 100:  # 100x normal rate seems excessive\n                return SLATestResult(\n                    test_name=\"error_budget_burn_rate\",\n                    objective_name=objective_name,\n                    passed=False,\n                    message=f\"Suspiciously high burn rate: {burn_rate}\",\n                    actual_value=burn_rate\n                )\n            \n            return SLATestResult(\n                test_name=\"error_budget_burn_rate\",\n                objective_name=objective_name,\n                passed=True,\n                message=\"Error budget burn rate validated\",\n                actual_value=burn_rate\n            )\n            \n        except Exception as e:\n            return SLATestResult(\n                test_name=\"error_budget_burn_rate\",\n                objective_name=objective_name,\n                passed=False,\n                message=f\"Burn rate validation failed: {e}\"\n            )\n\n\nclass SLATestSuite:\n    \"\"\"Comprehensive SLA test suite.\"\"\"\n    \n    def __init__(self):\n        self.validator = SLAValidator()\n        self.sla_tracker = get_sla_tracker()\n    \n    async def run_all_tests(self) -> Dict[str, List[SLATestResult]]:\n        \"\"\"Run all SLA validation tests.\"\"\"\n        results = {\n            \"system_tests\": [],\n            \"objective_tests\": {},\n            \"integration_tests\": []\n        }\n        \n        # System-level tests\n        logger.info(\"Running system-level SLA tests\")\n        \n        # Prometheus connectivity\n        prom_result = await self.validator.validate_prometheus_connectivity()\n        results[\"system_tests\"].append(prom_result)\n        \n        # Dashboard integrity\n        dashboard_result = await self.validator.validate_dashboard_data_integrity()\n        results[\"system_tests\"].append(dashboard_result)\n        \n        # Per-objective tests\n        logger.info(\"Running per-objective SLA tests\")\n        \n        for objective_name in self.sla_tracker.objectives.keys():\n            obj_results = []\n            \n            # SLA calculation validation\n            calc_result = await self.validator.validate_sla_calculation(objective_name)\n            obj_results.append(calc_result)\n            \n            # Alerting thresholds\n            alert_result = await self.validator.validate_alerting_thresholds(objective_name)\n            obj_results.append(alert_result)\n            \n            # Historical data consistency\n            hist_result = await self.validator.validate_historical_data_consistency(objective_name)\n            obj_results.append(hist_result)\n            \n            # Error budget burn rate\n            burn_result = await self.validator.validate_error_budget_burn_rate(objective_name)\n            obj_results.append(burn_result)\n            \n            results[\"objective_tests\"][objective_name] = obj_results\n        \n        # Integration tests\n        logger.info(\"Running integration tests\")\n        \n        # End-to-end SLA workflow test\n        e2e_result = await self._test_end_to_end_workflow()\n        results[\"integration_tests\"].append(e2e_result)\n        \n        return results\n    \n    async def _test_end_to_end_workflow(self) -> SLATestResult:\n        \"\"\"Test complete SLA workflow from calculation to alerting.\"\"\"\n        try:\n            # Pick the first available objective\n            if not self.sla_tracker.objectives:\n                return SLATestResult(\n                    test_name=\"end_to_end_workflow\",\n                    objective_name=\"system\",\n                    passed=False,\n                    message=\"No SLA objectives configured\"\n                )\n            \n            objective_name = list(self.sla_tracker.objectives.keys())[0]\n            objective = self.sla_tracker.objectives[objective_name]\n            \n            # 1. Calculate SLA\n            report = await self.sla_tracker.check_objective(objective)\n            \n            # 2. Record in history\n            await self.validator.history_tracker.record_sla_measurement(report)\n            \n            # 3. Check alerting\n            alerts = await self.validator.alert_manager.check_all_slas()\n            \n            # 4. Get dashboard data\n            dashboard_data = await self.validator.dashboard_api.get_dashboard_data()\n            \n            # Verify data flow\n            if objective_name not in dashboard_data.objectives:\n                return SLATestResult(\n                    test_name=\"end_to_end_workflow\",\n                    objective_name=objective_name,\n                    passed=False,\n                    message=\"Objective not found in dashboard data\"\n                )\n            \n            return SLATestResult(\n                test_name=\"end_to_end_workflow\",\n                objective_name=objective_name,\n                passed=True,\n                message=\"End-to-end workflow validated successfully\"\n            )\n            \n        except Exception as e:\n            return SLATestResult(\n                test_name=\"end_to_end_workflow\",\n                objective_name=\"system\",\n                passed=False,\n                message=f\"End-to-end test failed: {e}\"\n            )\n    \n    def generate_test_report(self, results: Dict[str, Any]) -> str:\n        \"\"\"Generate a comprehensive test report.\"\"\"\n        total_tests = 0\n        passed_tests = 0\n        failed_tests = []\n        \n        # Count system tests\n        for result in results[\"system_tests\"]:\n            total_tests += 1\n            if result.passed:\n                passed_tests += 1\n            else:\n                failed_tests.append(result)\n        \n        # Count objective tests\n        for obj_name, obj_results in results[\"objective_tests\"].items():\n            for result in obj_results:\n                total_tests += 1\n                if result.passed:\n                    passed_tests += 1\n                else:\n                    failed_tests.append(result)\n        \n        # Count integration tests\n        for result in results[\"integration_tests\"]:\n            total_tests += 1\n            if result.passed:\n                passed_tests += 1\n            else:\n                failed_tests.append(result)\n        \n        # Generate report\n        report_lines = [\n            \"# SLA Validation Test Report\",\n            f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n            \"\",\n            \"## Summary\",\n            f\"- **Total Tests**: {total_tests}\",\n            f\"- **Passed**: {passed_tests}\",\n            f\"- **Failed**: {len(failed_tests)}\",\n            f\"- **Success Rate**: {(passed_tests/total_tests*100):.1f}%\" if total_tests > 0 else \"- **Success Rate**: N/A\",\n            \"\"\n        ]\n        \n        if failed_tests:\n            report_lines.extend([\n                \"## Failed Tests\",\n                \"\"\n            ])\n            \n            for failure in failed_tests:\n                report_lines.extend([\n                    f\"### ❌ {failure.test_name} ({failure.objective_name})\",\n                    f\"**Message**: {failure.message}\",\n                    \"\"\n                ])\n                \n                if failure.actual_value is not None:\n                    report_lines.append(f\"**Actual Value**: {failure.actual_value}\")\n                \n                if failure.expected_value is not None:\n                    report_lines.append(f\"**Expected Value**: {failure.expected_value}\")\n                \n                report_lines.append(\"\")\n        \n        return \"\\n\".join(report_lines)\n\n\n# Convenience function for running validation\nasync def run_sla_validation() -> Dict[str, Any]:\n    \"\"\"Run complete SLA validation suite.\"\"\"\n    test_suite = SLATestSuite()\n    results = await test_suite.run_all_tests()\n    \n    # Generate summary\n    summary = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"results\": {}\n    }\n    \n    # Convert results to dictionaries for JSON serialization\n    for category, tests in results.items():\n        if isinstance(tests, list):\n            summary[\"results\"][category] = [test.to_dict() for test in tests]\n        else:\n            summary[\"results\"][category] = {\n                obj_name: [test.to_dict() for test in obj_tests]\n                for obj_name, obj_tests in tests.items()\n            }\n    \n    # Generate text report\n    summary[\"report\"] = test_suite.generate_test_report(results)\n    \n    return summary