"""
GraphQL Schema for Chapter Extraction API
SYNTHEX Agent 7 - Flexible GraphQL Interface

This module provides a comprehensive GraphQL schema for chapter extraction
with support for complex queries, mutations, and subscriptions.
"""

import asyncio
import json
import logging
from datetime import datetime
from typing import Any, Dict, List, Optional

import graphene
from graphene import (
    ObjectType, String, Int, Float, Boolean, DateTime,
    List as GrapheneList, Field, Schema, Mutation, 
    InputObjectType, Enum as GrapheneEnum, Union,
    Interface, Argument, NonNull
)
from graphql import GraphQLError

from src.api.chapter_extraction_api import (
    ChapterExtractor, ChapterSearcher, ExtractionConfig,
    ExportFormat, ProcessingStatus, Chapter as ChapterModel,
    ExtractionRequest, SearchQuery
)
from src.core.exceptions import ValidationError

logger = logging.getLogger(__name__)

# ===== ENUMS =====

class ExportFormatEnum(GrapheneEnum):
    """Export format enumeration."""
    JSON = "json"
    MARKDOWN = "markdown"
    HTML = "html"
    PDF = "pdf"\n    DOCX = "docx"\n\nclass ProcessingStatusEnum(GrapheneEnum):\n    """Processing status enumeration."""\n    PENDING = "pending"\n    PROCESSING = "processing"\n    COMPLETED = "completed"\n    FAILED = "failed"\n    CANCELLED = "cancelled"\n\nclass SortOrderEnum(GrapheneEnum):\n    """Sort order enumeration."""\n    RELEVANCE = "relevance"\n    WORD_COUNT = "word_count"\n    TITLE = "title"\n    DATE = "date"\n\n# ===== INTERFACES =====\n\nclass Node(Interface):\n    """Node interface for objects with unique IDs."""\n    id = NonNull(String)\n\n# ===== OBJECT TYPES =====\n\nclass ChapterMetadata(ObjectType):\n    """Chapter metadata GraphQL type."""\n    id = NonNull(String)\n    title = NonNull(String)\n    number = Int()\n    level = Int(default_value=1)\n    start_line = NonNull(Int)\n    end_line = NonNull(Int)\n    word_count = NonNull(Int)\n    character_count = NonNull(Int)\n    has_subsections = Boolean(default_value=False)\n    tags = GrapheneList(String)\n    extracted_at = DateTime()\n\nclass Chapter(ObjectType):\n    """Chapter GraphQL type."""\n    class Meta:\n        interfaces = (Node,)\n\n    id = NonNull(String)\n    metadata = NonNull(Field(ChapterMetadata))\n    content = String()\n    content_preview = String(description="First 200 characters of content")\n    subsections = GrapheneList(lambda: Chapter)\n    parent_id = String()\n    parent = Field(lambda: Chapter)\n\n    def resolve_id(self, info):\n        """Resolve chapter ID."""\n        return self.metadata.id\n\n    def resolve_content_preview(self, info):\n        """Resolve content preview."""\n        if self.content:\n            return self.content[:200] + "..." if len(self.content) > 200 else self.content\n        return None\n\n    def resolve_parent(self, info):\n        """Resolve parent chapter."""\n        if self.parent_id:\n            # In a real implementation, this would fetch from database\n            return None\n        return None\n\nclass SearchResult(ObjectType):\n    """Search result GraphQL type."""\n    chapter = NonNull(Field(Chapter))\n    score = Float()\n    snippet = String()\n    highlights = GrapheneList(String)\n\nclass SearchFacet(ObjectType):\n    """Search facet GraphQL type."""\n    name = NonNull(String)\n    value = NonNull(String)\n    count = NonNull(Int)\n\nclass SearchResponse(ObjectType):\n    """Search response GraphQL type."""\n    query = NonNull(String)\n    total_results = NonNull(Int)\n    results = NonNull(GrapheneList(SearchResult))\n    facets = GrapheneList(SearchFacet)\n    took = Float(description="Query execution time in milliseconds")\n\nclass ExtractionJob(ObjectType):\n    """Extraction job GraphQL type."""\n    class Meta:\n        interfaces = (Node,)\n\n    id = NonNull(String)\n    status = NonNull(Field(ProcessingStatusEnum))\n    chapters = GrapheneList(Chapter)\n    total_chapters = Int()\n    processing_time = Float()\n    error = String()\n    export_url = String()\n    created_at = DateTime()\n    updated_at = DateTime()\n    progress = Int(description="Processing progress percentage")\n\nclass ExtractionStats(ObjectType):\n    """Extraction statistics GraphQL type."""\n    total_characters = NonNull(Int)\n    total_words = NonNull(Int)\n    total_lines = NonNull(Int)\n    total_paragraphs = NonNull(Int)\n    avg_chapter_length = Float()\n    longest_chapter = Field(Chapter)\n    shortest_chapter = Field(Chapter)\n\nclass DocumentAnalysis(ObjectType):\n    """Document analysis GraphQL type."""\n    suggested_patterns = GrapheneList(String)\n    detected_headings = GrapheneList(String)\n    structure_score = Float(description="Structure quality score 0-1")\n    stats = Field(ExtractionStats)\n\n# ===== INPUT TYPES =====\n\nclass ExtractionConfigInput(InputObjectType):\n    """Extraction configuration input type."""\n    chapter_pattern = String(default_value="^(Chapter|CHAPTER)\\s+(\\d+)")\n    section_pattern = String()\n    min_chapter_length = Int(default_value=100)\n    max_chapter_length = Int()\n    include_metadata = Boolean(default_value=True)\n    preserve_formatting = Boolean(default_value=True)\n    extract_subsections = Boolean(default_value=True)\n    custom_patterns = GrapheneList(String)\n\nclass SearchFiltersInput(InputObjectType):\n    """Search filters input type."""\n    min_words = Int()\n    max_words = Int()\n    tags = GrapheneList(String)\n    job_id = String()\n    chapter_level = Int()\n\nclass PaginationInput(InputObjectType):\n    """Pagination input type."""\n    limit = Int(default_value=10)\n    offset = Int(default_value=0)\n\n# ===== MUTATIONS =====\n\nclass ExtractChapters(Mutation):\n    """Extract chapters mutation."""\n\n    class Arguments:\n        source = NonNull(String, description="Source content (text, file path, or URL)")\n        source_type = String(default_value="text", description="Source type: text, file, or url")\n        config = Argument(ExtractionConfigInput)\n        output_format = Argument(ExportFormatEnum, default_value="json")\n        async_processing = Boolean(default_value=False)\n\n    # Output fields\n    job = Field(ExtractionJob)\n    success = NonNull(Boolean)\n    error = String()\n\n    @staticmethod\n    async def mutate(\n        root, info, source, source_type="text", config=None,\n        output_format="json", async_processing=False\n    ):\n        """Execute chapter extraction."""\n        try:\n            # Create extraction configuration\n            extraction_config = ExtractionConfig()\n            if config:\n                extraction_config = ExtractionConfig(**config)\n\n            # Create request\n            request_data = {\n                "config": extraction_config,\n                "output_format": ExportFormat(output_format),\n                "async_processing": async_processing\n            }\n\n            if source_type == "text":\n                request_data["content"] = source\n            elif source_type == "file":\n                request_data["file_path"] = source\n            elif source_type == "url":\n                request_data["url"] = source\n            else:\n                raise ValidationError(f"Invalid source_type: {source_type}")\n\n            request = ExtractionRequest(**request_data)\n\n            # Extract chapters\n            extractor = ChapterExtractor()\n\n            if async_processing:\n                # Start async processing\n                from uuid import uuid4\n                job_id = str(uuid4())\n\n                # In real implementation, this would queue the job\n                # For now, return pending job\n                return ExtractChapters(\n                    job=ExtractionJob(\n                        id=job_id,\n                        status=ProcessingStatus.PENDING,\n                        total_chapters=0,\n                        created_at=datetime.utcnow()\n                    ),\n                    success=True\n                )\n            else:\n                # Process synchronously\n                chapters = await extractor.extract(\n                    source if source_type != "text" else request.content,\n                    extraction_config\n                )\n\n                from uuid import uuid4\n                job_id = str(uuid4())\n\n                return ExtractChapters(\n                    job=ExtractionJob(\n                        id=job_id,\n                        status=ProcessingStatus.COMPLETED,\n                        chapters=chapters,\n                        total_chapters=len(chapters),\n                        created_at=datetime.utcnow(),\n                        updated_at=datetime.utcnow()\n                    ),\n                    success=True\n                )\n\n        except Exception as e:\n            logger.error(f"Chapter extraction failed: {str(e)}")\n            return ExtractChapters(\n                job=None,\n                success=False,\n                error=str(e)\n            )\n\nclass BatchExtractChapters(Mutation):\n    """Batch extract chapters mutation."""\n\n    class Arguments:\n        sources = NonNull(GrapheneList(String))\n        config = Argument(ExtractionConfigInput)\n        parallel_processing = Boolean(default_value=True)\n        max_concurrent = Int(default_value=5)\n\n    # Output fields\n    jobs = GrapheneList(ExtractionJob)\n    success = NonNull(Boolean)\n    error = String()\n\n    @staticmethod\n    async def mutate(\n        root, info, sources, config=None,\n        parallel_processing=True, max_concurrent=5\n    ):\n        """Execute batch chapter extraction."""\n        try:\n            # Create extraction configuration\n            extraction_config = ExtractionConfig()\n            if config:\n                extraction_config = ExtractionConfig(**config)\n\n            extractor = ChapterExtractor()\n            jobs = []\n\n            if parallel_processing:\n                # Process with concurrency limit\n                semaphore = asyncio.Semaphore(max_concurrent)\n\n                async def process_source(source):\n                    async with semaphore:\n                        from uuid import uuid4\n                        job_id = str(uuid4())\n\n                        try:\n                            chapters = await extractor.extract(source, extraction_config)\n\n                            return ExtractionJob(\n                                id=job_id,\n                                status=ProcessingStatus.COMPLETED,\n                                chapters=chapters,\n                                total_chapters=len(chapters),\n                                created_at=datetime.utcnow(),\n                                updated_at=datetime.utcnow()\n                            )\n                        except Exception as e:\n                            return ExtractionJob(\n                                id=job_id,\n                                status=ProcessingStatus.FAILED,\n                                error=str(e),\n                                total_chapters=0,\n                                created_at=datetime.utcnow(),\n                                updated_at=datetime.utcnow()\n                            )\n\n                # Process all sources\n                tasks = [process_source(source) for source in sources]\n                jobs = await asyncio.gather(*tasks)\n\n            else:\n                # Process sequentially\n                for source in sources:\n                    from uuid import uuid4\n                    job_id = str(uuid4())\n\n                    try:\n                        chapters = await extractor.extract(source, extraction_config)\n\n                        jobs.append(ExtractionJob(\n                            id=job_id,\n                            status=ProcessingStatus.COMPLETED,\n                            chapters=chapters,\n                            total_chapters=len(chapters),\n                            created_at=datetime.utcnow(),\n                            updated_at=datetime.utcnow()\n                        ))\n                    except Exception as e:\n                        jobs.append(ExtractionJob(\n                            id=job_id,\n                            status=ProcessingStatus.FAILED,\n                            error=str(e),\n                            total_chapters=0,\n                            created_at=datetime.utcnow(),\n                            updated_at=datetime.utcnow()\n                        ))\n\n            return BatchExtractChapters(\n                jobs=jobs,\n                success=True\n            )\n\n        except Exception as e:\n            logger.error(f"Batch extraction failed: {str(e)}")\n            return BatchExtractChapters(\n                jobs=[],\n                success=False,\n                error=str(e)\n            )\n\nclass UpdateExtractionConfig(Mutation):\n    """Update extraction configuration mutation."""\n\n    class Arguments:\n        job_id = NonNull(String)\n        config = NonNull(Argument(ExtractionConfigInput))\n\n    # Output fields\n    job = Field(ExtractionJob)\n    success = NonNull(Boolean)\n    error = String()\n\n    @staticmethod\n    async def mutate(root, info, job_id, config):\n        """Update extraction configuration and re-extract."""\n        try:\n            # In real implementation, this would update the job config\n            # and potentially re-run extraction\n\n            return UpdateExtractionConfig(\n                job=None,  # Would return updated job\n                success=True\n            )\n\n        except Exception as e:\n            logger.error(f"Config update failed: {str(e)}")\n            return UpdateExtractionConfig(\n                job=None,\n                success=False,\n                error=str(e)\n            )\n\nclass CancelExtractionJob(Mutation):\n    """Cancel extraction job mutation."""\n\n    class Arguments:\n        job_id = NonNull(String)\n\n    # Output fields\n    job = Field(ExtractionJob)\n    success = NonNull(Boolean)\n    error = String()\n\n    @staticmethod\n    async def mutate(root, info, job_id):\n        """Cancel an extraction job."""\n        try:\n            # In real implementation, this would cancel the job\n\n            return CancelExtractionJob(\n                job=ExtractionJob(\n                    id=job_id,\n                    status=ProcessingStatus.CANCELLED,\n                    total_chapters=0,\n                    updated_at=datetime.utcnow()\n                ),\n                success=True\n            )\n\n        except Exception as e:\n            logger.error(f"Job cancellation failed: {str(e)}")\n            return CancelExtractionJob(\n                job=None,\n                success=False,\n                error=str(e)\n            )\n\n# ===== QUERIES =====\n\nclass Query(ObjectType):\n    """GraphQL root query."""\n\n    # Single object queries\n    chapter = Field(\n        Chapter,\n        id=NonNull(String),\n        description="Get a chapter by ID"\n    )\n\n    job = Field(\n        ExtractionJob,\n        id=NonNull(String),\n        description="Get an extraction job by ID"\n    )\n\n    # List queries\n    chapters = Field(\n        GrapheneList(Chapter),\n        job_id=String(),\n        limit=Int(default_value=10),\n        offset=Int(default_value=0),\n        description="Get chapters, optionally filtered by job ID"\n    )\n\n    jobs = Field(\n        GrapheneList(ExtractionJob),\n        status=Argument(ProcessingStatusEnum),\n        limit=Int(default_value=10),\n        offset=Int(default_value=0),\n        description="Get extraction jobs, optionally filtered by status"\n    )\n\n    # Search\n    search_chapters = Field(\n        SearchResponse,\n        query=NonNull(String),\n        filters=Argument(SearchFiltersInput),\n        pagination=Argument(PaginationInput),\n        sort_by=Argument(SortOrderEnum, default_value="relevance"),\n        include_content=Boolean(default_value=False),\n        description="Search through extracted chapters"\n    )\n\n    # Analysis\n    analyze_document = Field(\n        DocumentAnalysis,\n        source=NonNull(String),\n        source_type=String(default_value="text"),\n        description="Analyze document structure and suggest extraction patterns"\n    )\n\n    # Statistics\n    extraction_stats = Field(\n        ExtractionStats,\n        job_id=String(),\n        description="Get extraction statistics, optionally for a specific job"\n    )\n\n    # Resolvers\n    async def resolve_chapter(self, info, id):\n        """Resolve single chapter."""\n        # In real implementation, fetch from database\n        return None\n\n    async def resolve_job(self, info, id):\n        """Resolve single job."""\n        # In real implementation, fetch from database\n        return None\n\n    async def resolve_chapters(self, info, job_id=None, limit=10, offset=0):\n        """Resolve chapters list."""\n        # In real implementation, fetch from database with filters\n        return []\n\n    async def resolve_jobs(self, info, status=None, limit=10, offset=0):\n        """Resolve jobs list."""\n        # In real implementation, fetch from database with filters\n        return []\n\n    async def resolve_search_chapters(\n        self, info, query, filters=None, pagination=None,\n        sort_by="relevance", include_content=False\n    ):\n        """Resolve chapter search."""\n        try:\n            searcher = ChapterSearcher()\n\n            # Build search query\n            search_filters = {}\n            if filters:\n                if filters.get("min_words"):\n                    search_filters["min_words"] = filters["min_words"]\n                if filters.get("max_words"):\n                    search_filters["max_words"] = filters["max_words"]\n                if filters.get("tags"):\n                    search_filters["tags"] = filters["tags"]\n                if filters.get("job_id"):\n                    search_filters["job_id"] = filters["job_id"]\n\n            limit = 10\n            offset = 0\n            if pagination:\n                limit = pagination.get("limit", 10)\n                offset = pagination.get("offset", 0)\n\n            search_query = SearchQuery(\n                query=query,\n                filters=search_filters,\n                limit=limit,\n                offset=offset,\n                sort_by=sort_by,\n                include_content=include_content\n            )\n\n            # Execute search\n            start_time = datetime.utcnow()\n            results = await searcher.search(search_query)\n            end_time = datetime.utcnow()\n\n            took = (end_time - start_time).total_seconds() * 1000\n\n            # Convert to GraphQL types\n            search_results = []\n            for result in results.get("chapters", []):\n                # Convert result to SearchResult\n                # This would need proper conversion logic\n                pass\n\n            facets = []\n            for facet_name, facet_values in results.get("facets", {}).items():\n                if isinstance(facet_values, dict):\n                    for value, count in facet_values.items():\n                        facets.append(SearchFacet(\n                            name=facet_name,\n                            value=str(value),\n                            count=count\n                        ))\n\n            return SearchResponse(\n                query=query,\n                total_results=results.get("total", 0),\n                results=search_results,\n                facets=facets,\n                took=took\n            )\n\n        except Exception as e:\n            logger.error(f"Search failed: {str(e)}")\n            raise GraphQLError(f"Search failed: {str(e)}")\n\n    async def resolve_analyze_document(self, info, source, source_type="text"):\n        """Resolve document analysis."""\n        try:\n            extractor = ChapterExtractor()\n\n            # Load content\n            content = await extractor._load_content(source)\n\n            # Perform analysis\n            analysis_result = {\n                "suggested_patterns": [\n                    "^Chapter\\s+(\\d+)",\n                    "^CHAPTER\\s+(\\d+)",\n                    "^(\\d+)\\."\n                ],\n                "detected_headings": [],\n                "structure_score": 0.8,\n                "stats": ExtractionStats(\n                    total_characters=len(content),\n                    total_words=len(content.split()),\n                    total_lines=len(content.split('\n')),\n                    total_paragraphs=len([p for p in content.split('\n\n') if p.strip()]),\n                    avg_chapter_length=0.0\n                )\n            }\n\n            return DocumentAnalysis(**analysis_result)\n\n        except Exception as e:\n            logger.error(f"Document analysis failed: {str(e)}")\n            raise GraphQLError(f"Document analysis failed: {str(e)}")\n\n    async def resolve_extraction_stats(self, info, job_id=None):\n        """Resolve extraction statistics."""\n        # In real implementation, calculate stats from database\n        return ExtractionStats(\n            total_characters=0,\n            total_words=0,\n            total_lines=0,\n            total_paragraphs=0,\n            avg_chapter_length=0.0\n        )\n\n# ===== MUTATIONS =====\n\nclass Mutation(ObjectType):\n    """GraphQL root mutation."""\n\n    extract_chapters = ExtractChapters.Field()\n    batch_extract_chapters = BatchExtractChapters.Field()\n    update_extraction_config = UpdateExtractionConfig.Field()\n    cancel_extraction_job = CancelExtractionJob.Field()\n\n# ===== SUBSCRIPTIONS =====\n\nclass Subscription(ObjectType):\n    """GraphQL subscriptions for real-time updates."""\n\n    job_status_updated = Field(\n        ExtractionJob,\n        job_id=NonNull(String),\n        description="Subscribe to job status updates"\n    )\n\n    chapter_extracted = Field(\n        Chapter,\n        job_id=String(),\n        description="Subscribe to newly extracted chapters"\n    )\n\n    extraction_progress = Field(\n        String,  # JSON string with progress info\n        job_id=NonNull(String),\n        description="Subscribe to extraction progress updates"\n    )\n\n    async def resolve_job_status_updated(self, info, job_id):\n        """Subscribe to job status updates."""\n        # In real implementation, this would yield updates as they occur\n        # For now, this is a placeholder\n        yield ExtractionJob(\n            id=job_id,\n            status=ProcessingStatus.PROCESSING,\n            total_chapters=0,\n            progress=50,\n            updated_at=datetime.utcnow()\n        )\n\n    async def resolve_chapter_extracted(self, info, job_id=None):\n        """Subscribe to newly extracted chapters."""\n        # Placeholder implementation\n        pass\n\n    async def resolve_extraction_progress(self, info, job_id):\n        """Subscribe to extraction progress."""\n        # Placeholder implementation\n        pass\n\n# ===== SCHEMA =====\n\nschema = Schema(\n    query=Query,\n    mutation=Mutation,\n    subscription=Subscription\n)\n\n# ===== SCHEMA DEFINITION STRING =====\n\nschema_definition = """\n# Chapter Extraction GraphQL Schema\n# SYNTHEX Agent 7 - Flexible API Interface\n\nscalar DateTime\n\ninterface Node {\n  id: ID!\n}\n\nenum ExportFormat {\n  JSON\n  MARKDOWN\n  HTML\n  PDF\n  DOCX\n}\n\nenum ProcessingStatus {\n  PENDING\n  PROCESSING\n  COMPLETED\n  FAILED\n  CANCELLED\n}\n\nenum SortOrder {\n  RELEVANCE\n  WORD_COUNT\n  TITLE\n  DATE\n}\n\ntype ChapterMetadata {\n  id: ID!\n  title: String!\n  number: Int\n  level: Int\n  startLine: Int!\n  endLine: Int!\n  wordCount: Int!\n  characterCount: Int!\n  hasSubsections: Boolean\n  tags: [String!]!\n  extractedAt: DateTime\n}\n\ntype Chapter implements Node {\n  id: ID!\n  metadata: ChapterMetadata!\n  content: String\n  contentPreview: String\n  subsections: [Chapter!]!\n  parentId: String\n  parent: Chapter\n}\n\ntype SearchResult {\n  chapter: Chapter!\n  score: Float\n  snippet: String\n  highlights: [String!]!\n}\n\ntype SearchFacet {\n  name: String!\n  value: String!\n  count: Int!\n}\n\ntype SearchResponse {\n  query: String!\n  totalResults: Int!\n  results: [SearchResult!]!\n  facets: [SearchFacet!]!\n  took: Float\n}\n\ntype ExtractionJob implements Node {\n  id: ID!\n  status: ProcessingStatus!\n  chapters: [Chapter!]!\n  totalChapters: Int\n  processingTime: Float\n  error: String\n  exportUrl: String\n  createdAt: DateTime\n  updatedAt: DateTime\n  progress: Int\n}\n\ntype ExtractionStats {\n  totalCharacters: Int!\n  totalWords: Int!\n  totalLines: Int!\n  totalParagraphs: Int!\n  avgChapterLength: Float\n  longestChapter: Chapter\n  shortestChapter: Chapter\n}\n\ntype DocumentAnalysis {\n  suggestedPatterns: [String!]!\n  detectedHeadings: [String!]!\n  structureScore: Float\n  stats: ExtractionStats\n}\n\ninput ExtractionConfigInput {\n  chapterPattern: String = "^(Chapter|CHAPTER)\\\\s+(\\\\d+)"\n  sectionPattern: String\n  minChapterLength: Int = 100\n  maxChapterLength: Int\n  includeMetadata: Boolean = true\n  preserveFormatting: Boolean = true\n  extractSubsections: Boolean = true\n  customPatterns: [String!]\n}\n\ninput SearchFiltersInput {\n  minWords: Int\n  maxWords: Int\n  tags: [String!]\n  jobId: String\n  chapterLevel: Int\n}\n\ninput PaginationInput {\n  limit: Int = 10\n  offset: Int = 0\n}\n\ntype Query {\n  chapter(id: ID!): Chapter\n  job(id: ID!): ExtractionJob\n  chapters(jobId: String, limit: Int = 10, offset: Int = 0): [Chapter!]!\n  jobs(status: ProcessingStatus, limit: Int = 10, offset: Int = 0): [ExtractionJob!]!\n  searchChapters(\n    query: String!\n    filters: SearchFiltersInput\n    pagination: PaginationInput\n    sortBy: SortOrder = RELEVANCE\n    includeContent: Boolean = false\n  ): SearchResponse!\n  analyzeDocument(source: String!, sourceType: String = "text"): DocumentAnalysis\n  extractionStats(jobId: String): ExtractionStats\n}\n\ntype Mutation {\n  extractChapters(\n    source: String!\n    sourceType: String = "text"\n    config: ExtractionConfigInput\n    outputFormat: ExportFormat = JSON\n    asyncProcessing: Boolean = false\n  ): ExtractChaptersResult!\n\n  batchExtractChapters(\n    sources: [String!]!\n    config: ExtractionConfigInput\n    parallelProcessing: Boolean = true\n    maxConcurrent: Int = 5\n  ): BatchExtractChaptersResult!\n\n  updateExtractionConfig(\n    jobId: ID!\n    config: ExtractionConfigInput!\n  ): UpdateExtractionConfigResult!\n\n  cancelExtractionJob(jobId: ID!): CancelExtractionJobResult!\n}\n\ntype Subscription {\n  jobStatusUpdated(jobId: ID!): ExtractionJob\n  chapterExtracted(jobId: String): Chapter\n  extractionProgress(jobId: ID!): String\n}\n\ntype ExtractChaptersResult {\n  job: ExtractionJob\n  success: Boolean!\n  error: String\n}\n\ntype BatchExtractChaptersResult {\n  jobs: [ExtractionJob!]!\n  success: Boolean!\n  error: String\n}\n\ntype UpdateExtractionConfigResult {\n  job: ExtractionJob\n  success: Boolean!\n  error: String\n}\n\ntype CancelExtractionJobResult {\n  job: ExtractionJob\n  success: Boolean!\n  error: String\n}\n"""\n\n# Export schema and utilities\n__all__ = [\n    'schema',\n    'schema_definition',\n    'Query',\n    'Mutation',\n    'Subscription',\n    'Chapter',\n    'ExtractionJob',\n    'SearchResponse'\n]