"""
Commercial expert clients for GPT-4, Gemini, and other paid APIs.
"""

from __future__ import annotations
import os
import asyncio
from typing import Optional, Dict, Any, List
import json
import logging
from datetime import datetime

from openai import AsyncOpenAI
import google.generativeai as genai
import aiohttp

from src.circle_of_experts.models.response import ExpertResponse, ExpertType, ResponseStatus
from src.circle_of_experts.models.query import ExpertQuery
from src.core.retry import retry_api_call, RetryConfig, RetryStrategy
from src.circle_of_experts.experts.claude_expert import BaseExpertClient
from src.core.connections import get_connection_manager, ConnectionPoolConfig
from src.core.circuit_breaker import CircuitBreakerConfig, get_circuit_breaker_manager

__all__ = [
    "GPT4ExpertClient",
    "GeminiExpertClient",
    "GroqExpertClient",
    "DeepSeekExpertClient"
]


logger = logging.getLogger(__name__)


class GPT4ExpertClient(BaseExpertClient):
    """
    OpenAI GPT-4 expert client.
    
    Supports GPT-4, GPT-4-Turbo, and GPT-3.5-Turbo models.
    """
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = "gpt-4-turbo-preview",
        organization: Optional[str] = None
    ):
        """
        Initialize GPT-4 client.
        
        Args:
            api_key: OpenAI API key
            model: Model to use (gpt-4, gpt-4-turbo-preview, gpt-3.5-turbo)
            organization: Optional organization ID
        """
        super().__init__(api_key or os.getenv("OPENAI_API_KEY"))
        self.model = model
        self.organization = organization or os.getenv("OPENAI_ORGANIZATION")
        self.client = AsyncOpenAI(
            api_key=self.api_key,
            organization=self.organization
        ) if self.api_key else None
        
        # Model selection based on requirements
        self.model_selection = {
            "high_quality": "gpt-4-turbo-preview",
            "balanced": "gpt-4",
            "fast": "gpt-3.5-turbo-16k"
        }
    
    def _select_model_for_query(self, query: ExpertQuery) -> str:
        """Select appropriate GPT model based on query."""
        if query.priority == "critical" or query.query_type == "architectural":
            return self.model_selection["high_quality"]
        elif query.priority == "low" or len(query.content) < 500:
            return self.model_selection["fast"]
        else:
            return self.model_selection["balanced"]
    
    def _create_messages(self, query: ExpertQuery) -> List[Dict[str, str]]:
        """Create optimized messages for GPT-4."""
        system_message = """You are an expert consultant in the Circle of Experts system.
Provide detailed, actionable analysis with:
- Clear reasoning and explanations
- Specific code examples when relevant
- Best practices and industry standards
- Honest assessment of trade-offs and limitations

Format your response with clear sections and use markdown for structure."""
        
        # Add query-specific instructions
        if query.query_type == "review":
            system_message += "

Focus on: Code quality, potential bugs, performance, and maintainability."
        elif query.query_type == "optimization":
            system_message += "

Focus on: Performance improvements, resource efficiency, and scalability."
        
        user_message = query.content
        if query.context:
            user_message += f"\n\nContext: {json.dumps(query.context)}"\n        if query.constraints:\n            user_message += f"\n\nConstraints: {', '.join(query.constraints)}"\n\n        return [\n            {"role": "system", "content": system_message},\n            {"role": "user", "content": user_message}\n        ]\n\n    async def generate_response(self, query: ExpertQuery) -> ExpertResponse:\n        """Generate response using GPT-4."""\n        if not self.client:\n            raise ValueError("OpenAI API key not configured")\n\n        response = ExpertResponse(\n            query_id=query.id,\n            expert_type=ExpertType.GPT4,\n            status=ResponseStatus.IN_PROGRESS\n        )\n\n        try:\n            # Select model\n            model = self._select_model_for_query(query)\n            logger.info(f"Using GPT model: {model}")\n\n            # Get circuit breaker for this expert\n            manager = get_circuit_breaker_manager()\n            breaker = await manager.get_or_create(\n                f"gpt4_expert_{model}",\n                CircuitBreakerConfig(\n                    failure_threshold=3,\n                    timeout=60,\n                    failure_rate_threshold=0.5,\n                    minimum_calls=5,\n                    fallback=lambda: self._create_fallback_response(query)\n                )\n            )\n\n            # Generate response with circuit breaker protection\n            completion = await breaker.call(\n                self._api_call_with_retry,\n                model,\n                self._create_messages(query)\n            )\n\n            # Extract content\n            content = completion.choices[0].message.content\n\n            response.content = content\n            response.confidence = self._calculate_confidence(content, completion)\n            response.recommendations = self._extract_recommendations(content)\n            response.code_snippets = self._extract_code_snippets(content)\n\n            # Add metadata\n            response.metadata = {\n                "model": model,\n                "usage": {\n                    "prompt_tokens": completion.usage.prompt_tokens,\n                    "completion_tokens": completion.usage.completion_tokens,\n                    "total_tokens": completion.usage.total_tokens\n                },\n                "finish_reason": completion.choices[0].finish_reason\n            }\n\n            response.mark_completed()\n\n        except Exception as e:\n            logger.error(f"GPT-4 generation failed: {e}")\n            response.mark_failed(str(e))\n\n        return response\n\n    async def health_check(self) -> bool:\n        """Check OpenAI API availability."""\n        if not self.client:\n            return False\n\n        try:\n            # Simple test\n            await self.client.chat.completions.create(\n                model="gpt-3.5-turbo",\n                messages=[{"role": "user", "content": "Hi"}],\n                max_tokens=5\n            )\n            return True\n        except Exception as e:\n            logger.error(f"GPT-4 health check failed: {e}")\n            return False\n\n    def _calculate_confidence(self, content: str, completion) -> float:\n        """Calculate confidence based on response quality."""\n        confidence = 0.8  # Base confidence for GPT-4\n\n        # Adjust based on finish reason\n        if completion.choices[0].finish_reason == "stop":\n            confidence += 0.05\n\n        # Adjust based on content quality\n        if len(content) > 1000:\n            confidence += 0.05\n        if "```" in content:\n            confidence += 0.05\n\n        return min(1.0, confidence)\n\n    def _extract_recommendations(self, content: str) -> List[str]:\n        """Extract recommendations from GPT-4 response."""\n        recommendations = []\n        lines = content.split('\n')\n\n        in_recommendations = False\n        for line in lines:\n            line = line.strip()\n\n            if any(marker in line.lower() for marker in ["recommend", "suggestion", "advice"]):\n                in_recommendations = True\n\n            if in_recommendations and line.startswith(('-', '*', '•', '1.', '2.')):\n                rec = line.lstrip('-*•1234567890. ')\n                if rec and len(rec) > 10:\n                    recommendations.append(rec)\n\n            if in_recommendations and line == "":\n                in_recommendations = False\n\n        return recommendations[:10]\n\n    def _extract_code_snippets(self, content: str) -> List[Dict[str, str]]:\n        """Extract code snippets from response."""\n        snippets = []\n        parts = content.split('```')\n\n        for i in range(1, len(parts), 2):\n            if i < len(parts):\n                lines = parts[i].split('\n', 1)\n                language = lines[0].strip() if lines else ""\n                code = lines[1] if len(lines) > 1 else ""\n\n                if code.strip():\n                    snippets.append({\n                        "language": language or "text",\n                        "code": code.strip(),\n                        "title": f"Code Example {len(snippets) + 1}"\n                    })\n\n        return snippets\n\n    @retry_api_call(max_attempts=5, timeout=120)\n    async def _api_call_with_retry(self, model: str, messages: List[Dict[str, str]]):\n        """Make API call with retry logic."""\n        return await self.client.chat.completions.create(\n            model=model,\n            messages=messages,\n            temperature=0.7,\n            max_tokens=4096,\n            top_p=0.95,\n            presence_penalty=0.1,\n            frequency_penalty=0.1\n        )\n\n    def _create_fallback_response(self, query: ExpertQuery) -> ExpertResponse:\n        """Create fallback response when circuit is open."""\n        response = ExpertResponse(\n            query_id=query.id,\n            expert_type=ExpertType.GPT4,\n            status=ResponseStatus.FAILED\n        )\n        response.content = "OpenAI API is currently unavailable. The circuit breaker has been triggered due to repeated failures. Please try again later."\n        response.confidence = 0.0\n        response.metadata = {\n            "fallback": True,\n            "reason": "circuit_breaker_open"\n        }\n        return response\n\n\nclass GeminiExpertClient(BaseExpertClient):\n    """\n    Advanced Google Gemini expert client with intelligent model selection.\n\n    Supports multiple Gemini models with smart routing based on query characteristics.\n    """\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        model: str = "gemini-1.5-flash"\n    ):\n        """Initialize advanced Gemini client."""\n        super().__init__(api_key or os.getenv("GOOGLE_GEMINI_API_KEY"))\n        self.default_model = model\n\n        # Advanced model selection matrix\n        self.model_options = {\n            "experimental": "gemini-2.0-flash-exp",\n            "thinking": "gemini-2.0-flash-thinking-exp",\n            "high_quality": "gemini-2.0-flash",\n            "balanced": "gemini-1.5-flash",\n            "large_context": "gemini-1.5-pro",\n            "cost_optimized": "gemini-1.5-flash-8b",\n            "default": "gemini-1.5-flash"\n        }\n\n        # Model capabilities for intelligent selection\n        self.model_capabilities = {\n            "gemini-2.0-flash-exp": {"reasoning": 10, "speed": 9, "cost": 7, "reliability": 7},\n            "gemini-2.0-flash-thinking-exp": {"reasoning": 10, "speed": 8, "cost": 7, "reliability": 7},\n            "gemini-2.0-flash": {"reasoning": 9, "speed": 9, "cost": 8, "reliability": 9},\n            "gemini-1.5-pro": {"reasoning": 9, "speed": 7, "cost": 6, "reliability": 10},\n            "gemini-1.5-flash": {"reasoning": 8, "speed": 10, "cost": 10, "reliability": 9},\n            "gemini-1.5-flash-8b": {"reasoning": 7, "speed": 10, "cost": 10, "reliability": 9}\n        }\n\n        if self.api_key:\n            genai.configure(api_key=self.api_key)\n            # Initialize with default model, will be selected dynamically per query\n            self.client = None\n        else:\n            self.client = None\n\n    def _select_optimal_model(self, query: ExpertQuery) -> str:\n        """Select optimal Gemini model based on query characteristics."""\n        content_length = len(query.content)\n\n        # Priority-based selection\n        if query.priority == "critical":\n            return self.model_options["experimental"]\n\n        # Query type-based selection\n        if query.query_type == "architectural":\n            return self.model_options["thinking"]  # Reasoning transparency\n        elif query.query_type == "review" and content_length > 10000:\n            return self.model_options["large_context"]  # Large context for code review\n        elif query.query_type == "optimization":\n            return self.model_options["high_quality"]  # Latest capabilities\n\n        # Content length considerations\n        if content_length > 50000:\n            return self.model_options["large_context"]  # 2M context window\n\n        # Cost optimization for low priority\n        if query.priority == "low":\n            return self.model_options["cost_optimized"]\n\n        # Default balanced choice\n        return self.model_options["balanced"]\n\n    def _get_fallback_models(self, primary_model: str) -> List[str]:\n        """Get fallback model chain for reliability."""\n        fallback_chains = {\n            "gemini-2.0-flash-exp": ["gemini-2.0-flash", "gemini-1.5-flash"],\n            "gemini-2.0-flash-thinking-exp": ["gemini-1.5-pro", "gemini-1.5-flash"],\n            "gemini-2.0-flash": ["gemini-1.5-flash", "gemini-1.5-flash-8b"],\n            "gemini-1.5-pro": ["gemini-2.0-flash", "gemini-1.5-flash"],\n            "gemini-1.5-flash": ["gemini-1.5-flash-8b"],\n            "gemini-1.5-flash-8b": ["gemini-1.5-flash"]\n        }\n        return fallback_chains.get(primary_model, ["gemini-1.5-flash"])\n\n    @retry_api_call(max_attempts=5, timeout=120)\n    async def generate_response(self, query: ExpertQuery) -> ExpertResponse:\n        """Generate response using advanced Gemini model selection."""\n        if not self.api_key:\n            raise ValueError("Gemini API key not configured")\n\n        response = ExpertResponse(\n            query_id=query.id,\n            expert_type=ExpertType.GEMINI,\n            status=ResponseStatus.IN_PROGRESS\n        )\n\n        # Select optimal model\n        selected_model = self._select_optimal_model(query)\n        fallback_models = self._get_fallback_models(selected_model)\n\n        models_to_try = [selected_model] + fallback_models\n\n        for model_name in models_to_try:\n            try:\n                logger.info(f"Attempting Gemini model: {model_name}")\n\n                # Create model instance\n                model = genai.GenerativeModel(model_name)\n\n                # Create prompt\n                prompt = self._create_prompt(query)\n\n                # Configure generation parameters based on model\n                generation_config = self._get_generation_config(model_name, query)\n\n                # Generate response\n                result = await asyncio.to_thread(\n                    model.generate_content,\n                    prompt,\n                    generation_config=generation_config\n                )\n\n                # Extract content\n                content = result.text\n\n                response.content = content\n                response.confidence = self._calculate_confidence(content, model_name, query)\n                response.recommendations = self._extract_recommendations(content)\n                response.code_snippets = self._extract_code_snippets(content)\n\n                # Add metadata with model selection info\n                response.metadata = {\n                    "selected_model": model_name,\n                    "was_fallback": model_name != selected_model,\n                    "model_capabilities": self.model_capabilities.get(model_name, {}),\n                    "prompt_token_count": getattr(result.usage_metadata, 'prompt_token_count', 0),\n                    "candidates_token_count": getattr(result.usage_metadata, 'candidates_token_count', 0),\n                    "total_token_count": getattr(result.usage_metadata, 'total_token_count', 0)\n                }\n\n                response.mark_completed()\n                logger.info(f"✅ Gemini success with {model_name}")\n                return response\n\n            except Exception as e:\n                logger.warning(f"Gemini model {model_name} failed: {e}")\n                if model_name == models_to_try[-1]:  # Last model in chain\n                    response.mark_failed(f"All Gemini models failed. Last error: {str(e)}")\n                    return response\n                continue\n\n        return response\n\n    def _get_generation_config(self, model_name: str, query: ExpertQuery) -> Dict[str, Any]:\n        """Get optimized generation config based on model and query."""\n        base_config = {\n            "temperature": 0.7,\n            "top_p": 0.95,\n            "max_output_tokens": 4096,\n        }\n\n        # Adjust for thinking models (more structured output)\n        if "thinking" in model_name:\n            base_config["temperature"] = 0.5  # More focused\n\n        # Adjust for experimental models (allow creativity)\n        elif "exp" in model_name:\n            base_config["temperature"] = 0.8\n\n        # Adjust for critical queries (more output)\n        if query.priority == "critical":\n            base_config["max_output_tokens"] = 8192\n\n        return base_config\n\n    def _calculate_confidence(self, content: str, model_name: str, query: ExpertQuery) -> float:\n        """Calculate confidence based on model capabilities and response quality."""\n        # Base confidence from model capabilities\n        model_caps = self.model_capabilities.get(model_name, {})\n        base_confidence = model_caps.get("reasoning", 8) / 10.0\n\n        # Adjust based on response quality indicators\n        quality_indicators = 0\n        if len(content) > 500:\n            quality_indicators += 0.05\n        if "```" in content:  # Contains code examples\n            quality_indicators += 0.05\n        if any(word in content.lower() for word in ["analysis", "recommendation", "solution"]):\n            quality_indicators += 0.03\n        if "thinking" in model_name and any(word in content.lower() for word in ["reasoning", "step", "therefore"]):\n            quality_indicators += 0.07  # Bonus for thinking models showing reasoning\n\n        return min(0.98, base_confidence + quality_indicators)\n\n    async def health_check(self) -> bool:\n        """Check Gemini API availability with advanced model support."""\n        if not self.api_key:\n            return False\n\n        try:\n            # Test with default reliable model\n            test_model = genai.GenerativeModel(self.model_options["balanced"])\n\n            # Simple test\n            await asyncio.to_thread(\n                test_model.generate_content,\n                "Hello",\n                generation_config={"max_output_tokens": 10}\n            )\n            return True\n        except Exception as e:\n            logger.error(f"Gemini health check failed: {e}")\n            return False\n\n    def _create_prompt(self, query: ExpertQuery) -> str:\n        """Create optimized prompt for Gemini."""\n        prompt = f"""As an expert consultant, provide a detailed analysis for this query:\n\nQuery Type: {query.query_type}\nPriority: {query.priority}\n\n{query.content}\n\nProvide:\n1. Comprehensive analysis\n2. Specific recommendations with examples\n3. Code examples where applicable\n4. Potential limitations or considerations\n\nUse clear formatting with sections and markdown."""\n\n        if query.context:\n            prompt += f"\n\nContext: {json.dumps(query.context)}"\n\n        return prompt\n\n    def _extract_recommendations(self, content: str) -> List[str]:\n        """Extract recommendations from Gemini response."""\n        recommendations = []\n        lines = content.split('\n')\n\n        for line in lines:\n            line = line.strip()\n            if line.startswith(('-', '*', '•', '1.', '2.', '3.')):\n                if any(word in content[max(0, content.find(line)-100):content.find(line)].lower()\n                       for word in ["recommend", "suggest", "advice"]):\n                    rec = line.lstrip('-*•1234567890. ')\n                    if rec:\n                        recommendations.append(rec)\n\n        return recommendations[:10]\n\n    def _extract_code_snippets(self, content: str) -> List[Dict[str, str]]:\n        """Extract code snippets from response."""\n        snippets = []\n\n        # Find code blocks\n        import re\n        code_blocks = re.findall(r'```(\w*)\n(.*?)\n```', content, re.DOTALL)\n\n        for i, (language, code) in enumerate(code_blocks):\n            if code.strip():\n                snippets.append({\n                    "language": language or "text",\n                    "code": code.strip(),\n                    "title": f"Example {i + 1}"\n                })\n\n        return snippets\n\n\nclass GroqExpertClient(BaseExpertClient):\n    """\n    Groq expert client for fast inference.\n\n    Supports various open models with high-speed inference.\n    """\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        model: str = "mixtral-8x7b-32768"\n    ):\n        """Initialize Groq client."""\n        super().__init__(api_key or os.getenv("GROQ_API_KEY"))\n        self.model = model\n        self.base_url = "https://api.groq.com/openai/v1"\n\n    async def generate_response(self, query: ExpertQuery) -> ExpertResponse:\n        """Generate response using Groq."""\n        if not self.api_key:\n            raise ValueError("Groq API key not configured")\n\n        response = ExpertResponse(\n            query_id=query.id,\n            expert_type=ExpertType.SUPERGROK,\n            status=ResponseStatus.IN_PROGRESS\n        )\n\n        try:\n            # Get circuit breaker for this expert\n            manager = get_circuit_breaker_manager()\n            breaker = await manager.get_or_create(\n                f"groq_expert_{self.model}",\n                CircuitBreakerConfig(\n                    failure_threshold=3,\n                    timeout=60,\n                    failure_rate_threshold=0.5,\n                    minimum_calls=5,\n                    fallback=lambda: self._create_fallback_response(query)\n                )\n            )\n\n            # Generate response with circuit breaker protection\n            result = await breaker.call(\n                self._groq_api_call,\n                query\n            )\n\n            response.content = result['content']\n            response.confidence = result['confidence']\n            response.metadata = result['metadata']\n            response.mark_completed()\n\n        except Exception as e:\n            logger.error(f"Groq generation failed: {e}")\n            response.mark_failed(str(e))\n\n        return response\n\n    async def health_check(self) -> bool:\n        """Check Groq API availability with SSRF protection."""\n        if not self.api_key:\n            return False\n\n        try:\n            headers = {\n                "Authorization": f"Bearer {self.api_key}",\n                "Content-Type": "application/json"\n            }\n\n            # Use SSRF-protected session\n            resp = await self._make_safe_request(\n                "GET",\n                f"{self.base_url}/models",\n                headers=headers,\n                timeout=aiohttp.ClientTimeout(total=5)\n            )\n            return resp.status == 200\n        except Exception:\n            return False\n\n    @retry_api_call(max_attempts=5, timeout=120)\n    async def _groq_api_call(self, query: ExpertQuery) -> Dict[str, Any]:\n        """Make Groq API call with retry logic."""\n        headers = {\n            "Authorization": f"Bearer {self.api_key}",\n            "Content-Type": "application/json"\n        }\n\n        messages = [\n            {\n                "role": "system",\n                "content": "You are an expert technical consultant providing detailed analysis."\n            },\n            {\n                "role": "user",\n                "content": query.content\n            }\n        ]\n\n        # Use SSRF-protected request\n        resp = await self._make_safe_request(\n            "POST",\n            f"{self.base_url}/chat/completions",\n            headers=headers,\n            json={\n                "model": self.model,\n                "messages": messages,\n                "temperature": 0.7,\n                "max_tokens": 4096\n            }\n        )\n\n        if resp.status == 200:\n            data = await resp.json()\n            content = data["choices"][0]["message"]["content"]\n\n            return {\n                'content': content,\n                'confidence': 0.88,  # High confidence for Groq\n                'metadata': {\n                    "model": self.model,\n                    "usage": data.get("usage", {})\n                }\n            }\n        else:\n            error = await resp.text()\n            raise RuntimeError(f"Groq API error: {error}")\n\n    def _create_fallback_response(self, query: ExpertQuery) -> Dict[str, Any]:\n        """Create fallback response when circuit is open."""\n        return {\n            'content': "Groq API is currently unavailable. The circuit breaker has been triggered due to repeated failures. Please try again later.",\n            'confidence': 0.0,\n            'metadata': {\n                "fallback": True,\n                "reason": "circuit_breaker_open",\n                "model": self.model\n            }\n        }\n\n\nclass DeepSeekExpertClient(BaseExpertClient):\n    """\n    DeepSeek expert client for high-quality reasoning tasks.\n\n    Uses DeepSeek's reasoning models for complex analysis.\n    """\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        model: str = "deepseek-chat"\n    ):\n        """Initialize DeepSeek client."""\n        super().__init__(api_key or os.getenv("DEEPSEEK_API_KEY"))\n        self.model = model\n        self.base_url = "https://api.deepseek.com"\n\n        # Available models\n        self.model_selection = {\n            "reasoning": "deepseek-reasoner",\n            "chat": "deepseek-chat",\n            "coder": "deepseek-coder"\n        }\n\n    def _select_model_for_query(self, query: ExpertQuery) -> str:\n        """Select appropriate DeepSeek model based on query."""\n        if query.query_type in ["architectural", "optimization"] or query.priority == "critical":\n            return self.model_selection["reasoning"]\n        elif "code" in query.content.lower() or query.query_type == "review":\n            return self.model_selection["coder"]\n        else:\n            return self.model_selection["chat"]\n\n    def _create_messages(self, query: ExpertQuery) -> List[Dict[str, str]]:\n        """Create optimized messages for DeepSeek."""\n        system_message = """You are an expert consultant in the Circle of Experts system with advanced reasoning capabilities.\nProvide thorough, well-reasoned analysis with:\n- Step-by-step logical reasoning\n- Clear explanations of your thought process\n- Specific, actionable recommendations\n- Code examples with detailed explanations\n- Consideration of edge cases and limitations\n\nUse clear structure with headings and markdown formatting."""\n\n        # Add query-specific instructions\n        if query.query_type == "review":\n            system_message += "\n\nFocus on: Code quality analysis, potential issues, performance implications, and improvement suggestions."\n        elif query.query_type == "optimization":\n            system_message += "\n\nFocus on: Performance bottlenecks, efficiency improvements, scalability considerations, and optimization strategies."\n        elif query.query_type == "architectural":\n            system_message += "\n\nFocus on: System design principles, architecture patterns, trade-offs, and long-term maintainability."\n\n        user_message = query.content\n        if query.context:\n            user_message += f"\n\nContext: {json.dumps(query.context)}"\n        if query.constraints:\n            user_message += f"\n\nConstraints: {', '.join(query.constraints)}"\n\n        return [\n            {"role": "system", "content": system_message},\n            {"role": "user", "content": user_message}\n        ]\n\n    async def generate_response(self, query: ExpertQuery) -> ExpertResponse:\n        """Generate response using DeepSeek."""\n        if not self.api_key:\n            raise ValueError("DeepSeek API key not configured")\n\n        response = ExpertResponse(\n            query_id=query.id,\n            expert_type=ExpertType.DEEPSEEK,\n            status=ResponseStatus.IN_PROGRESS\n        )\n\n        try:\n            # Select model\n            model = self._select_model_for_query(query)\n            logger.info(f"Using DeepSeek model: {model}")\n\n            # Get circuit breaker for this expert\n            manager = get_circuit_breaker_manager()\n            breaker = await manager.get_or_create(\n                f"deepseek_expert_{model}",\n                CircuitBreakerConfig(\n                    failure_threshold=3,\n                    timeout=90,\n                    failure_rate_threshold=0.5,\n                    minimum_calls=5,\n                    fallback=lambda: self._create_fallback_response(query, model)\n                )\n            )\n\n            # Generate response with circuit breaker protection\n            result = await breaker.call(\n                self._deepseek_api_call,\n                model,\n                query\n            )\n\n            response.content = result['content']\n            response.confidence = result['confidence']\n            response.recommendations = result['recommendations']\n            response.code_snippets = result['code_snippets']\n            response.metadata = result['metadata']\n            response.mark_completed()\n\n        except Exception as e:\n            logger.error(f"DeepSeek generation failed: {e}")\n            response.mark_failed(str(e))\n\n        return response\n\n    async def health_check(self) -> bool:\n        """Check DeepSeek API availability."""\n        if not self.api_key:\n            return False\n\n        try:\n            headers = {\n                "Authorization": f"Bearer {self.api_key}",\n                "Content-Type": "application/json"\n            }\n\n            # Use SSRF-protected request for health check\n            resp = await self._make_safe_request(\n                "POST",\n                f"{self.base_url}/chat/completions",\n                headers=headers,\n                json={\n                    "model": "deepseek-chat",\n                    "messages": [{"role": "user", "content": "Hi"}],\n                    "max_tokens": 5\n                }\n            )\n            return resp.status == 200\n        except Exception as e:\n            logger.error(f"DeepSeek health check failed: {e}")\n            return False\n\n    def _calculate_confidence(self, content: str, completion_data) -> float:\n        """Calculate confidence based on response quality."""\n        confidence = 0.9  # High base confidence for DeepSeek reasoning models\n\n        # Adjust based on finish reason\n        finish_reason = completion_data["choices"][0].get("finish_reason")\n        if finish_reason == "stop":\n            confidence += 0.05\n\n        # Adjust based on content quality indicators\n        if len(content) > 1000:\n            confidence += 0.02\n        if "```" in content:  # Contains code examples\n            confidence += 0.02\n        if any(word in content.lower() for word in ["reasoning", "analysis", "step"]):\n            confidence += 0.01\n\n        return min(1.0, confidence)\n\n    def _extract_recommendations(self, content: str) -> List[str]:\n        """Extract recommendations from DeepSeek response."""\n        recommendations = []\n        lines = content.split('\n')\n\n        # Look for common recommendation patterns\n        in_recommendations = False\n        for line in lines:\n            line = line.strip()\n\n            # Start of recommendations section\n            if any(marker in line.lower() for marker in [\n                "recommend", "suggestion", "advice", "should consider",\n                "next steps", "action items"\n            ]):\n                in_recommendations = True\n                continue\n\n            # Extract bullet points and numbered items\n            if in_recommendations and line:\n                if line.startswith(('-', '*', '•', '1.', '2.', '3.', '4.', '5.')):\n                    rec = line.lstrip('-*•1234567890. ')\n                    if rec and len(rec) > 15:  # Filter out short items\n                        recommendations.append(rec)\n                elif line.startswith('#') or line == "":\n                    in_recommendations = False\n\n        return recommendations[:10]  # Limit to top 10\n\n    def _extract_code_snippets(self, content: str) -> List[Dict[str, str]]:\n        """Extract code snippets from DeepSeek response."""\n        snippets = []\n        parts = content.split('```')\n\n        for i in range(1, len(parts), 2):\n            if i < len(parts):\n                lines = parts[i].split('\n', 1)\n                language = lines[0].strip() if lines else ""\n                code = lines[1] if len(lines) > 1 else ""\n\n                if code.strip():\n                    snippets.append({\n                        "language": language or "text",\n                        "code": code.strip(),\n                        "title": f"DeepSeek Code Example {len(snippets) + 1}",\n                        "description": f"Generated by DeepSeek {self.model}"\n                    })\n\n        return snippets\n\n    @retry_api_call(max_attempts=5, timeout=120)\n    async def _deepseek_api_call(self, model: str, query: ExpertQuery) -> Dict[str, Any]:\n        """Make DeepSeek API call with retry logic."""\n        headers = {\n            "Authorization": f"Bearer {self.api_key}",\n            "Content-Type": "application/json"\n        }\n\n        # Use SSRF-protected request\n        resp = await self._make_safe_request(\n            "POST",\n            f"{self.base_url}/chat/completions",\n            headers=headers,\n            json={\n                "model": model,\n                "messages": self._create_messages(query),\n                "temperature": 0.7,\n                "max_tokens": 4096,\n                "top_p": 0.95,\n                "stream": False\n            }\n        )\n\n        if resp.status == 200:\n            data = await resp.json()\n            content = data["choices"][0]["message"]["content"]\n\n            return {\n                'content': content,\n                'confidence': self._calculate_confidence(content, data),\n                'recommendations': self._extract_recommendations(content),\n                'code_snippets': self._extract_code_snippets(content),\n                'metadata': {\n                    "model": model,\n                    "usage": data.get("usage", {}),\n                    "finish_reason": data["choices"][0].get("finish_reason")\n                }\n            }\n        else:\n            error_text = await resp.text()\n            raise RuntimeError(f"DeepSeek API error ({resp.status}): {error_text}")\n\n    def _create_fallback_response(self, query: ExpertQuery, model: str) -> Dict[str, Any]:\n        """Create fallback response when circuit is open."""\n        return {\n            'content': "DeepSeek API is currently unavailable. The circuit breaker has been triggered due to repeated failures. Please try again later.",\n            'confidence': 0.0,\n            'recommendations': [],\n            'code_snippets': [],\n            'metadata': {\n                "fallback": True,\n                "reason": "circuit_breaker_open",\n                "model": model\n            }\n        }\n